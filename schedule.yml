Monday:
  - name: "Workshop Registration"
    id: "registration-mon-1"
    type: "workshop"
    start: "08:30"
    end: "9:00"
    location: "Emailed to participants"

  - name: "Workshop"
    id: "workshop-mon"
    type: "workshop"
    start: "09:00"
    end: "17:00"
    Generalized Nonlinear Models in R: |
      - Speakers: **Heather Turner**
      - Building: Haydon-Allen Building, Australian National University
      - Room: Emailed to participants
      - Details: [workshop website](https://biometricsociety.org.au/conference2025/workshop.html#generalized-nonlinear-models-in-r)
    Analysing Complex Survey and Subsample Data (with R): |
      - Speakers: **Thomas lumley**
      - Building: Haydon-Allen Building, Australian National University
      - Room: Emailed to participants
      - Details: [workshop website](https://biometricsociety.org.au/conference2025/workshop.html#analysing-complex-survey-and-subsample-data-with-r)
    "Lost in Translation: Speaking Statistician in a Multi-Lingual World": |
      - Speakers: **Peter Humburg** and **Eve Slavich**
      - Building: National Film and Sound Archive of Australia
      - Room: Emailed to participants
      - Details: [workshop website](https://biometricsociety.org.au/conference2025/workshop.html#lost-in-translation-speaking-statistician-in-a-multi-lingual-world)
      - Note: This is a half-day workshop (13:30 - 17:00)
    "Deep Learning and Computer Vision in R: A Practical Introduction": |
      - Speakers: **Patrick (Weihao) Li**
      - Building: Haydon-Allen Building, Australian National University
      - Room: Emailed to participants
      - Details: [workshop website](https://biometricsociety.org.au/conference2025/workshop.html#deep-learning-and-computer-vision-in-r-a-practical-introduction)

  - name: "Conference Registration"
    id: "registration-mon-2"
    type: "general"
    start: "17:00"
    end: "18:00"
    location: "TBA"
    description: "Conference registration and badge pickup. Drop in to collect your materials and meet fellow attendees."

  - name: "Welcome Reception"
    id: "welcome-reception"
    type: "social"
    start: "18:00"
    end: "19:00"
    organizer: "Host Committee"
    location: "Garden Terrace"
    description: "Opening reception with drinks and light refreshments. Join fellow delegates for a relaxed evening to network and celebrate the opening of the conference."

Tuesday:
  - name: "Conference Registration"
    id: "registration-tue"
    type: "general"
    start: "08:30"
    end: "09:00"
    location: "Foyer"
    description: "Morning registration and info desk. Final registration opportunity and general assistance available."

  - name: "Keynote: On Finding Good Experiments"
    id: "keynote-1"
    type: "talk"
    start: "09:00"
    end: "10:00"
    speakers: "Cheng Soon Ong"
    speaker_imgs: "keynote/cheng-soon-ong/chengpic-2023-small.jpg"
    speaker_orgs: "CSIRO and ANU"
    location: "Arc Cinema"
    abstract: "One of the key choices we have as scientists is to design informative experiments. With computational methods like AI promising accurate predictions, we revisit the question of adaptively designing new measurements that take previous data into account. Using examples from genomics, we illustrate some recent ideas on using machine learning to recommend experiments. Then we discuss potential impacts on choosing measurements in spatiotemporal problems. We conclude by outlining some opportunities and challenges of including machine learning in the scientific discovery process."
    biography: "**Cheng Soon Ong** is an Associate Science Director at [Data61, CSIRO](https://www.csiro.au/en/research/technology-space?start=0&count=12) and a senior principal research scientist at the Statistical Machine Learning Group. He works on extending machine learning methods and enabling new approaches to scientific discovery, and has led the [machine learning and artificial intelligence future science platform](https://research.csiro.au/mlai-fsp/) at CSIRO. He supervises and mentors many junior scientists, collaborates with scientists on problems in genomics and astronomy, advocates for open science, and is an adjunct Associate Professor at the [Australian National University](https://www.anu.edu.au/). He is co-author of the textbook Mathematics for Machine Learning, and his career has spanned multiple roles in Malaysia, Germany, Switzerland, and Australia."

  - name: "Session 1A"
    id: "session-1a"
    type: "talk"
    start: "10:00"
    end: "10:45"
    location: "Arc Cinema"
    talk_1: Data-Adaptive Automatic Threshold Calibration for Stability Selection
    talk_1_speakers:
    - "Martin Huang \U0001F464"
    - Samuel Muller
    - Garth Tarr
    talk_1_orgs: The University of Sydney
    talk_1_abstract: Stability selection has gained popularity as a method for enhancing
      the performance of variable selection algorithms while controlling false discovery
      rates. However, achieving these desirable properties depends on correctly specifying
      the stable threshold parameter, which can be challenging. An arbitrary choice
      of this parameter can substantially alter the set of selected variables, as
      the variables' selection probabilities are inherently data-dependent. To address
      this issue, we propose Exclusion Automatic Threshold Selection (EATS), a data-adaptive
      algorithm that streamlines stability selection by automating the threshold specification
      process. EATS initially filters out potential noise variables using an exclusion
      probability threshold, derived from applying stability selection to a randomly
      shuffled version of the dataset. Following this, EATS selects the stable threshold
      parameter using the elbow method, balancing the marginal utility of including
      additional variables against the risk of selecting superfluous variables. We
      evaluate our approach through an extensive simulation study, benchmarking across
      commonly used variable selection algorithms and static stable threshold values.
    talk_2: Variable Selection in a Joint Model for Huntington's Disease Data
    talk_2_speakers:
    - "Rajan Shankar \U0001F464"
    - Tanya Garcia
    - Garth Tarr
    talk_2_orgs: The University of Sydney
    talk_2_abstract: |
      Huntington's disease is a neurodegenerative disease caused by
      a defective Huntingtin gene, with symptoms that progressively worsen and eventually
      lead to a clinical diagnosis. Identifying the clinical and demographic factors
      that influence symptom severity and time-to-diagnosis is critical for understanding
      disease progression so that early-intervention strategies can be timely implemented.
      We propose a joint model to relate symptom severity $y$ and time-to-diagnosis
      $x$, conditional on clinical and demographic predictor variables $\mathbf{z}$.
      However, it may be that certain predictor variables are important for $y$ but
      not for $x$ and vice-versa, so we use regularisation techniques to select different
      sets of predictor variables for $y$ and $x$. Since $x$ is a time-to-event variable,
      there is the added challenge that many of its values are right-censored due
      to individuals who did not develop the disease during the study. Therefore,
      to fit the joint model, we apply the expectation-maximisation (EM) algorithm
      to alternate between parameter estimation and imputation of the right-censored
      values until convergence. We demonstrate our method on Huntington's disease
      patient data, showcasing how users can choose appropriate values for the regularisation
      tuning parameters.
    talk_3: "StableMate: a regression framework for selecting stable predictors across
      heterogeneous data environments\r\n\r\n"
    talk_3_speakers:
    - "Yidi Deng \U0001F464"
    - Jiadong Mao
    - Jarny Choi
    - Kim-Anh Le Cao
    talk_3_orgs: Melbourne Integrative Genomics; Department of Anatomy and Physiology,
      The University of Melbourne; Research School of Finance, Actuarial Studies and
      Statistics, The Australian National University
    talk_3_abstract: "Inferring reproducible relationships between biological variables
      remains a challenge in the statistical analysis of omics data where p > 10,000
      and n < 500. Methods that identify statistical associations lack interpretability
      or reproducibility. We can address these limitations by inferring stable associations
      that are robust to external perturbation on the data. Stable associations can
      be an implication in causality since causal relationships are necessarily stable
      in some sense (Pearl et al. 2009). Unstable associations can also be of interests
      in certain biological applications to study functional heterogeneity in a biological
      system.\r\n\r\nWe developed a new regression framework, StableMate based on
      the concept of stabilised regression (SR), which utilise heterogenous data to
      enforce stability (Pfister et al. 2021). Given datasets generated from different
      environments, such as experiments or disease states, StableMate 1. identifies
      stable predictors with consistent functional dependency with the response across
      environments. 2. builds a robust regression model with stable predictors to
      enforce generalisable prediction in unseen environments. The ultimate aim is
      to build selection ensembles. However, unlike SR that selects stable predictors
      by performing stability tests on every possible predictor subset, StableMate
      optimizes efficiency with a greedy search based on our improved stochastic stepwise
      selection algorithm. In a simulation study, we show that StableMate outperformed
      SR for both variable selection and prediction and significantly reduces running
      time. In three case studies of cancer with different omics data types, we show
      that StableMate can also address a wide range of biological questions.\r\n"
    par_id: 1
    par_total: 2

  - name: "Session 1B"
    id: "session-1b"
    type: "talk"
    start: "10:00"
    end: "10:45"
    location: "Theatrette"
    talk_1: Estimating abundance in small populations using pedigree reconstruction
    talk_1_speakers:
    - "Sarah Croft \U0001F464"
    - Jamie Sanderlin
    - Michael Black
    - Richard Barker
    - Matthew Schofield
    talk_1_orgs: University of Otago
    talk_1_abstract: "Accurate measures of abundance are essential for successful
      monitoring of animal populations,\r\nand for assessing the efficacy of conservation
      interventions. In previous work, genotypic information\r\nhas been incorporated
      into Mark-Recapture models to enable the identification of individuals, as well\r\nas
      determination of kinship between observed individuals in Close-Kin Mark-Recapture
      (CKMR)\r\nmodels. Generally, CKMR models make large sample assumptions limiting
      their application to\r\nmany endangered and at-risk species.\r\nWe have developed
      Bayesian methodology to estimate population abundance and dynamics for\r\nsmall,
      isolated populations using pedigree reconstruction. The true underlying pedigree
      completely\r\ndescribes the abundance and population structure over time, however
      the true relationships between\r\nindividuals in wild populations are rarely
      known. Given a set of observed genotypes, along with\r\nsupplementary data,
      our methodology is able to successfully reconstruct the pedigree without the\r\nneed
      for large sample assumptions. Prior knowledge of the mating structure and reproductive\r\ndynamics
      of the population can also be incorporated in the model. In this talk I will
      present\r\nour pedigree reconstruction approach for population estimation using
      dead recovery data, and will\r\ndiscuss the challenges associated with the full
      pedigree approach."
    talk_2: Accounting for heterogeneous detection rates when inferring eradication
      of an invasive species
    talk_2_speakers:
    - "Sean A. Martin \U0001F464"
    - Leah South
    - Zachary T. Carter
    - Michael Bode
    talk_2_orgs: QUT School of Mathematics/SAEF
    talk_2_abstract: "Eradication of invasive species from islands is an expensive
      but often crucial activity for maintaining the ecosystems of these islands.
      A key challenge in limiting the costs of an eradication campaign is knowing
      the size of the remaining population of invasive animals; this information informs
      both ongoing resource planning and when to end a campaign. Campaign progress
      is measured by the decline in detections of the target species. However, detection
      of a population is imperfect and highly variable, obscuring the true rate of
      decline. As a result, uncertainty arises when detections cease - have all individuals
      truly been eradicated, or do some remain undetected? Statistical models have
      been developed to infer eradication success based on a record of detections
      (sighting records), however, most models ignore natural heterogeneity in detectability.
      \r\nOur work examines the influence of variance in detection rates on the false
      positive and false negative error rates of hypothetical eradication campaigns
      and relates this to overall costs of island conservation. We use a combination
      of Bayesian likelihood and likelihood-free models to overcome simultaneous inference
      limits on abundance and detectability within this highly stochastic study system."
    talk_3: Zero-inflated Tweedie distribution for abundance of rare ecological species
    talk_3_speakers: "Nokuthaba Sibanda \U0001F464"
    talk_3_orgs: Victoria University of Wellington
    talk_3_abstract: "Abundance data for rare species can be extremely zero-inflated,
      where percentage of zeros can be over 90%. This poses a challenge even for the
      standard Tweedie model which naturally allows for a probability mass at zero
      with continuous non-negative values. We investigate use of a zero-inflated Tweedie
      distribution when modelling non-negative continuous abundance values with zeros
      for rare species. Despite their significance, research on zero-inflated models
      has predominantly focused on count models such as zero-inflated Poisson or negative
      binomial regressions, with only recent studies exploring zero-inflated Tweedie
      models in insurance claims (Zhou, Qian, and Yang 2022; Gu2024; So and Valdez
      2024; So and Deng 2025).\r\n\r\nThe zero-inflated Tweedie model uses a mixture
      model approach that integrates a Tweedie model with a binary model to distinguish
      between excess zeros - those resulting from an independent process, and true
      zeros those resulting from the Tweedie model itself. We use a Bayesian approach
      to estimate the model parameters. We model means of the Tweedie model using
      a log-link function with covariates and unobserved random effects. The spatial
      association between observations is accounted for using a conditionally auto-regressive
      prior."
    par_id: 2
    par_total: 2

  - name: "Morning Tea"
    id: "morning-tea-tue"
    type: "social"
    start: "10:45"
    end: "11:15"
    location: "Gallery"
    description: "Light refreshments and beverages. Take a break and enjoy some tea or coffee while networking with peers."

  - name: "Session 2A"
    id: "session-2a"
    type: "talk"
    start: "11:15"
    end: "12:45"
    location: "Arc Cinema"
    talk_1: Extending Spatial Capture-Recapture with the Hawkes Process
    talk_1_speakers:
    - "Alec B. M. van Helsdingen \U0001F464"
    - Charlotte M. Jones-Todd
    talk_1_orgs: University of Auckland
    talk_1_abstract: "Spatial capture-recapture (SCR) is a well-established method
      used to estimate animal population size from animal sighting or trapping data.
      Standard SCR methods assume animal movements are independent and consequently
      cannot incorporate site fidelity (attachment to a particular region) nor the
      temporal correlation of an animal’s location. Recent work has sought to solve
      these issues by explicitly modelling animal movement.\r\n\r\nIn this talk we
      propose an alternative solution for camera trapping surveys based on a multivariate
      self-exciting Hawkes process. Here the rates of detection of a given animal
      at a given camera are a function of not only the location and its proximity
      to the animal’s activity center, but also where and when the animal was most
      recently detected.\r\n\r\nThrough a mixture of Gaussian distributions, our model
      expects more detections closer in space to the last detection, and reduces to
      SCR when an animal is yet to be detected. This formulation, we believe, better
      reflects animal behaviour because shortly after detection, we expect to see
      an individual close to where it was last seen. Thus, our model allows us to
      account for both site fidelity and the inherent temporal correlation in detections
      that have not previously been accounted for in SCR-type models.\r\n\r\nIn this
      talk, I will 1) give an overview of Self-Exciting Spatial Capture-Recapture
      (SESCR) models, and 2) demonstrate the additional inference that can be drawn
      from such models and 3) apply the framework using a few case studies to compare
      traditional SCR and SESCR."
    talk_2: A Test for Detecting Multiple Clusters with Hotspot Spatial Properties
    talk_2_speakers:
    - "Kunihiko Takahashi \U0001F464"
    - Hideyasu Shimadzu
    talk_2_orgs: Institute of Science Tokyo
    talk_2_abstract: Various statistical tests have been widely used in spatial epidemiology
      to investigate regional patterns in disease occurrence, particularly to assess
      whether disease risk is significantly elevated in specific areas compared to
      neighboring regions or adjacent time periods. One such method is the cluster
      detection test (CDT), which identifies non-random spatial distributions of diseases
      and highlights high-prevalence regions without prior assumptions. Among CDT
      methods, scan statistics are compelling and use a maximum likelihood framework
      to search across spatial and/or temporal windows for potential clusters. Examples
      include Kulldorff’s circular scan statistic and the flexibly shaped scan statistic
      by Tango and Takahashi. More recently, Takahashi and Shimadzu proposed a scan-based
      method that simultaneously detects multiple clusters by integrating generalized
      linear models and an information criterion to determine the optimal number of
      clusters. Traditional scan-based tests often assume that disease risk is uniformly
      elevated within a single cluster. However, they may mistakenly combine multiple
      adjacent hotspots—each with potentially different risk levels—into one, thereby
      masking meaningful spatial heterogeneity. In this study, we propose a new test
      procedure that more accurately identifies adjacent hotspot clusters as distinct
      entities. Our approach enhances the scan statistic framework by incorporating
      Cochran’s Q-statistic to assess heterogeneity within clusters. We demonstrate
      the effectiveness of the proposed method through real-world applications and
      compare its performance with conventional scan-based tests.
    talk_3: Outlier-robust estimation of state-space models using a penalised approach
    talk_3_speakers:
    - "Garth Tarr \U0001F464"
    - Rajan Shankar
    - Ines Wilms
    - Jakob Raymaekers
    talk_3_orgs: The University of Sydney
    talk_3_abstract: State-space models are a broad class of statistical models for
      time-varying data. The Gaussian distributional assumption on the disturbances
      in the model leads to poor parameter estimates in the presence of additive outliers.
      Whilst there are ways to mitigate the influence of outliers via traditional
      robust estimation methods such as M-estimation, this issue is approached from
      a more modern perspective that utilises penalisation. A shift parameter is introduced
      at each timepoint, with the goal being that outliers receive a non-zero shift
      parameter while clean timepoints receive a zero shift parameter after estimation.
      The vector of shift parameters is penalised to ensure that not all shift parameters
      are trivially non-zero. Apart from making it feasible to fit accurate and reliable
      time series models in the presence of additive outliers, other benefits of this
      approach include automatic outlier flagging and visual diagnostics to provide
      researchers and practitioners with better insights into the outlier structure
      of their data. We will demonstrate the utility of this method on animal tracking
      data.
    talk_4: Disease cluster detection via functional additive models incorporating
      spatial correlation
    talk_4_speakers:
    - "Michio Yamamoto \U0001F464"
    - Tatsuhiko Anzai
    - Kunihiko Takahashi
    talk_4_orgs: The University of Osaka / RIKEN AIP / Shiga University
    talk_4_abstract: Detecting spatial clusters of diseases is crucial for understanding
      disease patterns and developing effective prevention and treatment strategies.
      Spatial scan statistics are powerful tools for detecting spatial clusters with
      a variable scanning window size. If covariates are related to an outcome and
      not geographically randomly distributed, searching for spatial clusters may
      require adjusting for the covariates. In addition, spatial correlation in the
      outcome, which is often overlooked during cluster detection, can affect the
      results. In this study, we propose a new spatial scan statistic that handles
      multiple functional covariates indicating past information over time and the
      spatial correlation of the outcome. Our method flexibly models these factors
      in the framework of functional additive models. We develop an optimization algorithm
      to estimate the model parameters for the normal outcome case. A simulation study
      and real data analysis indicate that the proposed method can detect disease
      clusters despite longitudinal covariates and spatial correlations compared to
      existing methods.
    par_id: 1
    par_total: 2


  - name: "Session 2B"
    id: "session-2b"
    type: "talk"
    start: "11:15"
    end: "12:45"
    location: "Theatrette"
    par_id: 2
    par_total: 2
    talk_1: Group Sampling with Imperfect Testing for Biosecurity Applications
    talk_1_speakers:
    - "Adele Jackson \U0001F464"
    - Belinda Barnes
    - Mahdi Parsa
    talk_1_orgs: ABARES
    talk_1_abstract: Group sampling, also known as pooled or batch sampling, is a
      standard technique in biological sciences and the health sector to use limited
      resources efficiently. Common objectives include detecting pest species and
      inferring prevalence of disease in communities, livestock or wildlife. The purpose
      of this paper is to support the design of robust group sampling strategies when
      testing processes are imperfect. We formulate analytical distributions and statistics
      for grouped hypergeometric sampling and its binomial approximation that incorporate
      a variety of types of imperfect test. These include tests that respond to the
      presence or absence of contaminated material in a group, as well as PCR and
      serological testing processes where sensitivity depends on the number of contaminated
      items in each group. We also formulate the Hellinger information of a sampling
      scheme, which allows us to develop group sampling design strategies that increase
      the accuracy of inferred prevalence. This is an essential component in decision-making
      during outbreaks of disease and in operational biosecurity applications. Based
      on this work, we estimate leakage through a grouped sampling scheme and show
      how accounting for leakage can alter sampling strategies and improve risk management
      decisions.
    talk_2: Koala Distirbution and Abundance
    talk_2_speakers:
    - "Scott D. Foster \U0001F464"
    - Scott D. Foster
    - Wen-Hsi Yang
    - Peter Caley
    - John McEvoy
    - David Ramsey
    - Drew Terasaki Hart
    - David E. Uribe-Rivera
    - Romane H. Cristescu
    - Eric Vanderdys
    - Adam McKeown
    - Corey J. A. Bradshaw
    - Emma Lawrence
    - Andrew Hoskins
    talk_2_orgs: Data61, CSIRO
    talk_2_abstract: The koala (*Phascolarctos cinereus*) is a well-known and studied
      Australian marsupial, but the species presents a complex case for conservation.
      Currently, most koala conservation efforts focus on local-scale population estimates,
      which are often based on expert opinion or anecdotal evidence, and either precede
      or ignore most (or all) available data. In contrast, conservation listing advice
      and associated recovery plans require population estimates at the species-range
      scale. A data-driven, national-scale population estimate including all available
      information will therefore guide effective management of koala populations by
      providing high-quality and objective information. To this end, we have designed
      nationally consistent survey, implemented it (with the aid of partners) and
      analysed the resulting data (and others). The design uses recently developed
      techniques (clustered spatially-balanced designs) whilst the analysis uses emerging
      models that incorporate multiple data types (e.g. point process, binary and
      count) that are often-enough collected using different equipment, protocols
      and staff. Integrated species distribution models (ISDMs) have, at their heart,
      a simple point process but this simplicity still allows for some complexity
      in terms of how different types of data can inform the point process. The model,
      when fitted to koala data, indicates that the distribution of koalas is patchy
      throughout much of eastern Australia. It also infers that there are more koalas
      than previously guessed. Our estimates provide unprecedented evidence to support
      nationally consistent and spatially explicit decision-making for koala conservation,
      and do so with relevant measures of uncertainty. 
    talk_3: 'Speed: An R package for Spatially Efficient Experimental Designs'
    talk_3_speakers:
    - "Sam Rogers \U0001F464"
    - Wasin Pipattungsakul
    - Julian Taylor
    talk_3_orgs: University of Adelaide
    talk_3_abstract: "Agricultural field trials are typically designed using robust
      statistical randomisation, and best practice agricultural field trials also
      consider the spatial layouts of the experiment, and how that interacts with
      the treatments of interest. However, software providing access to spatially
      optimised experimental designs for field trials is not readily available and
      can be difficult for new users to get started with due to sparse documentation.\r\n\r\nIn
      this talk, we discuss the newly developed *speed* R package which provides easy
      access to a fully open-source package with comprehensive documentation to enable
      the design of spatially optimal experiments. The package offers model-free spatially
      optimal designs via a simulated annealing optimisation algorithm. It can produce
      a spatially optimal version of many types of experimental designs commonly used
      in agricultural research as well as many more complex designs such as incomplete
      block designs and partially replicated designs. It provides multiple objective
      functions out of the box, with the additional flexibility to choose or enable
      custom optimisation metrics, depending on the objective of the researcher. It
      also provides some helper functions for plotting and evaluating experimental
      designs either produced via *speed* or alternative design packages.\r\n\r\nTo
      demonstrate the package's capabilities, we present spatially optimal designs
      for challenging scenarios including two-dimensional blocking and partially replicated
      designs. We also show that speed is significantly faster compared to alternative
      software."
    talk_4: Running Human Subject Experiments via Online Crowdsourcing
    talk_4_speakers: "Patrick Li \U0001F464"
    talk_4_orgs: Australian National University
    talk_4_abstract: Crowdsourcing platforms such as Amazon Mechanical Turk and Prolific
      provide scalable and accessible tools for running online human subject experiments.
      This talk offers an overview of how to design, set up, and manage such studies
      effectively. Drawing on experience from multiple projects, I will walk through
      the key steps for obtaining ethics approval, compare platform workflows, and
      discuss considerations for participant recruitment, screening, task design,
      quality control, cost estimation, and common pitfalls. This session is intended
      for researchers planning to conduct behavioural, perceptual, or decision-making
      experiments, as well as those developing data annotation pipelines for machine
      learning or applied research.


  - name: "Lunch"
    id: "lunch-tue"
    type: "social"
    start: "12:45"
    end: "13:45"
    location: "Gallery"

  - name: "Session 3A"
    id: "session-3a"
    type: "talk"
    start: "13:45"
    end: "15:00"
    location: "Arc Cinema"
    par_id: 1
    par_total: 2
    talk_1: Bayesian clustered ensemble prediction for multivariate time series
    talk_1_speakers: "Shonosuke Sugasawa \U0001F464"
    talk_1_orgs: Keio University
    talk_1_abstract: We propose a novel methodology called the mixture of Bayesian
      predictive syntheses (MBPS) for multiple time series count data and apply the
      methodology to predict the numbers of COVID-19 inpatients and isolated cases
      in Japan and Korea at the subnational level. MBPS combines a set of predictive
      models and partitions the multiple time series into clusters based on their
      contribution to predicting the outcome. In this way MBPS leverages the shared
      information within each cluster and avoids using a multivariate count model,
      which is generally cumbersome to develop and implement. Our data analyses demonstrate
      that the proposed MBPS methodology has improved predictive accuracy and uncertainty
      quantification.
    talk_2: The difficulties of clustering categorical or mixed data
    talk_2_speakers:
    - "Louise McMillan \U0001F464"
    - Daniel Fernández
    - Shirley Pledger
    - Richard Arnold
    - Ivy Liu
    - Eleni Matechou
    - Adam Glucksman
    talk_2_orgs: Victoria University of Wellington
    talk_2_abstract: I will discuss techniques for clustering categorical data that
      go beyond treating the data as numerical or converting the categories to dummy
      variables. I will talk about a range of approaches to clustering categorical
      data, including the \texttt{clustord} R package for clustering ordinal data,
      which uses model-based clustering via likelihoods and the EM algorithm. Then
      I will discuss a Bayesian approach from population genetics that may be extendable
      to general mixed datasets and is the subject of my latest research.
    talk_3: Species archetype models for presence-only data
    talk_3_speakers:
    - "Skipton N.C. Woolley \U0001F464"
    - "Scott Woolley \U0001F464"
    - David Warton
    - Piers Dunstan
    talk_3_orgs: CSIRO
    talk_3_abstract: Joint species distribution modelling is a recently emerged and
      potentially powerful statistical method for analysing biodiversity data. Despite
      the plethora of presence-only occurrence data available for biodiversity analysis,
      there remain few examples of presence-only multiple-species modelling approaches.
      We present a mixture-of-regressions model for understanding how groups of species
      are distributed based on presence-only data. Our approach extends Species Archetype
      Models using a point process framework and incorporates joint estimation of
      sighting bias based on all species occurrences included in the model. We demonstrate
      that our method can accurately recapture mean and variance of parameters from
      simulated data sets and provide better estimates than those generated from multiple
      single species presence-only species distribution models. We apply our approach
      to a Myrtaceae presence-only occurrence dataset from New South Wales, Australia.
      We show that presence-only Species Archetype Models allow for the propagation
      of variance and uncertainty from the data through predictions, improving inference
      made on presence-only biodiversity data for multiple species.

  - name: "Session 3B"
    id: "session-3b"
    type: "talk"
    start: "13:45"
    end: "15:00"
    location: "Theatrette"
    par_id: 2
    par_total: 2
    talk_1: 'Building Trust Without Peer Review: Establishing Reproducibility Standards
      in Industrial Statistical Consulting'
    talk_1_speakers: "Dean Marchiori \U0001F464"
    talk_1_orgs: Wave Data Labs
    talk_1_abstract: "In academic research, peer review serves as a cornerstone for
      ensuring the credibility and reproducibility of findings. However, in industry,
      statistical consultants often operate without the formal structures of peer
      review, posing gaps in ensuring the reproducibility and trustworthiness of their
      analyses. \r\n\r\nThis talk explores how statistical consultants for industry
      can proactively establish and adhere to standards that promote reproducibility
      and foster trust, even in the absence of traditional peer review mechanisms.
      \r\n\r\nResearchers, data scientists and statisticians often engage with and
      provide expertise to government, industry and other groups outside of academia.
      This expertise is often trusted implicitly and relied on to make important decisions.
      On the other hand, there are many well known criticisms of conventional peer
      review systems both in academic publishing and in commercial work. \r\n\r\nDrawing
      on practices from various industries, we will propose a pragmatic framework
      for the development and implementation of standardized reporting formats, the
      use of open-source tools for reproducible analysis, and the adoption of best
      practices for documentation and code sharing. We will also examine the role
      of professional organizations and industry consortia in setting guidelines that
      encourage transparency and accountability.\r\n\r\nAttendees will gain insights
      into practical strategies for implementing reproducibility standards in their
      own consulting and research practices and contribute to a broader movement towards
      transparency and accountability in industrial statistics."
    talk_2: Teaching Meta-Analysis for Systematic Reviewers with Mixed Statistical
      Training
    talk_2_speakers:
    - "Xu Ning \U0001F464"
    - David K E Chan
    - Coralie Williams
    - ''
    - Peter Humburg
    talk_2_orgs: University of New South Wales
    talk_2_abstract: Higher-degree research (HDR) students often consider conducting
      a systematic review of their research domain, with the goal of synthesising
      their findings with a meta-analysis. However, the statistical training of HDR
      students and their supervisors can range from little to comprehensive. Consequently,
      we found in our consultations with these students that they may struggle in
      specifying their meta-analytical model, misinterpret their model outputs and
      diagnostics, or both, especially with complex meta-analytical models. Hence,
      we have developed a short course on meta-analysis to address these knowledge
      gaps. The course aims to develop students’ mastery in meta-analytical methods.
      We will present empirical findings on whether the course’s aim was met and discuss
      the pedagogical considerations made to accommodate the range of statistical
      training in our target cohort.
    talk_3: 'Tales from the jungle: a personal perspective of statistical consulting
      since COVID'
    talk_3_speakers: "Alice Richardson \U0001F464"
    talk_3_orgs: Australian National University
    talk_3_abstract: "The period since COVID-19 has brought unique challenges to statistical
      consulting, with researchers venturing into unfamiliar statistical territory.
      The activities of an academic statistical consulting practice have therefore
      very much taken on the feel of a journey through a statistical jungle.\r\n\r\nIn
      this talk I will describe some of the more alarming encounters I have experienced
      in this jungle over the last five years. Examples will be selected from my client
      interactions within the broad biometric research field. Names and topics will
      be altered to protect the participants, but not so much as to alter the key
      learnings from the encounters.\r\n\r\nThese examples will cover the entire range
      of statistical endeavour. From power and sample size calculation through data
      collection to modelling, presentation of results and visualisation, no area
      of statistical activity is untouched by surprising methodological suggestions
      from researchers. The simultaneous emergence of large language models has also
      brought unexpected consequences in consulting practice, as researchers increasingly
      turn to AI tools for statistical guidance. I’ll draw out the patterns of common
      traps and highlight how the biometric community can assist and advise.\r\n\r\nI
      will also discuss some of the practical strategies I have developed to carve
      out a safe path. These strategies can be used for navigating similar consulting
      challenges and for fostering better statistical practices in other collaborative
      research environments beyond academia.\r\n"
    talk_4: 'Better Conversations, Better Support: Strengthening Consulting through
      Practical Education and Community'
    talk_4_speakers:
    - "Sharon  G. Nielsen \U0001F464"
    - Sam Rogers
    - Annie Conway
    talk_4_orgs: Adelaide University
    talk_4_abstract: "Effective statistical consulting in agriculture depends not
      just on technical expertise, but on shared understanding, clear communication,
      and the confidence of those seeking advice. Through its series of practical
      agronomic workshops and the linked Community of Practice, the Biometry Hub has
      shown how targeted education can transform the way consulting support is delivered
      and received.\r\n\r\nThe workshops, typically four core sessions delivered over
      five days, cover the design of robust experiments, sound data collection, practical
      analysis, and clear interpretation of results. By equipping researchers and
      industry staff with these practical skills, the workshops lay the groundwork
      for more productive conversations between statisticians and collaborators.\r\n\r\nThe
      Community of Practice then extends this support beyond the workshop room. Monthly
      meetings held over Zoom create an informal space for participants to revisit
      concepts, tackle real-world questions, and build confidence alongside peers
      and mentors.\r\nThis talk will share how combining hands-on training, tailored
      tools like the biometryassist R package, and an ongoing network of support strengthens
      the consulting relationship, helping ensure that good statistical practice is
      not only taught, but actively put into practice across the grains industry."

  - name: "Afternoon Tea"
    id: "afternoon-tea-tue"
    type: "social"
    start: "15:00"
    end: "15:30"
    location: "Gallery"
    description: "Afternoon refreshments. Light snacks and drinks served."

  - name: "Session 4A"
    id: "session-4a"
    type: "talk"
    start: "15:30"
    end: "16:45"
    location: "Arc Cinema"
    talk_1: 'Nested-factorial treatment models: types, their uses and examples'
    talk_1_speakers: "Chris Brien \U0001F464"
    talk_1_orgs: The Plant Accelerator, University of Adelaide
    talk_1_abstract: Most commonly factorial experiments involve several crossed treatment
      factors and the treatment model includes terms for all possible combinations
      of the treatment factors. However, treatment models that employ nested treatment
      factors are beneficial in a number of situations and could be used more often.
      Applicable situations include those in which (i) treatment factors are intrinsically
      nested, (ii) treatments involve one or more factors plus a control and (iii)
      treatments have been systematically allocated and/or are pseudoreplicated. Four
      basic types of nested factors that start with a single nesting factor are (i)
      one nested factor, (ii) multiple, crossed, nested factors, (iii) multiple, independent,
      nested factors, and (iv) a hierarchy of multiple, nested factors. Examples involving
      the different situations and types of nesting will be presented. Constructing
      the factors using the `R` package `dae` (Brien, 2025, <https://cran.r-project.org/package=dae>)
      will be described and the properties of the models discussed for the examples
      via their anatomies produced using `dae`.
    talk_2: Integrating Spatial Data and On-Farm Experimentation to Understand Wheat
      Variety Performance Across Western Australia
    talk_2_speakers: "Sandra K. Tanz \U0001F464"
    talk_2_orgs: Centre for Crop and Disease Management, Curtin University
    talk_2_abstract: "Understanding spatial variability in crop performance is critical
      to improving the statistical design and interpretation of variety trials at
      the farm scale. This pilot study investigates the integration of large-scale
      on-farm experimentation (OFE) with spatial data analysis to better understand
      genotype-by-environment interactions in wheat. The trial spans five grower-managed
      sites across Western Australia, incorporating two core wheat varieties—Scepter
      (a widely grown commercial line) and IGW6993 (a near-release InterGrain line)—with
      some growers also including Rockstar and Tomahawk CL Plus based on local relevance.\r\n\r\nEach
      trial is embedded within commercial paddocks and implemented using strip trial
      designs aligned with paddock production zones identified from historical yield
      maps. This design allows statistical comparisons of varietal performance across
      contrasting spatial zones defined by environmental variation in soil, topography,
      and management history. The study aims to (1) develop robust methods for spatially
      aware experimental design in commercial cropping systems, and (2) assess the
      potential for OFE to complement small-plot trials in supporting variety selection
      and agronomic decision-making.\r\n\r\nWhile the trial was only recently seeded
      (May 2025), preliminary outputs include trial design validation and early NDVI
      imagery from emergence. Future analyses will incorporate additional covariates
      such as soil electrical conductivity (EM mapping), NDVI time series, and topographic
      indices to model spatial responses. This work highlights the value of integrating
      spatial data into trial workflows and explores the role of predictive breeding
      tools in commercial agriculture."
    talk_3: Multi-environment trial analysis of count data with complex variance structures
      using generalised linear mixed models
    talk_3_speakers:
    - "Michael H. Mumford \U0001F464"
    - Alan Huang
    - Alison M. Kelly
    talk_3_orgs: Queensland Department of Primary Industries
    talk_3_abstract: "The analysis of response data from agricultural field experiments
      conducted in multi-environment trials (METs) is typically performed using linear
      mixed models (LMMs). The strength of the LMM framework is in the ability to
      seamlessly model experimental design terms and to include complex variance structures,
      especially for unbalanced data. When the response variable is count data, the
      assumptions underpinning the LMM are violated and it is necessary to extend
      to a generalised linear mixed model (GLMM) approach. Statistical modelling using
      GLMMs introduces additional complexities arising from a combination of (i) susceptibility
      to large estimation biases due to approximations of the marginal likelihood,
      (ii) accounting for heterogeneity of variance/dispersion, and (iii) the increase
      in computational resources required.\r\nIn this talk, a statistical methodology
      is proposed for the MET analysis of count data. The analysis approach uses a
      GLMM framework, assuming an underlying mean-parameterised Conway-Maxwell Poisson
      distribution, that can account for arbitrarily under and over-dispersed count
      data. This framework enables partitioning of residual variation from genetic
      and other extraneous sources of variation, and adopts a factor-analytic model
      for the genotype by environment interaction effects. The proposed methodology
      is applied to a series of common bean trials, where the aim is genotype selection
      for the response variable pod count per plant. The analysis is implemented using
      the `glmmTMB R`-package, which uses automatic differentiation to enhance computational
      speed, the Laplace approximation estimation method to reduce estimation biases,
      and a residual maximum likelihood (REML)-like correction to further reduce estimation
      biases for variance components.\r\n"
    par_id: 1
    par_total: 2

  - name: "Session 4B"
    id: "session-4b"
    type: "talk"
    start: "15:30"
    end: "16:45"
    location: "Theaterette"
    talk_1: 'Enhancing Fraud Detection in Banking through Random Survival Forests:
      Addressing Data Imbalance and Model Transparency'
    talk_1_speakers:
    - "Arjun Sekhar \U0001F464"
    - Samuel Muller
    - Maurizio Manuguerra
    talk_1_orgs: Macquarie University
    talk_1_abstract: "With the rise in financial fraud amid growing transaction volumes,
      in this presentation we share our insights from the use of Random Survival Forests
      (RSFs) for enhancing fraud detection in banking systems. While many machine
      learning techniques prioritise accuracy, they overlook two crucial issues: the
      imbalance of fraud versus legitimate transaction data, and the transparency
      of the model decision-making process. \r\n\r\nWe position RSFs as a potential
      solution, leveraging their time-to-event structure to enable dynamic fraud prediction
      and their ensemble nature to handle high-dimensional, skewed datasets. Using
      real-world transactional data, the presentation addresses model imbalance through
      resampling strategies and evaluates the model’s ability to detect minority-class
      fraud events without inflating false positives. \r\n\r\nFurthermore, the interpretability
      of RSFs is explored through explainable AI frameworks, offering transparency
      essential for regulatory compliance and institutional trust. By investigating
      RSFs to deliver fraud detection that is both accurate and accountable, this
      presentation bridges algorithmic robustness with transparency, advancing a practical
      and governance-aligned solution for responsible AI deployment in today’s increasingly
      regulated financial services landscape."
    talk_2: Using point cloud data to discover genomic regions associated with dynamic
      height
    talk_2_speakers:
    - "Colleen H Hunt \U0001F464"
    - David R. Jordan
    - Emma S. Mace
    talk_2_orgs: University of Queensland
    talk_2_abstract: "Plant height in grain sorghum (*Sorghum bicolor L. Moench*),
      influences yield, lodging resistance, and harvestability, yet conventional methods
      usually measure plant height at maturity, but this single-time-point measurement
      misses the growth dynamics across the season. Tracking and analysing height
      over time can reveal more about the genetic factors controlling development
      rate and pattern. \r\nData from 881 sorghum lines from a diverse population
      were planted in a partially replicated design with 1190 plots. UAV-based high-throughput
      phenotyping was used to generate weekly plant height measurements from emergence
      to flowering. These aerial images were collected and processed using photogrammetric
      software to create dense point clouds and canopy height models for each plot.
      \r\nA logistic regression model within a linear mixed model framework was applied
      to effectively depict the sigmoidal growth pattern typical of sorghum, with
      parameters indicating maximum height, growth rate, and inflection point. These
      biologically relevant parameters offered a straightforward, quantitative summary
      of each genotype’s growth trajectory. The logistic growth parameters were then
      used in a genome-wide association study (GWAS) to identify loci linked to the
      dynamic aspects of height development, with several significant associations
      detected. \r\nThis is among the first studies to combine logistic growth modelling
      of high-throughput UAV data with GWAS to dissect the genetic control of growth
      dynamics in sorghum. Our findings underline the usefulness of logistic regression
      for modelling crop growth and showcase the effectiveness of combining UAV phenotyping,
      statistical growth modelling, and GWAS to identify genomic regions that influence
      developmental trajectories in sorghum.\r\n"
    talk_3: 'Automatic debiased machine learning (autoDML) for causal inference: implementation
      and evaluation in real-world observational studies'
    talk_3_speakers:
    - "Tong Chen \U0001F464"
    - Stijn Vansteelandt
    - Margarita Moreno-Betancur
    talk_3_orgs: Murdoch Children's Research Institute
    talk_3_abstract: "The estimation of average treatment effect (ATE) is a central
      goal of observational health research. Causal machine learning (CML) methods
      are increasingly popular for ATE estimation, as they leverage data-adaptive
      algorithms to account for complex relationships amongst confounders, exposure
      and outcome while ensuring valid inference. Major CML methods, such as double
      machine learning and targeted maximum likelihood estimation, offer robustness
      and efficiency advantages but they require first estimating the propensity score
      and then calculating treatment divided by the propensity score, known as the
      Riesz representer. Standard machine learning algorithms for the propensity score
      estimation are suboptimal as they are designed to minimise prediction error,
      ignoring that equally large estimation errors in small and large propensity
      scores propagate very differently into the ATE estimates upon taking the reciprocal.
      Automatic debiased machine learning (autoDML), introduced by Chernozhukov et
      al. (2021), addresses this issue by directly estimating the Riesz representer
      using machine learning methods, yielding a more stable alternative to standard
      CML methods. Despite its theoretical advances, autoDML remains largely unknown
      in biostatistical research and has not been compared to standard CML methods
      in realistic simulation studies. \r\nIn this talk, we bridge these gaps by introducing
      autoDML and highlighting its advantages. We describe a practical, step-by-step
      guide for implementing autoDML, and compare autoDML with standard CML methods
      using comprehensive, realistic simulation studies. We further illustrate the
      utility of autoDML using data from the Longitudinal Study of Australian Children
      to evaluate the impact of overweight or obesity on cardiovascular outcomes in
      adolescence."
    par_id: 2
    par_total: 2

  - name: "Poster Session"
    id: "poster-tue"
    type: "poster"
    start: "17:00"
    end: "19:00"
    location: "Gallery"
    description: "Poster presentations and discussions. Explore a wide range of research topics during this informal poster session."
    talk_1: Using atmospheric transport models to predict species incursions in northern
      Australia
    talk_1_speakers: "Zhenhua (Iris) Hao Dr \U0001F464"
    talk_1_orgs: Department of Agriculture, Fisheries and Forestry
    talk_1_abstract: "Predicting the arrival of invasive species is critical for strengthening
      biosecurity, particularly in regions like northern Australia that are vulnerable
      to aerial incursions. This ongoing study compares atmospheric modelling with
      detection patterns from long-term field surveillance to evaluate how different
      models can be integrated to increase confidence in identifying likely source
      regions, arrival pathways and periods of elevated risk. We apply three complementary
      models: HYSPLIT (Hybrid Single-Particle Lagrangian Integrated Trajectory), FLEXPART
      (FLEXible PARTicle dispersion model), and Lagrangian Coherent Structures (LCS).
      These models range in complexity - from simulating basic wind trajectories to
      identifying hidden transport patterns within the atmosphere.\r\nA rich empirical
      foundation is provided by two decades of fruit fly trapping data collected through
      the Northern Australia Quarantine Strategy (NAQS) in the Torres Strait. By identifying
      likely incursion windows for Oriental Fruit Fly (Bactrocera dorsalis) arrivals
      from Papua New Guinea, we evaluate and compare model performance and gain insights
      into potential dispersal mechanisms. While model implementation and case analysis
      are still underway, preliminary results suggest that combining outputs from
      different models improves the detection and interpretation of likely incursion
      events.\r\nFuture work will focus on integrating these model outputs with additional
      environmental and biological datasets to produce dynamic risk maps. These maps
      aim to support NAQS surveillance planning and resource prioritisation. This
      framework can also be extended to other high-risk species to enhance predictive
      capacity and improve biosecurity strategies for Australia's north."
    talk_2: Visualisation of multinomial multilevel time-series modelling with application
      to current smoking status
    talk_2_speakers:
    - "Alice Richardson \U0001F464"
    - Sumonkanti Das
    - Ashis Talukder
    - Mu Li
    - ''
    - Bernard Baffour
    talk_2_orgs: Australian National University
    talk_2_abstract: "In Australia, tobacco smoking has declined overall over the
      last 25 years, yet substantial disparities exist across age, sex and state.
      In this talk we will present the results of multilevel multinomial time series
      modelling to estimate trends in smoking status (current, former and never) across
      small domains defined by seven age groups, two genders, and eight states and
      territories from 2001–2022.\r\n\r\nThe model is developed using ex-smokers as
      the reference category, as their proportion remains relatively stable over time.
      Statistically significant random intercepts and random slopes for linear time
      trends are identified at the state-age-sex level. The spatio-temporal extension
      of the multinomial multilevel logistic model yields detailed estimates, helping
      to uncover disparities in the trends of smoking decline. Temporal random effects
      at the state-sex and age-sex levels also substantially contribute to achieving
      numerically consistent trend estimates. \r\n\r\nVisualisations of these models
      are especially important for conveying results in a compelling manner to health
      researchers and policy makers and program implementation teams. We will discuss
      the process of arriving at our visualisations using R for this particular application."
    talk_3: 'Paired Comparison with Cyclic Dominance: An Extension of the Bradley–Terry
      Model'
    talk_3_speakers:
    - "Yuki Ohno \U0001F464"
    - Kouji Tahata
    talk_3_orgs: Tokyo University of Science
    talk_3_abstract: "The Bradley–Terry model has become a cornerstone for analyzing
      paired-comparison data, providing a principled approach to estimating \"strength\"
      parameters without imposing a latent, continuous ordering on teams, treatments,
      or evaluators. Its transitivity constraint—each pairwise preference must be
      consistent with a global ranking—yields excellent performance in many applications,
      including sports analytics, psychometric scaling, and clinical preference studies.
      However, empirical datasets sometimes exhibit cyclic dominance, rendering the
      classical model inadequate and leading to systematic misfit.\r\n\r\nReal-world
      data often display cyclic patterns, resembling a “rock-paper-scissors” dynamic,
      which can violate the transitivity constraint typically expected in such analyses.
      To address this issue, we first review important applications of the Bradley-Terry
      model and highlight instances where non-transitive outcomes occur. Next, we
      introduce a minimal extension of the model that preserves the original likelihood
      but adds a single \"cycle-strength\" parameter. This parameter is designed to
      capture uniform three-way dominance among any trio of outcomes. When the cycle-strength
      parameter is set to zero, the model reverts to the classical Bradley-Terry framework.
      Importantly, we demonstrate that our cycle strength corresponds precisely to
      the quasi-asymmetry measure proposed by Tahata et al. (2004), establishing a
      clear quantitative relationship between model fit and deviations from transitivity."
    talk_4: Cleaning Text Data with Large Language Models
    talk_4_speakers:
    - "Jiajia Li \U0001F464"
    - Emi Tanaka
    talk_4_orgs: Australian National University
    talk_4_abstract: "Data cleaning – particularly the standardisation and feature
      extraction from unstructured text – is a time-consuming and often manual process
      in data science workflows. Traditional methods, such as approximate string matching,
      lack semantic understanding and still require inefficient, manual corrections.
      We leverage the transformative potential of large language models (LLMs) to
      automate these tedious tasks and significantly accelerate data preparation.\r\n\r\nWe
      introduce the `emend` R package, which leverages LLMs to perform various data
      cleaning tasks, including matching categorical variables, reordering ordinal
      variables, translating languages, and standardising dates and addresses. The
      package integrates with local LLMs via Ollama as well as cloud-based APIs via
      the R package ellmer, offering multiple options for using LLMs from different
      providers. A comprehensive simulation study evaluated emend’s core functionality
      across 13 LLMs and a baseline string-matching method on real-world datasets
      of country names, institutional affiliations, and species names. Results demonstrate
      high accuracy for LLM-based methods in general text standardisation, with GPT-4o-mini
      and GPT-4o excelling on country names and affiliations, respectively. The `emend`
      package streamlines data cleaning in R, reducing the need for extensive manual
      corrections for data analysts. This work highlights LLMs as powerful tools for
      increasing efficiency of cleaning text data."
    talk_5: 'ggincerta: An R Package for Uncertainty Visualisation with a Layered
      Grammar of Graphics'
    talk_5_speakers:
    - "Xueqi Ma \U0001F464"
    - Emi Tanaka
    - Francis KC Hui
    - Weihao Li
    - Quan Vu
    talk_5_orgs: Australian National University
    talk_5_abstract: "Uncertainty is inherent in all estimates derived from data.
      However, when visualising these estimates on spatial maps, uncertainty is often
      overlooked, leading to decision-making based solely on point estimates. The
      `Vizumap` R package addresses this gap by integrating both estimates and their
      uncertainties in areal data, offering four types of visualisations: bivariate
      choropleth maps, pixel maps, glyph maps, and exceedance probability maps. \r\n\r\nBuilding
      upon `Vizumap`, we introduce the `ggincerta` R package, a ground-up reimplementation
      that recreates all of `Vizumap`’s visualisations, but also provides enhanced
      functionality, within the layered grammar of graphics framework provided by
      `ggplot2`. By leveraging `ggplot2`, `ggincerta` seamlessly integrates uncertainty
      visualisation into users’ existing workflows, employing familiar syntax and
      conventions. This approach not only enhances accessibility for those already
      comfortable with `ggplot2`, but also exemplifies how existing systems can be
      extended to new purposes, rather than developing separate, ad-hoc solutions
      that require learning additional languages or interfaces."
    talk_6: The “Theory of Sampling” (ToS) - have statisticians missed the boat?
    talk_6_speakers:
    - "Damian Collins \U0001F464"
    - Anne Harris
    talk_6_orgs: NSW DPIRD
    talk_6_abstract: "The \"Theory of Sampling\" (ToS) framework originated with chemical
      engineer Pierre Gy over 50 years ago. It essentially deals with the sampling
      of materials, particularly soils, composts,and minerals. It enjoys a strong
      advocacy amongst chemical engineers, geologists and similar professionals. The
      ToS instructs practitioners to stop and think about the sampling procedure.
      It encourages them to consider all the sources of variation in the material
      they wish to sample rather than just choosing to take the most convenient sample
      such as a “grab sample” from the surface. However, one of its strongest advocates
      argues that, in order to save on analysis costs, only one “correct” composite
      sample is necessary. This single sample, deemed to be “representative” of the
      entire material, is all that is required to characterise a material (Esbenson
      (2017)). Composting sampling guidelines around the world use the ToS to justify
      this practice of single sampling. For example, it has currently been recommended
      in the AS4454 review in Australia (https://www.as4454review.com.au/). However,
      with only one sample analysed, contributions to measurement uncertainty from
      either the sampling or analysis processes, cannot be assessed. It is concerning
      that despite the apparent sophistication of ToS methodology and the breadth
      of its advocacy network, it perpetuates the scientifically unsound idea of a
      “representative sample.” .Even the name, “Theory of Sampling”, is a misnomer,
      suggesting an even broader application than just the sampling of materials.
      We review the ToS and consider implications to statistical practise and adoption.\r\n\r\nEsbenson,
      K.H. (2017). Sampling: Theory and Practice. Alchemist 85 pages 3–6.\r\nhttps://www.lbma.org.uk/alchemist/issue-85/sampling-theory-and-practice"
    talk_7: Evaluating Diagnostic Performance via Bayesian $F_1$ Score Estimation
      without a Gold Standard
    talk_7_speakers:
    - "Jun Tamura \U0001F464"
    - Yusuke Saigusa
    - Kouji Yamamoto
    talk_7_orgs: Yokohama City University
    talk_7_abstract: "In clinical medicine, diseases are often diagnosed with high
      accuracy based on characteristic signs and symptoms. However, the gold standard
      tests used to confirm diagnoses are frequently invasive, expensive, or impractical
      for population-level screening. This highlights the importance of developing
      and validating simple, non-invasive screening tests.\r\nDiagnostic performance
      is typically evaluated using metrics such as accuracy, sensitivity (Se), specificity
      (Sp), positive predictive value (PPV), and negative predictive value (NPV).
      However, when disease prevalence is low or class imbalance exists, accuracy
      and Sp may not adequately reflect test performance. $F_1$ score, which considers
      both Se and PPV, has been widely adopted in such settings as a balanced measure
      of diagnostic ability.\r\nA limitation of the $F_1$ score is its reliance on
      the true disease status defined by a gold standard. In some fields, however,
      the disease mechanism may be unclear or the definition of clinical signs ambiguous,
      making it difficult to define a reliable gold standard. This presents challenges
      for evaluating diagnostic accuracy using conventional methods.\r\nTo address
      this issue, we propose a Bayesian framework that enables estimation of the $F_1$
      score without directly observing gold standard outcomes. The approach uses latent
      class analysis to estimate unobserved disease status and offers a promising
      method for assessing test performance in settings where the gold standard is
      unavailable or uncertain\r\n"
    talk_8: 'Evaluating Remote Monitoring in Automated Peritoneal Dialysis: A Difference-in-Differences
      Analysis'
    talk_8_speakers:
    - "Annie Conway \U0001F464"
    - Stephen McDonald
    talk_8_orgs: ANZORRG | ANZDATA | Australia & New Zealand Dialysis and Transplant
      Registry
    talk_8_abstract: "During a cholera outbreak in 1850s London, physician John Snow
      famously traced the source of the outbreak to contaminated water supply. In
      what could be deemed a \"natural experiment\", he compared cholera mortality
      rates between areas served by different water companies before and after one
      company relocated its water intake, using an early form of the method _difference-in-differences_
      (DiD).\r\n\r\nRecently, DiD has gained renewed popularity in health and epidemiological
      sciences for testing the effectiveness of technological or policy interventions.
      However, real-world applications can have complexities such as staggered roll-out
      of the intervention and heterogeneity between treated units. Additionally, these
      often involve count outcomes that violate the linear model assumptions underlying
      existing DiD methods.\r\n\r\nIn this project I consider the impact of a new
      technology for automated peritoneal dialysis (APD) that allows patients to be
      remotely monitored by clinicians as they complete dialysis at home. This new
      APD cycler was rolled out gradually in PD centres across Australia and New Zealand
      between 2017 and 2023. Using data from the ANZDATA registry, I assess remote
      monitoring's impact across PD centres, counting the number of deaths, peritonitis
      infections and technique failures, comparing before and after the roll-out and
      between treated and untreated centres.\r\n\r\nThis analysis employs various
      DiD methods to overcome the challenges of staggered adoption and heterogeneous
      effects, to provide an evaluation of remote monitoring's impact on critical
      patient outcomes."
    talk_9: Judgement Post-Stratification for Covariate Adjustment in Pairwise Comparisons
      in Block Designs
    talk_9_speakers:
    - "Sam Rogers \U0001F464"
    - Omer Ozturk
    - Olena Y Kravchuk
    talk_9_orgs: University of Adelaide
    talk_9_abstract: The randomised complete block design (RCBD) remains one of the
      most widely used experimental designs in agronomy, agriculture, and controlled-environment
      research. In this study, we introduce a novel post-experimental method, judgement
      post-stratification, for improving the efficiency of pairwise treatment comparisons
      using plot-level covariate information. The method operates within the RCBD
      framework to focus on generalised designs with multiple replications of treatments
      within blocks. Using simulations based on uniformity trials, we show that when
      plots can be consistently ranked according to an expected outcome, independent
      of treatment assignment, this ranking can be used post-hoc to enhance the precision
      of estimated treatment differences. This approach offers a practical and flexible
      tool for incorporating auxiliary information into the analysis of block designs
      without compromising the integrity of the original randomisation.
    talk_10: Small Area Estimation of Fertility Rate in the Philippines using DHS
      and Census Data
    talk_10_speakers: "George Manapat \U0001F464"
    talk_10_orgs: University of the Philippines Diliman
    talk_10_abstract: Accurate and timely estimation of fertility indicators at subnational
      levels is essential for effective health planning, especially in countries with
      geographically diverse populations and uneven data availability. In the Philippines,
      while national and regional fertility rates are routinely measured through national
      surveys such as the Demographic and Health Survey (DHS), local-level estimates,
      e.g., at the provincial level, remain sparse and unreliable due to limited sample
      sizes. Using the data from the 2022 Philippine DHS and auxiliary covariates
      from the 2020 Census of Population and Housing, small area estimates of fertility
      rate at the provincial level are obtained using a Bayesian hierarchical model
      accounting for spatial correlation. Preliminary findings show substantial differences
      in fertility rates between provinces. Some areas, especially in more remote
      or underserved regions, have much higher fertility than the national average.
      The results underscore the necessity for area-specific health programs and policies
      to address the unique demographic challenges of each locality.
    talk_11: Bayesian inference for sparse Gaussian copula graphical model
    talk_11_speakers: "Tomotaka Momozaki \U0001F464"
    talk_11_orgs: Tokyo University of Science
    talk_11_abstract: We consider Bayesian inference for sparse dependence structures
      among variables in multivariate mixed data, which consists of both continuous
      and discrete variables. Traditional graphical modeling approaches often struggle
      with mixed-type data, as they typically focus on either purely continuous or
      purely discrete settings. This limitation becomes particularly problematic in
      high-dimensional scenarios where identifying relevant variable relationships
      is crucial for interpretability and prediction accuracy. This study proposes
      a novel Bayesian approach that combines Gaussian copula graphical models with
      advanced shrinkage priors, including horseshoe priors and double exponential
      priors. The Gaussian copula framework enables unified modeling of mixed-type
      variables by transforming them to a common latent Gaussian scale, while shrinkage
      priors facilitate automatic variable selection by adaptively shrinking irrelevant
      dependencies toward zero. This combination addresses both the mixed-data challenge
      and the high-dimensionality issue simultaneously. We develop an efficient posterior
      sampling algorithm that maintains the positive definiteness of correlation matrices
      by employing a block Gibbs sampler with hit-and-run algorithm. Our approach
      utilizes rank information to handle mixed-type data within the copula framework,
      enabling unified treatment of continuous and discrete variables. The proposed
      method is particularly valuable for analyzing complex datasets in genomics,
      economics, and social sciences, where variables of different types naturally
      co-occur. We present comprehensive evaluation of our method's effectiveness
      through extensive simulation studies comparing with existing approaches and
      real data analysis demonstrating practical applicability.
    talk_12: Visualization for departures from symmetry with the power-divergence-type
      measure in square contingency tables
    talk_12_speakers:
    - "Wataru Urasaki \U0001F464"
    - Tomoyuki Nakagawa
    - Jun Tsuchida
    - Kouji Tahata
    talk_12_orgs: Department of Information Sciences, Tokyo University of Science
    talk_12_abstract: When the row and column variables consist of the same category
      in a two-way contingency table, it is called a square contingency table. Such
      tables often have an association structure concentrated along the main diagonal,
      making the analysis of symmetric relationships and transitions important. Various
      models and measures have been proposed to analyze these structures to understand
      the changes between two variables' behavior at two-time points or cohorts. This
      is necessary for a detailed investigation of individual categories and their
      interrelationships, such as shifts in brand preferences. We propose a novel
      correspondence analysis (CA) framework to evaluate departures from symmetry
      in square contingency tables with nominal scales, using a power-divergence-type
      measure. This approach ensures that well-known divergences can also be visualized
      and, regardless of the divergence used, the CA plot consists of two principal
      axes with equal contribution rates. Our visualization method enables the magnitude
      of each category’s departure from symmetry to be assessed by its deviation from
      the origin, while asymmetric relationships between category pairs can be interpreted
      through corresponding triangle areas. Importantly, the scaling of the departures
      from symmetry provided by the measure is independent of sample size, allowing
      for meaningful comparisons and unification of results across different tables.
      This standardization supports broader applicability in empirical studies. We
      also present some considerations from the results of the analysis with actual
      data. Our framework thus offers an effective tool for studying structural shifts
      in categorical data.
    talk_13: Performance of Factor Analytic Mixed Models and Plackett–Luce Models
      for Genotype Ranking in Multi-Environment Trials
    talk_13_speakers:
    - "Jiazhe Lin \U0001F464"
    - Emi Tanaka
    - Fonti Kar
    talk_13_orgs: ANU
    talk_13_abstract: Multi-environment trials are central to plant breeding, providing
      evaluations of genotype performance across locations and time to capture environmental
      variability. The resulting data are often heterogeneous and unbalanced, motivating
      the use of advanced statistical models. The Factor Analytic Linear Mixed Model
      (FALMM) has become a benchmark approach, offering a parsimonious decomposition
      of genotype and genotype-by-environment effects and supporting inference on
      average genotype performance. Alternatively, the Plackett-Luce Model (PLM),
      originally developed in psychology, allows aggregation of environment-specific
      rankings into a global ranking that reflects overall genotype performance. We
      propose a two-stage framework that integrates FALMM and PLM, and through extensive
      simulation, evaluate conditions under which each method provides optimal inference.

Wednesday:
  - name: "Conference Registration"
    id: "registration-wed"
    type: "general"
    start: "08:30"
    end: "09:00"
    location: "Foyer"
    description: "Morning registration and help desk. Assistance for late arrivals and info services."

  - name: "Keynote: Uses of gnm for Generalized (Non)linear Modelling"
    id: "keynote-2"
    type: "talk"
    start: "09:00"
    end: "10:00"
    speaker_imgs: "keynote/heather-turner/headshot_500x500.jpg"
    speakers: "Heather L. Turner"
    speaker_orgs: "University of Warwick"
    location: "Arc Cinema"
    abstract: |
      The R package {gnm} was designed as a unified interface to fit Generalized Nonlinear Models: _generalized_ to handle responses with restricted range and/or a variance that depends on the mean, and _nonlinear_ to allow the predictor for the mean to be nonlinear in its parameters. This framework covers several models that were proposed in the literature and adopted in practice before {gnm} was released, but used to require a mixed bag of specialised software to fit.

      With {gnm} celebrating its 20th birthday this year, it's a good time to review how the package is being used. I'll highlight some of the applications we were aware of when {gnm} was first developed, that remain in common use, and explore more recent applications, particularly in the field of biometrics.

      We'll discover one motivation for using {gnm}, is for the "eliminate" feature that efficiently estimates stratification parameters. This can be useful even when the predictor is linear, as in the case of using conditional Poisson models to analyse case-crossover studies in epidemiology.  We'll also look at two of the packages that have built on {gnm}. The first, {multgee}, uses {gnm} to fit multiplicative interactions for certain correlation structures when modelling categorical data, with applications in public health, agriculture, and psychology. The second, {VFP}, is a more specialised package that uses {gnm} to model the mean-variance relationship for in-vitro diagnostic assays.

      Through these use cases we'll see how different features of {gnm} can be applied, demonstrating the versatility of this software.
    biography: "**Heather L. Turner** is an Associate Professor and EPSRC Research Software Engineering Fellow in the Statistics Department at the University of Warwick, UK. She has over 20 years of experience in the development of statistical code and software, gained through positions in academia, industry, and as a freelance consultant. In research, she has developed a portfolio of R packages for statistical modelling and collaborated on applications in sports, social science and agriculture. In her work with industry, she has specialised in applications in pharmaceutical R&D, with companies including Pfizer, Johnson & Johnson and Roche. Heather is active in community management and engagement among R users and developers. She is on the board of the R Foundation and chairs the R Contribution Working Group (fostering the community of contributors to the R project) and the R Forwards taskforce (widening the participation of under-represented groups in the R community)."

  - name: "Session 5A"
    id: "session-5a"
    type: "talk"
    start: "10:00"
    end: "10:45"
    location: "Arc Cinema"
    talk_1: Scalable finite mixture of regression models for clustering species responses
      in ecology
    talk_1_speakers: "Francis KC Hui \U0001F464"
    talk_1_orgs: The Australian National University
    talk_1_abstract: "When modeling species assemblages in ecology, clustering species
      with similar responses to the environment can facilitate a more parsimonious
      understanding of the assemblage, and improve prediction by borrowing strength
      across species within the same cluster. One statistical method for achieving
      the above is species archetype models (SAMs), a type of finite mixture of regression
      model where species are clustered according to the *shape* of their environmental
      response. \r\n\r\nIn this talk, we introduce approximate and scalable SAMs or
      asSAMs, which overcomes some of the current computational drawbacks in fitting
      SAMs. We show how asSAMs promotes fast uncertainty quantification via bootstrapping,
      along with fast variable selection on archetypal regression coefficients courtesy
      of a sparsity-inducing penalty. Simulation studies and an application to presence-absence
      records of over 230 species from the Great Barrier Reef Seabed biodiversity
      project demonstrate asSAMs can achieve similar to or better estimation, selection,
      and predictive performance than several existing methods in the literature."
    talk_2: 'Elastic Net Regularization for Vector Generalized Linear Models: A Flexible
      Framework for High-Dimensional Biomedical Data'
    talk_2_speakers:
    - "Wenqi Zhao \U0001F464"
    - Dr.Thomas Yee
    talk_2_orgs: University of Auckland, Auckland, New Zealand
    talk_2_abstract: "We introduce a novel implementation of elastic net regularization
      for vector generalized linear models (VGLMs), capable of fitting over 100 family
      functions and designed to support complex, high-dimensional modeling tasks commonly
      encountered in the biosciences. VGLMs extend classical GLMs by accommodating
      multivariate and multi-parameter responses, making them particularly well-suited
      for heterogeneous biomedical data.\r\n\r\nOur method integrates sparse estimation
      techniques—such as lasso, ridge, and their convex combinations—into this broader
      modeling framework, enhancing model interpretability and stability in high-dimensional
      settings. The algorithm is implemented in the vglmnet function within the new
      VGAMplus package for R. It leverages a modified iteratively reweighted least
      squares (IRLS) procedure, combined with pathwise coordinate descent, Karush-Kuhn-Tucker
      (KKT) condition checks, and strong rules for variable screening to ensure computational
      efficiency.\r\n\r\nThis framework supports a wide range of models beyond the
      exponential family, including ordinal, categorical, and zero-inflated distributions
      commonly encountered in fields such as epidemiology, genomics, and clinical
      research. We illustrate the utility of our approach through comparisons with
      existing tools (glmnet, ordinalNet, mpath) and apply it to real-world datasets
      involving survival outcomes, count data, and bivariate binary responses. By
      uniting the structural flexibility of VGLMs with the benefits of regularization,
      our method provides a powerful and scalable solution for modern statistical
      modeling in the biosciences.\r\n\r\n"
    talk_3: Fitting Generalised Linear Mixed Models using Sequential Quadratic Programming
    talk_3_speakers: "Peter Green \U0001F464"
    talk_3_orgs: Tadorna Data Science
    talk_3_abstract: "Finding maximum likelihood estimates in generalised linear mixed
      models (GLMMs) can be difficult due to the often-intractable integral over the
      random effects. \r\n\r\nEnsuring convergence can be tricky, especially in binomial
      GLMMs, and often multiple optimisers and settings need to be tried to get satisfactory
      results. This is exacerbated if you want to use parametric bootstrap for your
      inference. \r\n\r\nSequential quadratic programming (SQP) is a method for solving
      optimisation problems with non-linear constraints. SQP offers an alternative
      option for maximising the Laplace approximation to the GLMM likelihood, bypassing
      the need for an inner penalised iterative reweighted least squares (PIRLS) loop.
      \r\n\r\nThis talk discusses the implementation of SQP for GLMMs and compares
      its performance to other common approaches."
    par_id: 1
    par_total: 2

  - name: "Session 5B"
    id: "session-5b"
    type: "talk"
    start: "10:00"
    end: "10:45"
    location: "Theatrette"
    talk_1: Using a linear mixed model based wavelet transform to model non-smooth
      trends arising from designed experiments
    talk_1_speakers:
    - "Clayton Forknall \U0001F464"
    - Alison Kelly
    - Yoni Nazarathy
    - Ari Verbyla
    talk_1_orgs: Queensland Department of Primary Industries
    talk_1_abstract: The linear mixed model (LMM) representation of the cubic smoothing
      spline is a powerful tool for modelling smooth trends arising from designed
      experiments. However, when trends arising from such experiments are non-smooth,
      meaning that they are characterised by jagged features, spikes and/or regions
      of rapid change approximating discontinuities, smoothing spline techniques prove
      ineffective. A technique that has proven useful for the modelling of non-smooth
      trends is the wavelet transform. Existing methods to incorporate the wavelet
      transform into the LMM framework are varied, but often share a common limitation;
      that is, a reliance on classical wavelet approaches that require observations
      to be equidistant and dyadic ($\log_{2}(n)$ is an integer) in number. More recently,
      second generation wavelet methods have been developed, which overcome the limiting
      constraints imposed by classical wavelet approaches, enabling the wavelet transform
      to be applied to sets of non-equidistant observations, of any number. We present
      a method for the incorporation of these second generation wavelets, namely second
      generation B-spline wavelets, into an LMM framework to facilitate the wavelet
      transform. Furthermore, using the structure implicit in the resulting B-spline
      wavelet basis, we propose extensions to the LMM framework to enable heterogeneity
      of the associated wavelet variance across wavelet scales. This provides a new
      LMM based method which enables the flexible modelling of non-smooth trends arising
      in the conduct of designed experiments. The proposed method is demonstrated
      through application to a data set exhibiting non-smooth characteristics, that
      arises from a designed experiment exploring the proteome of barley malt.
    talk_2: Functional Data Analysis for the Australian Grains Industry
    talk_2_speakers:
    - "Braden J. Thorne \U0001F464"
    - Adam H. Sparks
    talk_2_orgs: Curtin University
    talk_2_abstract: When performing statistical analysis on time series data with
      regular sampling, it is common to apply filtering or windowing methods to reduce
      the complexity of the task. While this process enables many classical approaches,
      it inevitably leads to compression of the full information available. An alternative
      approach is to treat observations as samples of a continuous mathematical function
      and focus analysis on the curves these functions produce rather than the samples.
      This is the underlying idea of functional data analysis, a statistical analysis
      approach that has seen growing attention in recent years. In this talk I will
      offer an introduction to functional data analysis and detail our exploration
      of these methods for application to the grains industry across Australia. Specifically,
      I will present two case studies; estimating charcoal rot prevalence in sixteen
      years of data from in-paddock soybean experimental trials using weather data,
      and analysing frost risk in broadacre crops with varying stubble management
      practices using sensor data.
    talk_3: Bayesian Ordinal Regression for Crop Development and Disease Assessment
    talk_3_speakers:
    - "Zhanglong Cao \U0001F464"
    - Rose Megirian
    - Matthew Nguyen
    - Adam Sparks
    talk_3_orgs: Centre for Crop and Disease Management, Curtin University
    talk_3_abstract: "Accurate assessment of crop development and disease severity
      is essential for informed agronomic decision-making. This study presents a Bayesian
      framework for analysing ordinal data from field trials, including growth scale
      progression and disease ranking scores. Using the brms package in R, we applied
      cumulative logit models to evaluate the effects of sowing depth and treatment
      combinations on cereal growth stages, measured via the Zadok’s scale, across
      two Western Australian sites (Merredin and Wickepin). The same framework is
      being extended to model disease severity scores, demonstrating its versatility
      across categorical biological measurements.\r\nOur workflow incorporates rigorous
      model testing and evaluation, including posterior predictive checks and leave-one-out
      cross-validation (LOO-CV), to ensure robust inference and model fit. Rather
      than relying on p-values from linear mixed models, the Bayesian approach provides
      interpretable probabilities of achieving specific growth stages or disease severity
      levels. This shift enables more nuanced understanding of treatment effects and
      supports decision-making under uncertainty."
    par_id: 2
    par_total: 2

  - name: "Morning Tea"
    id: "morning-tea-wed"
    type: "social"
    start: "10:45"
    end: "11:15"
    speaker: ""
    location: "Gallery"
    abstract: "Light refreshments and beverages."
    description: "Take a break and enjoy some tea or coffee while networking with peers."

  - name: "Invited Session"
    id: "invited-1"
    type: "talk"
    start: "11:15"
    end: "12:45"
    Organiser and Chair: "A/Prof Robert Clark"
    location: "Arc Cinema"
    par_id: 1
    par_total: 2
    talk_1: Surveillance for counterfactual scenarios of invasive species - why it's
      useful and a convenient  way to do it.
    talk_1_speakers:
    - "Andrew P. Robinson \U0001F464"
    - John Kean
    - Melissa Welsh
    - ''
    - Kuo-Szu Chiang
    talk_1_orgs: The University of Melbourne
    talk_1_abstract: "The impact of invasive species is affected by a range of factors,
      many of which can be anticipated in advance – for example, the prevalence of
      host material, climate suitability, the size of affected agricultural resources,
      and so on. One factor that cannot be anticipated in advance is the size of the
      incursion at the time of its detection. Unfortunately, the impact of an incursion
      is tightly tied to its maturity at detection, ranging from a single seed, for
      example, to a 50,000 hectare infestation of plants. \n\nWe propose a simple
      probability model for the detection of an invasive species that can either capture
      or integrate out the consequent uncertainty of the maturity of the incursion.
      We represent the relationship between surveillance and the detection of the
      organism using survival analysis: the detection of the incursion is analogous
      to the survival event; it is a binary occurrence that happens at some point
      in time, and once it has happened it does not happen again.\n\nUnder such a
      model, we can connect the distribution of the size of the infestation at the
      time to detection to the probability of detecting the incursion given that it
      has not already been detected, namely, the hazard function. For example, a popular
      model for the detection of an incursion of size x with number of traps t and
      probability of detecting a single pest $p$ is:\n\n$h(x,t,p) = 1 - (1 - p)^{tx}$
      \n\nAlgebra leads us to a size-at-detection pdf.  Other corrections are also
      applied as needed. The outcome is a pdf that is a function of process parameters,
      enabling straightforward assessment of different surveillance choices. Parameter
      estimates for the distribution can be derived from first principles, field experiments,
      or expert elicitation.\n\nIn this presentation we will derive and demonstrate
      the use of the survival-based incursion size at detection pdf and discuss its
      implications and challenges."
    talk_2: Optimal allocation of resources between control and surveillance for complex
      eradication scenarios
    talk_2_speakers:
    - "Mahdi Parsa \U0001F464"
    - "Belinda Parsa \U0001F464"
    talk_2_orgs: Department of Agriculture, Fisheries and Forestry
    talk_2_abstract: |-
      Effective eradication of invasive species over large areas requires strategic
      allocation of resources between control measures and surveillance activities. This study
      presents an analytical Bayesian framework that integrates stochastic modelling and explicit
      measures of uncertainty to guide decisions in complex eradication scenarios. By applying
      Shannon entropy to quantify uncertainty and incorporating the expected value of perfect
      information (EVPI), the framework identifies conditions under which investment into control
      or surveillance becomes worthwhile. Findings show that strategies which hedge against
      uncertainty can markedly improve the robustness of eradication outcomes with only marginal increases in expected costs. This approach offers practical tools for designing more cost-effective and reliable eradication programs and for prioritising data collection to reduce uncertainty where it has the greatest impact.
    talk_3: Inferring the rate of undetected contamination using random effects modelling
      of biosecurity screening histories
    talk_3_speakers:
    - "Sumonkanti Das \U0001F464"
    - "Robert Das \U0001F464"
    talk_3_orgs: Australian National University
    talk_3_abstract: |-
      Group testing plays a vital role in biosecurity operations worldwide, particularly in minimising the risk of introducing exotic pests, contaminants, and pathogens through imported agricultural products. A common screening strategy involves pooling items from consignments and testing each group for contamination presence, with consignments typically rejected if any group tests positive. Although screening designs often target a high probability of detection assuming a fixed minimum prevalence, analysing the historical results of these tests to infer the extent of contamination in non-rejected consignments (referred to as leakage) is less common.

      This study advances censored beta-binomial (BB) models to address contamination risk in frozen seafood imports into Australia, incorporating imperfect tests. Motivated by the characteristics of our case study, we develop a new class of BB models that impose a minimum positive consignment propensity threshold, capturing scenarios where contamination is either absent or exceeds a known minimum level. To fit these models, we propose a Metropolis-Hastings (MH) algorithm conditioned on prior distributions for sensitivity and specificity, allowing efficient estimation of quantities related to contamination levels. We analyse historical testing data under multiple scenarios using the proposed MH algorithm, yielding novel insights into both contamination risk and leakage.

      Finally, we use model-based simulations to communicate risk levels, providing key insights into potential undetected contamination.
    talk_4: 'Optimal sampling in border biosecurity: Application to skip-lot sampling'
    talk_4_speakers: "Raphael Trouve \U0001F464"
    talk_4_orgs: The University of Melbourne
    talk_4_abstract: |-
      Border biosecurity faces mounting pressure from increasing global trade, requiring cost-effective inspection strategies to reduce the risk of importing pest and diseases. Current international standards recommend inspecting all incoming consignments (full census) with fixed sample sizes (e.g., 600 units) for high-risk pathways, but this may be overkill for lower-risk pathways with established compliance records. When should agencies use skip-lot sampling (SLS, sometimes called continuous sampling plan), which adaptively reduces inspections based on recent compliance history, over full census inspection?

      We developed a propagule pressure equation for SLS in overdispersed pathways and used Lagrange multipliers to derive a solution. Results show the choice depends on pathway overdispersion, sampling costs, and budget constraints. Optimal sample sizes are typically smaller than current recommendations, with better returns from inspecting a larger proportion of consignments rather than larger samples per consignment. This framework provides biosecurity agencies with data-driven guidance for implementing adaptive sampling strategies.

  - name: "Session 6"
    id: "session-6"
    type: "talk"
    start: "11:15"
    end: "12:45"
    location: "Theatrette"
    par_id: 2
    par_total: 2
    talk_1: Estimating extinction time from the fossil record using regression inversion
    talk_1_speakers:
    - "David I. Warton \U0001F464"
    - Victor Tsang
    talk_1_orgs: UNSW Sydney
    talk_1_abstract: "An important problem in palaeoecology is estimating the extinction
      or invasion time of a species from the fossil record - whether because this
      is of interest in and of itself, or in order to understand the causes of extinctions
      and invasions, for which we need to know when they actually happened. There
      are two main sources of error to contend with - sampling error (because the
      last time you see a species need not be the last time it was there) and measurement
      error (dating specimens, usually well known). The paleobiology literature typically
      ignores one or other of these sources of error, leading to bias and underestimation
      of uncertainty to an extent that is often qualitatively important. \r\n\r\nThe
      problem is surprisingly difficult to address statistically, because while standard
      regularity conditions are technically satisfied, we are typically close to a
      boundary where they break down, and hence standard asymptotic approaches to
      inference typically perform poorly in practice. We propose using a novel method,
      which we call regression inversion, for exact inference, and we apply this technique
      to a compound uniform-truncated t (CUTT) model for fossil data. We show via
      simulation that this approach leads to unbiased estimators, and accurate interval
      inference, in contrast to its competitors. We show how to check the CUTT assumption
      visually, and provide software to apply all of the above in the reginv package."
    talk_2: The performance of Yu and Hoff's confidence intervals for treatment means
      in a one-way layout
    talk_2_speakers: "Paul Kabaila \U0001F464"
    talk_2_orgs: La Trobe University
    talk_2_abstract: "Consider a one-way layout and suppose that we have uncertain
      prior information that the treatment population means are equal or close to
      equal. Yu & Hoff (2018) extended the \"tail method\" for finding a confidence
      interval for a scalar parameter of interest that has (a) specified coverage
      probability and (b) relatively small expected length when this parameter takes
      values in some given set. They used this extension to find confidence intervals
      for these treatment means that have (a) specified coverage probability individually
      and (b) relatively small expected lengths when this uncertain prior information
      happens to be correct. They assessed the expected lengths of these confidence
      intervals, over the whole parameter space, using a semi-Bayesian analysis. I
      describe a revealing alternative assessment of these expected lengths using
      a fully frequentist analysis.\r\n\r\nYu, C. & Hoff, P. (2018) Adaptive multigroup
      confidence intervals with coverage. *Biometrika*, 105, 319-335."
    talk_3: Rate-optimal sparse gamma scale mixture detection
    talk_3_speakers:
    - "Michael Stewart \U0001F464"
    - Qikun Chen
    talk_3_orgs: The University of Sydney
    talk_3_abstract: We consider a model where observations from a known gamma distribution
      are possibly contaminated by observations from another gamma distribution with
      the same shape but a different mean. Such a model has been considered for times
      between neurotransmitter releases based on a Markov chain with amalgamated indistinguishable
      states. We focus on the case where the contaminating component occurs rarely,
      the so-called sparse gamma scale mixture detection problem. Due to the irregularity
      of such models theoretical results concerning detectability bounds are non-standard.
      Nonetheless in recent years a body of theory has been developed which covers
      the case when the mean of the unknown contaminating component is smaller than
      the null mean, but not when it is larger. We present some recent results filling
      this gap in the literature. In particular we describe a test which attains the
      optimal rate of convergence in various local alternative scenarios which is
      a Bonferroni-type test combining three different tests.
    talk_4: Extension of the corrected score estimator in a Poisson regression model
      with a measurement error
    talk_4_speakers:
    - "Kentarou Wada \U0001F464"
    - T. Kurosawa
    talk_4_orgs: Tokyo University of Science
    talk_4_abstract: Kukush et al. (2004) discussed the bias of the naive estimator
      for the regression parameters in a Poisson regression model with a measurement
      error for the case where the explanatory variable and measurement error follow
      normal distributions. Wada and Kurosawa (2023) proposed the corrected naive
      (CN) estimator as a consistent estimator for a Poisson regression model with
      a measurement error for the case where the explanatory variable and measurement
      error are general distributions. The CN estimator directly calibrates the bias
      of the naive estimator. The CN estimator is given by the solution of the estimation
      equation of the Poisson regression model under the error-in-variables framework.
      However, the CN estimator does not always have an explicit expression under
      the condition that the explanatory variable and measurement error follow general
      distributions. On the other hand, Kukush et al. (2004) considered the corrected
      score (CS) estimator as a consistent estimator for the true parameter of the
      Poisson regression model with a measurement error. In this research, we extend
      the CS estimator to the case where the explanatory variable and measurement
      error are general distributions. The new estimator can be applied for the condition
      that the CN estimator does not have an explicit expression. As illustrative
      examples, we give simulation studies to verify the effectiveness of the new
      estimator.


  - name: "Lunch"
    id: "lunch-wed"
    type: "social"
    start: "12:45"
    end: "13:45"
    location: "Gallery"

  - name: "Social Activities"
    id: "social-wed"
    type: "social"
    start: "13:45"
    end: "17:00"
    speaker: "Organizing Committee"
    location: "Outdoor Grounds"
    description: "Networking games and tours. Fun and informal activities for relaxing and networking."

Thursday:
  - name: "Conference Registration"
    id: "registration-thu"
    type: "general"
    start: "08:30"
    end: "09:00"
    location: "Foyer"
    description: "Morning registration and info desk. Final registration opportunity and general assistance available."

  - name: "Keynote: Modularizing Biometric Models Facilitates Multistage Computing"
    id: "keynote-3"
    type: "talk"
    start: "09:00"
    end: "10:00"
    speakers: Mevin B. Hooten
    speaker_imgs: "keynote/mevin-hooten/Hooten_photo.jpg"
    speaker_orgs: "The University of Texas at Austin"
    location: "Arc Cinema"
    abstract: Bayesian modeling has become invaluable in biometrics.  It allows
      us to formally consider unobserved processes while accommodating uncertainty
      about data collection and our understanding of biological and ecological mechanisms.  Several
      excellent software packages are available for fitting Bayesian models to data
      and are being applied every day to analyze biometric data.  These methods allow
      us to answer questions using data in ways that has never before been possible.  The
      adoption of Bayesian methods has led to bigger models necessary to answer tough
      questions using large and varied data sets.  Bigger models and data sets lead
      to computing bottlenecks.  Fortunately, a solution to Bayesian computing roadblocks
      sits in plain sight.  The structure of Bayesian models allows us to rearrange
      them so that we can perform computing in stages.  We can break big models into
      pieces, fit them separately, and then recombine them in later computing stages.  Recursive
      Bayesian approaches can save us time by leveraging the parallel architecture
      of modern computers.  A modular perspective allows us to see Bayesian models
      in a way that facilitates multistage computing.  I will demonstrate the procedure
      with a set of biometric examples.  These include geostatistical models in marine
      science, capture-recapture models for abundance estimation, and spatial point
      process models for species distributions.
    biography: |
      **Mevin Hooten** is a Professor in Statistics and Data Sciences at The University of Texas at Austin. His research focuses on developing statistical methodology for ecological and environmental applications that involve spatial and spatio-temporal data. He is a Fellow of the American Statistical Association (ASA) and received the Distinguished Achievement Award from the ASA section on statistics and the environment. He has authored over 185 papers and 3 textbooks and serves as Associate Editor for Biometrics, Environmetrics, and JABES.

  - name: "Session 7A"
    id: "session-7a"
    type: "talk"
    start: "10:00"
    end: "10:45"
    speaker: "TBA"
    location: "Arc Cinema"
    abstract: "TBA"
    description: "TBA"
    par_id: 1
    par_total: 2

  - name: "Session 7B"
    id: "session-7b"
    type: "talk"
    start: "10:00"
    end: "10:45"
    speaker: "TBA"
    location: "Theatrette"
    abstract: "TBA"
    description: "TBA"
    par_id: 2
    par_total: 2

  - name: "Morning Tea"
    id: "morning-tea-thu"
    type: "social"
    start: "10:45"
    end: "11:15"
    speaker: ""
    location: "Gallery"
    abstract: "Light refreshments and beverages."
    description: "Take a break and enjoy some tea or coffee while networking with peers."

  - name: "Invited Session"
    id: "invited-3"
    type: "talk"
    start: "11:15"
    end: "12:45"
    speaker: "TBA"
    location: "Arc Cinema"
    par_id: 1
    par_total: 2
    abstract: "TBA"
    description: "TBA"

  - name: "Session 8"
    id: "session-8"
    type: "talk"
    start: "11:15"
    end: "12:45"
    speaker: "TBA"
    location: "Theatrette"
    par_id: 2
    par_total: 2
    abstract: "TBA"
    description: "TBA"


  - name: "Lunch"
    id: "lunch-thu"
    type: "social"
    start: "12:45"
    end: "13:45"
    location: "Gallery"
    par_id: 1
    par_total: 2

  - name: "AGM"
    id: "agm"
    type: "general"
    start: "12:45"
    end: "13:45"
    location: "Gallery"
    par_id: 2
    par_total: 2

  - name: "Session 9A"
    id: "session-9a"
    type: "talk"
    start: "13:45"
    end: "15:00"
    speaker: "TBA"
    location: "Arc Cinema"
    par_id: 1
    par_total: 2
    abstract: "TBA"
    description: "TBA"

  - name: "Session 9B"
    id: "session-9b"
    type: "talk"
    start: "13:45"
    end: "15:00"
    speaker: "TBA"
    location: "Theatrette"
    par_id: 2
    par_total: 2
    abstract: "TBA"
    description: "TBA"

  - name: "Afternoon Tea"
    id: "afternoon-tea-thu"
    type: "social"
    start: "15:00"
    end: "15:30"
    location: "Gallery"
    abstract: "Afternoon refreshments."
    description: "Light snacks and drinks served."

  - name: "Session 10A"
    id: "session-10a"
    type: "talk"
    start: "15:30"
    end: "16:45"
    speaker: "Various"
    location: "Arc Cinema"
    abstract: "TBA"
    description: "TBA"
    par_id: 1
    par_total: 2

  - name: "Session 10B"
    id: "session-10b"
    type: "talk"
    start: "15:30"
    end: "16:45"
    speaker: "Various"
    location: "Theaterette"
    abstract: "TBA"
    description: "TBA"
    par_id: 2
    par_total: 2

  - name: "Conference Dinner"
    id: "dinner"
    type: "social"
    start: "18:00"
    end: "20:00"
    speaker: "Dinner Host"
    location: "Banquet Hall"
    abstract: "Celebratory closing dinner."
    description: "Enjoy an evening of fine dining, speeches, and socializing."

Friday:
  - name: "Conference Registration"
    id: "registration-fri"
    type: "general"
    start: "08:30"
    end: "09:00"
    location: "Foyer"
    description: "Morning registration and info desk. Final registration opportunity and general assistance available."

  - name: "Keynote 4"
    id: "keynote-4"
    type: "talk"
    start: "09:00"
    end: "10:00"
    speaker: "TBA"
    location: "Arc Cinema"
    abstract: "TBA"
    description: "TBA"

  - name: "Session 11A"
    id: "session-11a"
    type: "talk"
    start: "10:00"
    end: "10:45"
    speaker: "TBA"
    location: "Arc Cinema"
    abstract: "TBA"
    description: "TBA"
    par_id: 1
    par_total: 2

  - name: "Session 11B"
    id: "session-11b"
    type: "talk"
    start: "10:00"
    end: "10:45"
    speaker: "TBA"
    location: "Theatrette"
    abstract: "TBA"
    description: "TBA"
    par_id: 2
    par_total: 2

  - name: "Morning Tea"
    id: "morning-tea-fri"
    type: "social"
    start: "10:45"
    end: "11:15"
    speaker: ""
    location: "Gallery"
    abstract: "Light refreshments and beverages."
    description: "Take a break and enjoy some tea or coffee while networking with peers."

  - name: "Keynote 5"
    id: "keynote-5"
    type: "talk"
    start: "11:15"
    end: "12:00"
    speaker: "TBA"
    location: "Arc Cinema"
    abstract: "TBA"
    description: "TBA"

  - name: "Closing Ceremony"
    id: "closing"
    type: "general"
    start: "12:00"
    end: "12:45"
    speaker: "Organizing Committee"
    location: "Arc Cinema"
    abstract: "Wrap-up and acknowledgements."
    description: "Thanks to participants, sponsors, and staff. Final remarks and farewell."


  - name: "Lunch"
    id: "lunch-fri"
    type: "social"
    start: "12:45"
    end: "13:45"
    speaker: ""
    location: "Dining Hall"
    abstract: "Farewell lunch."
    description: "Join peers one last time over lunch."

