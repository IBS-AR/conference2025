Tuesday:
  0A:
    talk_1: On Finding Good Experiments
    talk_1_speakers: "Cheng Soon Ong \U0001F464"
    talk_1_orgs: CSIRO and ANU
    talk_1_abstract: One of the key choices we have as scientists is to design informative
      experiments. With computational methods like AI promising accurate predictions,
      we revisit the question of adaptively designing new measurements that take previous
      data into account. Using examples from genomics, we illustrate some recent ideas
      on using machine learning to recommend experiments. Then we discuss potential
      impacts on choosing measurements in spatiotemporal problems. We conclude by
      outlining some opportunities and challenges of including machine learning in
      the scientific discovery process.
  1A:
    talk_1: Data-Adaptive Automatic Threshold Calibration for Stability Selection
    talk_1_speakers:
    - "Martin Huang \U0001F464"
    - Samuel Muller
    - Garth Tarr
    talk_1_orgs: The University of Sydney
    talk_1_abstract: Stability selection has gained popularity as a method for enhancing
      the performance of variable selection algorithms while controlling false discovery
      rates. However, achieving these desirable properties depends on correctly specifying
      the stable threshold parameter, which can be challenging. An arbitrary choice
      of this parameter can substantially alter the set of selected variables, as
      the variables' selection probabilities are inherently data-dependent. To address
      this issue, we propose Exclusion Automatic Threshold Selection (EATS), a data-adaptive
      algorithm that streamlines stability selection by automating the threshold specification
      process. EATS initially filters out potential noise variables using an exclusion
      probability threshold, derived from applying stability selection to a randomly
      shuffled version of the dataset. Following this, EATS selects the stable threshold
      parameter using the elbow method, balancing the marginal utility of including
      additional variables against the risk of selecting superfluous variables. We
      evaluate our approach through an extensive simulation study, benchmarking across
      commonly used variable selection algorithms and static stable threshold values.
    talk_2: Variable Selection in a Joint Model for Huntington's Disease Data
    talk_2_speakers:
    - "Rajan Shankar \U0001F464"
    - Tanya Garcia
    - Garth Tarr
    talk_2_orgs: The University of Sydney
    talk_2_abstract: Huntington's disease is a neurodegenerative disease caused by
      a defective Huntingtin gene, with symptoms that progressively worsen and eventually
      lead to a clinical diagnosis. Identifying the clinical and demographic factors
      that influence symptom severity and time-to-diagnosis is critical for understanding
      disease progression so that early-intervention strategies can be timely implemented.
      We propose a joint model to relate symptom severity $y$ and time-to-diagnosis
      $x$, conditional on clinical and demographic predictor variables $\mathbf{z}$.
      However, it may be that certain predictor variables are important for $y$ but
      not for $x$ and vice-versa, so we use regularisation techniques to select different
      sets of predictor variables for $y$ and $x$. Since $x$ is a time-to-event variable,
      there is the added challenge that many of its values are right-censored due
      to individuals who did not develop the disease during the study. Therefore,
      to fit the joint model, we apply the expectation-maximisation (EM) algorithm
      to alternate between parameter estimation and imputation of the right-censored
      values until convergence. We demonstrate our method on Huntington's disease
      patient data, showcasing how users can choose appropriate values for the regularisation
      tuning parameters.
    talk_3: "StableMate: a regression framework for selecting stable predictors across
      heterogeneous data environments\r\n\r\n"
    talk_3_speakers:
    - "Yidi Deng \U0001F464"
    - Jiadong Mao
    - Jarny Choi
    - Kim-Anh L√™ Cao
    talk_3_orgs: Melbourne Integrative Genomics; Department of Anatomy and Physiology,
      The University of Melbourne; Research School of Finance, Actuarial Studies and
      Statistics, The Australian National University
    talk_3_abstract: "Inferring reproducible relationships between biological variables
      remains a challenge in the statistical analysis of omics data where p > 10,000
      and n < 500. Methods that identify statistical associations lack interpretability
      or reproducibility. We can address these limitations by inferring stable associations
      that are robust to external perturbation on the data. Stable associations can
      be an implication in causality since causal relationships are necessarily stable
      in some sense (Pearl et al. 2009). Unstable associations can also be of interests
      in certain biological applications to study functional heterogeneity in a biological
      system.\r\n\r\nWe developed a new regression framework, StableMate based on
      the concept of stabilised regression (SR), which utilise heterogenous data to
      enforce stability (Pfister et al. 2021). Given datasets generated from different
      environments, such as experiments or disease states, StableMate 1. identifies
      stable predictors with consistent functional dependency with the response across
      environments. 2. builds a robust regression model with stable predictors to
      enforce generalisable prediction in unseen environments. The ultimate aim is
      to build selection ensembles. However, unlike SR that selects stable predictors
      by performing stability tests on every possible predictor subset, StableMate
      optimizes efficiency with a greedy search based on our improved stochastic stepwise
      selection algorithm. In a simulation study, we show that StableMate outperformed
      SR for both variable selection and prediction and significantly reduces running
      time. In three case studies of cancer with different omics data types, we show
      that StableMate can also address a wide range of biological questions.\r\n"
  1B:
    talk_1: Estimating abundance in small populations using pedigree reconstruction
    talk_1_speakers:
    - "Sarah Croft \U0001F464"
    - Jamie Sanderlin
    - Michael Black
    - Richard Barker
    - Matthew Schofield
    talk_1_orgs: University of Otago
    talk_1_abstract: "Accurate measures of abundance are essential for successful
      monitoring of animal populations,\r\nand for assessing the efficacy of conservation
      interventions. In previous work, genotypic information\r\nhas been incorporated
      into Mark-Recapture models to enable the identification of individuals, as well\r\nas
      determination of kinship between observed individuals in Close-Kin Mark-Recapture
      (CKMR)\r\nmodels. Generally, CKMR models make large sample assumptions limiting
      their application to\r\nmany endangered and at-risk species.\r\nWe have developed
      Bayesian methodology to estimate population abundance and dynamics for\r\nsmall,
      isolated populations using pedigree reconstruction. The true underlying pedigree
      completely\r\ndescribes the abundance and population structure over time, however
      the true relationships between\r\nindividuals in wild populations are rarely
      known. Given a set of observed genotypes, along with\r\nsupplementary data,
      our methodology is able to successfully reconstruct the pedigree without the\r\nneed
      for large sample assumptions. Prior knowledge of the mating structure and reproductive\r\ndynamics
      of the population can also be incorporated in the model. In this talk I will
      present\r\nour pedigree reconstruction approach for population estimation using
      dead recovery data, and will\r\ndiscuss the challenges associated with the full
      pedigree approach."
    talk_2: Accounting for heterogeneous detection rates when inferring eradication
      of an invasive species
    talk_2_speakers:
    - "Sean A. Martin \U0001F464"
    - Leah South
    - Zachary T. Carter
    - Michael Bode
    talk_2_orgs: QUT School of Mathematics/SAEF
    talk_2_abstract: "Eradication of invasive species from islands is an expensive
      but often crucial activity for maintaining the ecosystems of these islands.
      A key challenge in limiting the costs of an eradication campaign is knowing
      the size of the remaining population of invasive animals; this information informs
      both ongoing resource planning and when to end a campaign. Campaign progress
      is measured by the decline in detections of the target species. However, detection
      of a population is imperfect and highly variable, obscuring the true rate of
      decline. As a result, uncertainty arises when detections cease - have all individuals
      truly been eradicated, or do some remain undetected? Statistical models have
      been developed to infer eradication success based on a record of detections
      (sighting records), however, most models ignore natural heterogeneity in detectability.
      \r\nOur work examines the influence of variance in detection rates on the false
      positive and false negative error rates of hypothetical eradication campaigns
      and relates this to overall costs of island conservation. We use a combination
      of Bayesian likelihood and likelihood-free models to overcome simultaneous inference
      limits on abundance and detectability within this highly stochastic study system."
    talk_3: Zero-inflated Tweedie distribution for abundance of rare ecological species
    talk_3_speakers: "Nokuthaba Sibanda \U0001F464"
    talk_3_orgs: Victoria University of Wellington
    talk_3_abstract: "Abundance data for rare species can be extremely zero-inflated,
      where percentage of zeros can be over 90%. This poses a challenge even for the
      standard Tweedie model which naturally allows for a probability mass at zero
      with continuous non-negative values. We investigate use of a zero-inflated Tweedie
      distribution when modelling non-negative continuous abundance values with zeros
      for rare species. Despite their significance, research on zero-inflated models
      has predominantly focused on count models such as zero-inflated Poisson or negative
      binomial regressions, with only recent studies exploring zero-inflated Tweedie
      models in insurance claims (Zhou, Qian, and Yang 2022; Gu2024; So and Valdez
      2024; So and Deng 2025).\r\n\r\nThe zero-inflated Tweedie model uses a mixture
      model approach that integrates a Tweedie model with a binary model to distinguish
      between excess zeros - those resulting from an independent process, and true
      zeros those resulting from the Tweedie model itself. We use a Bayesian approach
      to estimate the model parameters. We model means of the Tweedie model using
      a log-link function with covariates and unobserved random effects. The spatial
      association between observations is accounted for using a conditionally auto-regressive
      prior."
  2A:
    talk_1: Extending Spatial Capture-Recapture with the Hawkes Process
    talk_1_speakers:
    - "Alec B. M. van Helsdingen \U0001F464"
    - Charlotte M. Jones-Todd
    talk_1_orgs: University of Auckland
    talk_1_abstract: "Spatial capture-recapture (SCR) is a well-established method
      used to estimate animal population size from animal sighting or trapping data.
      Standard SCR methods assume animal movements are independent and consequently
      cannot incorporate site fidelity (attachment to a particular region) nor the
      temporal correlation of an animal’s location. Recent work has sought to solve
      these issues by explicitly modelling animal movement.\r\n\r\nIn this talk we
      propose an alternative solution for camera trapping surveys based on a multivariate
      self-exciting Hawkes process. Here the rates of detection of a given animal
      at a given camera are a function of not only the location and its proximity
      to the animal’s activity center, but also where and when the animal was most
      recently detected.\r\n\r\nThrough a mixture of Gaussian distributions, our model
      expects more detections closer in space to the last detection, and reduces to
      SCR when an animal is yet to be detected. This formulation, we believe, better
      reflects animal behaviour because shortly after detection, we expect to see
      an individual close to where it was last seen. Thus, our model allows us to
      account for both site fidelity and the inherent temporal correlation in detections
      that have not previously been accounted for in SCR-type models.\r\n\r\nIn this
      talk, I will 1) give an overview of Self-Exciting Spatial Capture-Recapture
      (SESCR) models, and 2) demonstrate the additional inference that can be drawn
      from such models and 3) apply the framework using a few case studies to compare
      traditional SCR and SESCR."
    talk_2: A Test for Detecting Multiple Clusters with Hotspot Spatial Properties
    talk_2_speakers:
    - "Kunihiko Takahashi \U0001F464"
    - Hideyasu Shimadzu
    talk_2_orgs: Institute of Science Tokyo
    talk_2_abstract: Various statistical tests have been widely used in spatial epidemiology
      to investigate regional patterns in disease occurrence, particularly to assess
      whether disease risk is significantly elevated in specific areas compared to
      neighboring regions or adjacent time periods. One such method is the cluster
      detection test (CDT), which identifies non-random spatial distributions of diseases
      and highlights high-prevalence regions without prior assumptions. Among CDT
      methods, scan statistics are compelling and use a maximum likelihood framework
      to search across spatial and/or temporal windows for potential clusters. Examples
      include Kulldorff’s circular scan statistic and the flexibly shaped scan statistic
      by Tango and Takahashi. More recently, Takahashi and Shimadzu proposed a scan-based
      method that simultaneously detects multiple clusters by integrating generalized
      linear models and an information criterion to determine the optimal number of
      clusters. Traditional scan-based tests often assume that disease risk is uniformly
      elevated within a single cluster. However, they may mistakenly combine multiple
      adjacent hotspots—each with potentially different risk levels—into one, thereby
      masking meaningful spatial heterogeneity. In this study, we propose a new test
      procedure that more accurately identifies adjacent hotspot clusters as distinct
      entities. Our approach enhances the scan statistic framework by incorporating
      Cochran’s Q-statistic to assess heterogeneity within clusters. We demonstrate
      the effectiveness of the proposed method through real-world applications and
      compare its performance with conventional scan-based tests.
    talk_3: Outlier-robust estimation of state-space models using a penalised approach
    talk_3_speakers:
    - "Garth Tarr \U0001F464"
    - Rajan Shankar
    - Ines Wilms
    - Jakob Raymaekers
    talk_3_orgs: The University of Sydney
    talk_3_abstract: State-space models are a broad class of statistical models for
      time-varying data. The Gaussian distributional assumption on the disturbances
      in the model leads to poor parameter estimates in the presence of additive outliers.
      Whilst there are ways to mitigate the influence of outliers via traditional
      robust estimation methods such as M-estimation, this issue is approached from
      a more modern perspective that utilises penalisation. A shift parameter is introduced
      at each timepoint, with the goal being that outliers receive a non-zero shift
      parameter while clean timepoints receive a zero shift parameter after estimation.
      The vector of shift parameters is penalised to ensure that not all shift parameters
      are trivially non-zero. Apart from making it feasible to fit accurate and reliable
      time series models in the presence of additive outliers, other benefits of this
      approach include automatic outlier flagging and visual diagnostics to provide
      researchers and practitioners with better insights into the outlier structure
      of their data. We will demonstrate the utility of this method on animal tracking
      data.
    talk_4: Disease cluster detection via functional additive models incorporating
      spatial correlation
    talk_4_speakers:
    - "Michio Yamamoto \U0001F464"
    - Tatsuhiko Anzai
    - Kunihiko Takahashi
    talk_4_orgs: The University of Osaka / RIKEN AIP / Shiga University
    talk_4_abstract: Detecting spatial clusters of diseases is crucial for understanding
      disease patterns and developing effective prevention and treatment strategies.
      Spatial scan statistics are powerful tools for detecting spatial clusters with
      a variable scanning window size. If covariates are related to an outcome and
      not geographically randomly distributed, searching for spatial clusters may
      require adjusting for the covariates. In addition, spatial correlation in the
      outcome, which is often overlooked during cluster detection, can affect the
      results. In this study, we propose a new spatial scan statistic that handles
      multiple functional covariates indicating past information over time and the
      spatial correlation of the outcome. Our method flexibly models these factors
      in the framework of functional additive models. We develop an optimization algorithm
      to estimate the model parameters for the normal outcome case. A simulation study
      and real data analysis indicate that the proposed method can detect disease
      clusters despite longitudinal covariates and spatial correlations compared to
      existing methods.
  2B:
    talk_1: Group Sampling with Imperfect Testing for Biosecurity Applications
    talk_1_speakers:
    - "Adele Jackson \U0001F464"
    - Belinda Barnes
    - Mahdi Parsa
    talk_1_orgs: ABARES
    talk_1_abstract: Group sampling, also known as pooled or batch sampling, is a
      standard technique in biological sciences and the health sector to use limited
      resources efficiently. Common objectives include detecting pest species and
      inferring prevalence of disease in communities, livestock or wildlife. The purpose
      of this paper is to support the design of robust group sampling strategies when
      testing processes are imperfect. We formulate analytical distributions and statistics
      for grouped hypergeometric sampling and its binomial approximation that incorporate
      a variety of types of imperfect test. These include tests that respond to the
      presence or absence of contaminated material in a group, as well as PCR and
      serological testing processes where sensitivity depends on the number of contaminated
      items in each group. We also formulate the Hellinger information of a sampling
      scheme, which allows us to develop group sampling design strategies that increase
      the accuracy of inferred prevalence. This is an essential component in decision-making
      during outbreaks of disease and in operational biosecurity applications. Based
      on this work, we estimate leakage through a grouped sampling scheme and show
      how accounting for leakage can alter sampling strategies and improve risk management
      decisions.
    talk_2: Koala Distirbution and Abundance
    talk_2_speakers:
    - "Scott D. Foster \U0001F464"
    - Scott D. Foster
    - Wen-Hsi Yang
    - Peter Caley
    - John McEvoy
    - David Ramsey
    - Drew Terasaki Hart
    - David E. Uribe-Rivera
    - Romane H. Cristescu
    - Eric Vanderdys
    - Adam McKeown
    - Corey J. A. Bradshaw
    - Emma Lawrence
    - Andrew Hoskins
    talk_2_orgs: Data61, CSIRO
    talk_2_abstract: The koala (*Phascolarctos cinereus*) is a well-known and studied
      Australian marsupial, but the species presents a complex case for conservation.
      Currently, most koala conservation efforts focus on local-scale population estimates,
      which are often based on expert opinion or anecdotal evidence, and either precede
      or ignore most (or all) available data. In contrast, conservation listing advice
      and associated recovery plans require population estimates at the species-range
      scale. A data-driven, national-scale population estimate including all available
      information will therefore guide effective management of koala populations by
      providing high-quality and objective information. To this end, we have designed
      nationally consistent survey, implemented it (with the aid of partners) and
      analysed the resulting data (and others). The design uses recently developed
      techniques (clustered spatially-balanced designs) whilst the analysis uses emerging
      models that incorporate multiple data types (e.g. point process, binary and
      count) that are often-enough collected using different equipment, protocols
      and staff. Integrated species distribution models (ISDMs) have, at their heart,
      a simple point process but this simplicity still allows for some complexity
      in terms of how different types of data can inform the point process. The model,
      when fitted to koala data, indicates that the distribution of koalas is patchy
      throughout much of eastern Australia. It also infers that there are more koalas
      than previously guessed. Our estimates provide unprecedented evidence to support
      nationally consistent and spatially explicit decision-making for koala conservation,
      and do so with relevant measures of uncertainty. 
    talk_3: 'Speed: An R package for Spatially Efficient Experimental Designs'
    talk_3_speakers:
    - "Sam Rogers \U0001F464"
    - Wasin Pipattungsakul
    - Julian Taylor
    talk_3_orgs: University of Adelaide
    talk_3_abstract: "Agricultural field trials are typically designed using robust
      statistical randomisation, and best practice agricultural field trials also
      consider the spatial layouts of the experiment, and how that interacts with
      the treatments of interest. However, software providing access to spatially
      optimised experimental designs for field trials is not readily available and
      can be difficult for new users to get started with due to sparse documentation.\r\n\r\nIn
      this talk, we discuss the newly developed *speed* R package which provides easy
      access to a fully open-source package with comprehensive documentation to enable
      the design of spatially optimal experiments. The package offers model-free spatially
      optimal designs via a simulated annealing optimisation algorithm. It can produce
      a spatially optimal version of many types of experimental designs commonly used
      in agricultural research as well as many more complex designs such as incomplete
      block designs and partially replicated designs. It provides multiple objective
      functions out of the box, with the additional flexibility to choose or enable
      custom optimisation metrics, depending on the objective of the researcher. It
      also provides some helper functions for plotting and evaluating experimental
      designs either produced via *speed* or alternative design packages.\r\n\r\nTo
      demonstrate the package's capabilities, we present spatially optimal designs
      for challenging scenarios including two-dimensional blocking and partially replicated
      designs. We also show that speed is significantly faster compared to alternative
      software."
    talk_4: Running Human Subject Experiments via Online Crowdsourcing
    talk_4_speakers: "Patrick Li \U0001F464"
    talk_4_orgs: Australian National University
    talk_4_abstract: Crowdsourcing platforms such as Amazon Mechanical Turk and Prolific
      provide scalable and accessible tools for running online human subject experiments.
      This talk offers an overview of how to design, set up, and manage such studies
      effectively. Drawing on experience from multiple projects, I will walk through
      the key steps for obtaining ethics approval, compare platform workflows, and
      discuss considerations for participant recruitment, screening, task design,
      quality control, cost estimation, and common pitfalls. This session is intended
      for researchers planning to conduct behavioural, perceptual, or decision-making
      experiments, as well as those developing data annotation pipelines for machine
      learning or applied research.
Wednesday:
  0A:
    talk_1: Uses of gnm for Generalized (Non)linear Modelling
    talk_1_speakers: "Heather L. Turner \U0001F464"
    talk_1_orgs: University of Warwick
    talk_1_abstract: "The R package {gnm} was designed as a unified interface to fit
      Generalized Nonlinear Models: _generalized_ to handle responses with restricted
      range and/or a variance that depends on the mean, and _nonlinear_ to allow the
      predictor for the mean to be nonlinear in its parameters. This framework covers
      several models that were proposed in the literature and adopted in practice
      before {gnm} was released, but used to require a mixed bag of specialised software
      to fit.\n\nWith {gnm} celebrating its 20th birthday this year, it's a good time
      to review how the package is being used. I'll highlight some of the applications
      we were aware of when {gnm} was first developed, that remain in common use,
      and explore more recent applications, particularly in the field of biometrics.\n\nWe'll
      discover one motivation for using {gnm}, is for the \"eliminate\" feature that
      efficiently estimates stratification parameters. This can be useful even when
      the predictor is linear, as in the case of using conditional Poisson models
      to analyse case-crossover studies in epidemiology.  We'll also look at two of
      the packages that have built on {gnm}. The first, {multgee}, uses {gnm} to fit
      multiplicative interactions for certain correlation structures when modelling
      categorical data, with applications in public health, agriculture, and psychology.
      The second, {VFP}, is a more specialised package that uses {gnm} to model the
      mean-variance relationship for in-vitro diagnostic assays. \n\nThrough these
      use cases we'll see how different features of {gnm} can be applied, demonstrating
      the versatility of this software."
  1A:
    talk_1: Scalable finite mixture of regression models for clustering species responses
      in ecology
    talk_1_speakers: "Francis KC Hui \U0001F464"
    talk_1_orgs: The Australian National University
    talk_1_abstract: "When modeling species assemblages in ecology, clustering species
      with similar responses to the environment can facilitate a more parsimonious
      understanding of the assemblage, and improve prediction by borrowing strength
      across species within the same cluster. One statistical method for achieving
      the above is species archetype models (SAMs), a type of finite mixture of regression
      model where species are clustered according to the *shape* of their environmental
      response. \r\n\r\nIn this talk, we introduce approximate and scalable SAMs or
      asSAMs, which overcomes some of the current computational drawbacks in fitting
      SAMs. We show how asSAMs promotes fast uncertainty quantification via bootstrapping,
      along with fast variable selection on archetypal regression coefficients courtesy
      of a sparsity-inducing penalty. Simulation studies and an application to presence-absence
      records of over 230 species from the Great Barrier Reef Seabed biodiversity
      project demonstrate asSAMs can achieve similar to or better estimation, selection,
      and predictive performance than several existing methods in the literature."
    talk_2: 'Elastic Net Regularization for Vector Generalized Linear Models: A Flexible
      Framework for High-Dimensional Biomedical Data'
    talk_2_speakers:
    - "Wenqi Zhao \U0001F464"
    - Dr.Thomas Yee
    talk_2_orgs: University of Auckland, Auckland, New Zealand
    talk_2_abstract: "We introduce a novel implementation of elastic net regularization
      for vector generalized linear models (VGLMs), capable of fitting over 100 family
      functions and designed to support complex, high-dimensional modeling tasks commonly
      encountered in the biosciences. VGLMs extend classical GLMs by accommodating
      multivariate and multi-parameter responses, making them particularly well-suited
      for heterogeneous biomedical data.\r\n\r\nOur method integrates sparse estimation
      techniques—such as lasso, ridge, and their convex combinations—into this broader
      modeling framework, enhancing model interpretability and stability in high-dimensional
      settings. The algorithm is implemented in the vglmnet function within the new
      VGAMplus package for R. It leverages a modified iteratively reweighted least
      squares (IRLS) procedure, combined with pathwise coordinate descent, Karush-Kuhn-Tucker
      (KKT) condition checks, and strong rules for variable screening to ensure computational
      efficiency.\r\n\r\nThis framework supports a wide range of models beyond the
      exponential family, including ordinal, categorical, and zero-inflated distributions
      commonly encountered in fields such as epidemiology, genomics, and clinical
      research. We illustrate the utility of our approach through comparisons with
      existing tools (glmnet, ordinalNet, mpath) and apply it to real-world datasets
      involving survival outcomes, count data, and bivariate binary responses. By
      uniting the structural flexibility of VGLMs with the benefits of regularization,
      our method provides a powerful and scalable solution for modern statistical
      modeling in the biosciences.\r\n\r\n"
    talk_3: Fitting Generalised Linear Mixed Models using Sequential Quadratic Programming
    talk_3_speakers: "Peter Green \U0001F464"
    talk_3_orgs: Tadorna Data Science
    talk_3_abstract: "Finding maximum likelihood estimates in generalised linear mixed
      models (GLMMs) can be difficult due to the often-intractable integral over the
      random effects. \r\n\r\nEnsuring convergence can be tricky, especially in binomial
      GLMMs, and often multiple optimisers and settings need to be tried to get satisfactory
      results. This is exacerbated if you want to use parametric bootstrap for your
      inference. \r\n\r\nSequential quadratic programming (SQP) is a method for solving
      optimisation problems with non-linear constraints. SQP offers an alternative
      option for maximising the Laplace approximation to the GLMM likelihood, bypassing
      the need for an inner penalised iterative reweighted least squares (PIRLS) loop.
      \r\n\r\nThis talk discusses the implementation of SQP for GLMMs and compares
      its performance to other common approaches."
  1B:
    talk_1: Using a linear mixed model based wavelet transform to model non-smooth
      trends arising from designed experiments
    talk_1_speakers:
    - "Clayton Forknall \U0001F464"
    - Alison Kelly
    - Yoni Nazarathy
    - Ari Verbyla
    talk_1_orgs: Queensland Department of Primary Industries
    talk_1_abstract: The linear mixed model (LMM) representation of the cubic smoothing
      spline is a powerful tool for modelling smooth trends arising from designed
      experiments. However, when trends arising from such experiments are non-smooth,
      meaning that they are characterised by jagged features, spikes and/or regions
      of rapid change approximating discontinuities, smoothing spline techniques prove
      ineffective. A technique that has proven useful for the modelling of non-smooth
      trends is the wavelet transform. Existing methods to incorporate the wavelet
      transform into the LMM framework are varied, but often share a common limitation;
      that is, a reliance on classical wavelet approaches that require observations
      to be equidistant and dyadic ($\log_{2}(n)$ is an integer) in number. More recently,
      second generation wavelet methods have been developed, which overcome the limiting
      constraints imposed by classical wavelet approaches, enabling the wavelet transform
      to be applied to sets of non-equidistant observations, of any number. We present
      a method for the incorporation of these second generation wavelets, namely second
      generation B-spline wavelets, into an LMM framework to facilitate the wavelet
      transform. Furthermore, using the structure implicit in the resulting B-spline
      wavelet basis, we propose extensions to the LMM framework to enable heterogeneity
      of the associated wavelet variance across wavelet scales. This provides a new
      LMM based method which enables the flexible modelling of non-smooth trends arising
      in the conduct of designed experiments. The proposed method is demonstrated
      through application to a data set exhibiting non-smooth characteristics, that
      arises from a designed experiment exploring the proteome of barley malt.
    talk_2: Functional Data Analysis for the Australian Grains Industry
    talk_2_speakers:
    - "Braden J. Thorne \U0001F464"
    - Adam H. Sparks
    talk_2_orgs: Curtin University
    talk_2_abstract: When performing statistical analysis on time series data with
      regular sampling, it is common to apply filtering or windowing methods to reduce
      the complexity of the task. While this process enables many classical approaches,
      it inevitably leads to compression of the full information available. An alternative
      approach is to treat observations as samples of a continuous mathematical function
      and focus analysis on the curves these functions produce rather than the samples.
      This is the underlying idea of functional data analysis, a statistical analysis
      approach that has seen growing attention in recent years. In this talk I will
      offer an introduction to functional data analysis and detail our exploration
      of these methods for application to the grains industry across Australia. Specifically,
      I will present two case studies; estimating charcoal rot prevalence in sixteen
      years of data from in-paddock soybean experimental trials using weather data,
      and analysing frost risk in broadacre crops with varying stubble management
      practices using sensor data.
    talk_3: Bayesian Ordinal Regression for Crop Development and Disease Assessment
    talk_3_speakers:
    - "Zhanglong Cao \U0001F464"
    - Rose Megirian
    - Matthew Nguyen
    - Adam Sparks
    talk_3_orgs: Centre for Crop and Disease Management, Curtin University
    talk_3_abstract: "Accurate assessment of crop development and disease severity
      is essential for informed agronomic decision-making. This study presents a Bayesian
      framework for analysing ordinal data from field trials, including growth scale
      progression and disease ranking scores. Using the brms package in R, we applied
      cumulative logit models to evaluate the effects of sowing depth and treatment
      combinations on cereal growth stages, measured via the Zadok’s scale, across
      two Western Australian sites (Merredin and Wickepin). The same framework is
      being extended to model disease severity scores, demonstrating its versatility
      across categorical biological measurements.\r\nOur workflow incorporates rigorous
      model testing and evaluation, including posterior predictive checks and leave-one-out
      cross-validation (LOO-CV), to ensure robust inference and model fit. Rather
      than relying on p-values from linear mixed models, the Bayesian approach provides
      interpretable probabilities of achieving specific growth stages or disease severity
      levels. This shift enables more nuanced understanding of treatment effects and
      supports decision-making under uncertainty."
  2A:
    talk_1: Surveillance for counterfactual scenarios of invasive species - why it's
      useful and a convenient  way to do it.
    talk_1_speakers:
    - "Andrew P. Robinson \U0001F464"
    - John Kean
    - Melissa Welsh
    - ''
    - Kuo-Szu Chiang
    talk_1_orgs: The University of Melbourne
    talk_1_abstract: "The impact of invasive species is affected by a range of factors,
      many of which can be anticipated in advance – for example, the prevalence of
      host material, climate suitability, the size of affected agricultural resources,
      and so on. One factor that cannot be anticipated in advance is the size of the
      incursion at the time of its detection. Unfortunately, the impact of an incursion
      is tightly tied to its maturity at detection, ranging from a single seed, for
      example, to a 50,000 hectare infestation of plants. \n\nWe propose a simple
      probability model for the detection of an invasive species that can either capture
      or integrate out the consequent uncertainty of the maturity of the incursion.
      We represent the relationship between surveillance and the detection of the
      organism using survival analysis: the detection of the incursion is analogous
      to the survival event; it is a binary occurrence that happens at some point
      in time, and once it has happened it does not happen again.\n\nUnder such a
      model, we can connect the distribution of the size of the infestation at the
      time to detection to the probability of detecting the incursion given that it
      has not already been detected, namely, the hazard function. For example, a popular
      model for the detection of an incursion of size x with number of traps t and
      probability of detecting a single pest $p$ is:\n\n$h(x,t,p) = 1 - (1 - p)^{tx}$
      \n\nAlgebra leads us to a size-at-detection pdf.  Other corrections are also
      applied as needed. The outcome is a pdf that is a function of process parameters,
      enabling straightforward assessment of different surveillance choices. Parameter
      estimates for the distribution can be derived from first principles, field experiments,
      or expert elicitation.\n\nIn this presentation we will derive and demonstrate
      the use of the survival-based incursion size at detection pdf and discuss its
      implications and challenges."
    talk_2: Optimal allocation of resources between control and surveillance for complex
      eradication scenarios
    talk_2_speakers:
    - "Mahdi Parsa \U0001F464"
    - "Belinda Parsa \U0001F464"
    talk_2_orgs: Department of Agriculture, Fisheries and Forestry
    talk_2_abstract: |-
      Effective eradication of invasive species over large areas requires strategic
      allocation of resources between control measures and surveillance activities. This study
      presents an analytical Bayesian framework that integrates stochastic modelling and explicit
      measures of uncertainty to guide decisions in complex eradication scenarios. By applying
      Shannon entropy to quantify uncertainty and incorporating the expected value of perfect
      information (EVPI), the framework identifies conditions under which investment into control
      or surveillance becomes worthwhile. Findings show that strategies which hedge against
      uncertainty can markedly improve the robustness of eradication outcomes with only marginal increases in expected costs. This approach offers practical tools for designing more cost-effective and reliable eradication programs and for prioritising data collection to reduce uncertainty where it has the greatest impact.
    talk_3: Inferring the rate of undetected contamination using random effects modelling
      of biosecurity screening histories
    talk_3_speakers:
    - "Sumonkanti Das \U0001F464"
    - "Robert Das \U0001F464"
    talk_3_orgs: Australian National University
    talk_3_abstract: |-
      Group testing plays a vital role in biosecurity operations worldwide, particularly in minimising the risk of introducing exotic pests, contaminants, and pathogens through imported agricultural products. A common screening strategy involves pooling items from consignments and testing each group for contamination presence, with consignments typically rejected if any group tests positive. Although screening designs often target a high probability of detection assuming a fixed minimum prevalence, analysing the historical results of these tests to infer the extent of contamination in non-rejected consignments (referred to as leakage) is less common.

      This study advances censored beta-binomial (BB) models to address contamination risk in frozen seafood imports into Australia, incorporating imperfect tests. Motivated by the characteristics of our case study, we develop a new class of BB models that impose a minimum positive consignment propensity threshold, capturing scenarios where contamination is either absent or exceeds a known minimum level. To fit these models, we propose a Metropolis-Hastings (MH) algorithm conditioned on prior distributions for sensitivity and specificity, allowing efficient estimation of quantities related to contamination levels. We analyse historical testing data under multiple scenarios using the proposed MH algorithm, yielding novel insights into both contamination risk and leakage.

      Finally, we use model-based simulations to communicate risk levels, providing key insights into potential undetected contamination.
    talk_4: 'Optimal sampling in border biosecurity: Application to skip-lot sampling'
    talk_4_speakers: "Raphael Trouve \U0001F464"
    talk_4_orgs: The University of Melbourne
    talk_4_abstract: |-
      Border biosecurity faces mounting pressure from increasing global trade, requiring cost-effective inspection strategies to reduce the risk of importing pest and diseases. Current international standards recommend inspecting all incoming consignments (full census) with fixed sample sizes (e.g., 600 units) for high-risk pathways, but this may be overkill for lower-risk pathways with established compliance records. When should agencies use skip-lot sampling (SLS, sometimes called continuous sampling plan), which adaptively reduces inspections based on recent compliance history, over full census inspection?

      We developed a propagule pressure equation for SLS in overdispersed pathways and used Lagrange multipliers to derive a solution. Results show the choice depends on pathway overdispersion, sampling costs, and budget constraints. Optimal sample sizes are typically smaller than current recommendations, with better returns from inspecting a larger proportion of consignments rather than larger samples per consignment. This framework provides biosecurity agencies with data-driven guidance for implementing adaptive sampling strategies.
  2B:
    talk_1: Estimating extinction time from the fossil record using regression inversion
    talk_1_speakers:
    - "David I. Warton \U0001F464"
    - Victor Tsang
    talk_1_orgs: UNSW Sydney
    talk_1_abstract: "An important problem in palaeoecology is estimating the extinction
      or invasion time of a species from the fossil record - whether because this
      is of interest in and of itself, or in order to understand the causes of extinctions
      and invasions, for which we need to know when they actually happened. There
      are two main sources of error to contend with - sampling error (because the
      last time you see a species need not be the last time it was there) and measurement
      error (dating specimens, usually well known). The paleobiology literature typically
      ignores one or other of these sources of error, leading to bias and underestimation
      of uncertainty to an extent that is often qualitatively important. \r\n\r\nThe
      problem is surprisingly difficult to address statistically, because while standard
      regularity conditions are technically satisfied, we are typically close to a
      boundary where they break down, and hence standard asymptotic approaches to
      inference typically perform poorly in practice. We propose using a novel method,
      which we call regression inversion, for exact inference, and we apply this technique
      to a compound uniform-truncated t (CUTT) model for fossil data. We show via
      simulation that this approach leads to unbiased estimators, and accurate interval
      inference, in contrast to its competitors. We show how to check the CUTT assumption
      visually, and provide software to apply all of the above in the reginv package."
    talk_2: The performance of Yu and Hoff's confidence intervals for treatment means
      in a one-way layout
    talk_2_speakers: "Paul Kabaila \U0001F464"
    talk_2_orgs: La Trobe University
    talk_2_abstract: "Consider a one-way layout and suppose that we have uncertain
      prior information that the treatment population means are equal or close to
      equal. Yu & Hoff (2018) extended the \"tail method\" for finding a confidence
      interval for a scalar parameter of interest that has (a) specified coverage
      probability and (b) relatively small expected length when this parameter takes
      values in some given set. They used this extension to find confidence intervals
      for these treatment means that have (a) specified coverage probability individually
      and (b) relatively small expected lengths when this uncertain prior information
      happens to be correct. They assessed the expected lengths of these confidence
      intervals, over the whole parameter space, using a semi-Bayesian analysis. I
      describe a revealing alternative assessment of these expected lengths using
      a fully frequentist analysis.\r\n\r\nYu, C. & Hoff, P. (2018) Adaptive multigroup
      confidence intervals with coverage. *Biometrika*, 105, 319-335."
    talk_3: Rate-optimal sparse gamma scale mixture detection
    talk_3_speakers:
    - "Michael Stewart \U0001F464"
    - Qikun Chen
    talk_3_orgs: The University of Sydney
    talk_3_abstract: We consider a model where observations from a known gamma distribution
      are possibly contaminated by observations from another gamma distribution with
      the same shape but a different mean. Such a model has been considered for times
      between neurotransmitter releases based on a Markov chain with amalgamated indistinguishable
      states. We focus on the case where the contaminating component occurs rarely,
      the so-called sparse gamma scale mixture detection problem. Due to the irregularity
      of such models theoretical results concerning detectability bounds are non-standard.
      Nonetheless in recent years a body of theory has been developed which covers
      the case when the mean of the unknown contaminating component is smaller than
      the null mean, but not when it is larger. We present some recent results filling
      this gap in the literature. In particular we describe a test which attains the
      optimal rate of convergence in various local alternative scenarios which is
      a Bonferroni-type test combining three different tests.
    talk_4: Extension of the corrected score estimator in a Poisson regression model
      with a measurement error
    talk_4_speakers:
    - "Kentarou Wada \U0001F464"
    - T. Kurosawa
    talk_4_orgs: Tokyo University of Science
    talk_4_abstract: Kukush et al. (2004) discussed the bias of the naive estimator
      for the regression parameters in a Poisson regression model with a measurement
      error for the case where the explanatory variable and measurement error follow
      normal distributions. Wada and Kurosawa (2023) proposed the corrected naive
      (CN) estimator as a consistent estimator for a Poisson regression model with
      a measurement error for the case where the explanatory variable and measurement
      error are general distributions. The CN estimator directly calibrates the bias
      of the naive estimator. The CN estimator is given by the solution of the estimation
      equation of the Poisson regression model under the error-in-variables framework.
      However, the CN estimator does not always have an explicit expression under
      the condition that the explanatory variable and measurement error follow general
      distributions. On the other hand, Kukush et al. (2004) considered the corrected
      score (CS) estimator as a consistent estimator for the true parameter of the
      Poisson regression model with a measurement error. In this research, we extend
      the CS estimator to the case where the explanatory variable and measurement
      error are general distributions. The new estimator can be applied for the condition
      that the CN estimator does not have an explicit expression. As illustrative
      examples, we give simulation studies to verify the effectiveness of the new
      estimator.
Thursday:
  0A:
    talk_1: Modularizing Biometric Models Facilitates Multistage Computing
    talk_1_speakers: "Mevin B. Hooten \U0001F464"
    talk_1_orgs: The University of Texas at Austin
    talk_1_abstract: Bayesian modeling has become invaluable in biometrics.  It allows
      us to formally consider unobserved processes while accommodating uncertainty
      about data collection and our understanding of biological and ecological mechanisms.  Several
      excellent software packages are available for fitting Bayesian models to data
      and are being applied every day to analyze biometric data.  These methods allow
      us to answer questions using data in ways that has never before been possible.  The
      adoption of Bayesian methods has led to bigger models necessary to answer tough
      questions using large and varied data sets.  Bigger models and data sets lead
      to computing bottlenecks.  Fortunately, a solution to Bayesian computing roadblocks
      sits in plain sight.  The structure of Bayesian models allows us to rearrange
      them so that we can perform computing in stages.  We can break big models into
      pieces, fit them separately, and then recombine them in later computing stages.  Recursive
      Bayesian approaches can save us time by leveraging the parallel architecture
      of modern computers.  A modular perspective allows us to see Bayesian models
      in a way that facilitates multistage computing.  I will demonstrate the procedure
      with a set of biometric examples.  These include geostatistical models in marine
      science, capture-recapture models for abundance estimation, and spatial point
      process models for species distributions.
  1A:
    talk_1: Visualize your fitted non-linear dimension reduction model in the high-dimensional
      data space
    talk_1_speakers:
    - "P. G. Jayani Lakshika \U0001F464"
    - Dianne Cook
    - Paul Harrison
    - Michael Lydeamore
    - Thiyanga S. Talagala
    talk_1_orgs: Monash University, Australia
    talk_1_abstract: "Non-linear dimension reduction (NLDR) techniques such as t-SNE,
      UMAP, PHATE,\r\nPaCMAP and TriMAP provide a low-dimensional representation of
      high-dimensional data\r\nby applying a non-linear transformation. The methods
      and parameter choices can create wildly different representations, so much so
      that it is difficult to decide which is best, or whether any or all are accurate
      or misleading. NLDR often exaggerates random patterns, sometimes due to the
      samples observed, but NLDR views have an important role in data analysis because,
      if done well, they provide a concise visual (and conceptual) summary of high-dimensional
      distributions. To help evaluate the NLDR, we have developed a way to take the
      fitted model, as represented by the positions of points in 2D, and turn it into
      a high-dimensional wireframe to overlay on the data, viewing it with a tour.
      Viewing a model in the data space is an ideal way to examine the fit. One can
      see whether it fits the points everywhere or fits better in some places, or
      simply mismatches the pattern. It is used here to help with the difficult decision
      on which 2D layout is the best representation of the high-dimensional distribution,
      or whether the 2D layout is displaying mostly random structure. It can also
      help to see how different layouts made by different methods are effectively
      the same summary, or how the different methods have some particular quirks.
      This methodology is available in the R package `quollr`. We will demonstrate
      the technique using single-cell data, particularly to understand cluster structure."
    talk_2: 'The geometry of diet: using projections to quantify the similarity between
      sets of dietary patterns'
    talk_2_speakers:
    - "Beatrix Jones \U0001F464"
    - Shiyu Fan
    - Karen Mumme
    - John Thompson
    talk_2_orgs: University of Auckland
    talk_2_abstract: "Food consumption is complex and high dimensional. Nutrition
      researchers measure (or attempt to measure) consumption using food frequency
      questionnaires, food recalls, or food records. This generates high dimensional
      data which is frequently summarized using principle components, principle components
      with rotation, or factor analysis. The resulting linear combinations are called
      “dietary patterns.” In this talk we consider quantifying how similar two different
      sets of dietary patterns are, assuming the same set of underlying variables
      has been collected. We explore using a multivariate extension of Tucker’s congruence
      coefficient for this purpose. Tucker’s congruence coefficient can be thought
      of as the length of the projection of one factor direction onto another, or
      equivalently the (absolute) cosine of the angle between them; thus it ranges
      from 0 to 1. In (eg) two dimensions, we consider the square root of the area
      of a unit square projected from one space onto another; this can be generalized
      for any number of dimensions. The measure is symmetric and invariant to rotation.
      To contextualize scores on this measure, we compute this measure of agreement
      for several datasets from the dietary pattern validation literature, where the
      same food questionnaire is given to the same people a few weeks apart. We also
      consider the effect of truncating small loadings, as is common when describing
      dietary patterns. \r\n"
    talk_3: Multivariate meta-analysis methods for high-dimensional data
    talk_3_speakers:
    - "Alysha M. De Livera \U0001F464"
    - Luke Prendergast
    - Robert Staudte
    - ''
    - Jayamini Liyanage
    talk_3_orgs: La Trobe University
    talk_3_abstract: "Multivariate meta-analysis methods for high-dimensional data
      \r\n\r\nMeta-analysis is a statistical method that combines quantitative results
      from multiple independent studies on a particular research question or hypothesis,
      with the goal of making inference about the population effect size of interest.
      Traditional meta-analysis methods have focused on combining results from multiple
      independent studies, each of which has measured an effect size associated with
      a single outcome of interest. Modern studies in evidence synthesis, such as
      those in biological studies have focused on combining results from studies which
      have measured multiple effect sizes associated with multiple correlated outcomes.
      We will present a novel, multivariate meta-analysis method for obtaining summary
      estimates of the effect sizes of interest for high-dimensional data, with applications
      to real and simulated high-dimensional data. We will discuss advantages, disadvantages
      and the statistical challenges, and present a CRAN-based R package for implementation
      of the methods.\r\n"
  1B:
    talk_1: Reporting Odds Ratios under Fluctuating Reporting Rates in Spontaneous
      Reporting Systems
    talk_1_speakers:
    - "Tatsuhiko Anzai \U0001F464"
    - Kunihiko Takahashi
    talk_1_orgs: Institute of Science Tokyo
    talk_1_abstract: Spontaneous adverse event reporting systems, including the Japanese
      Adverse Drug Event Report (JADER) database, the FDA Adverse Event Reporting
      System (FAERS), and VigiBase, play a critical role in post-marketing drug safety
      surveillance. The reporting odds ratio (ROR) is a commonly used measure for
      detecting suspected adverse drug reactions as "signals", based on disproportionality
      in reporting between a specific drug and all others, as summarized in a two-by-two
      contingency table. However, during events such as the COVID-19 pandemic, reporting
      rates of adverse drug reactions can fluctuate, potentially introducing bias
      into ROR. This study proposes a method for deriving the ROR that incorporates
      four parameters, each corresponding to fluctuations in reporting ratios for
      the cells in the two-by-two contingency table. These parameters can be estimated
      using the divergence between observed and predicted values, where the predicted
      values are derived from a regression model under the assumption that the reporting
      rates for the drug and the adverse reaction remain stable. We evaluate the properties
      and performance of the proposed method through both real-world data analysis
      and simulation studies, demonstrating its effectiveness for signal detection.
    talk_2: Pooled testing with penalized regression models
    talk_2_speakers:
    - "Christopher Bilder \U0001F464"
    - Pranta Das
    - Joshua Tebbs
    - ''
    - Christopher McMahan
    talk_2_orgs: University of Nebraska-Lincoln, Department of Statistics
    talk_2_abstract: Pooled testing (also known as group testing) is a widely used
      procedure to test individuals for infectious diseases. Rather than testing each
      specimen separately, multiple specimens are pooled together and tested as one.
      The pool test outcome, along with further tests as needed, are used to determine
      which individuals are positive or negative for an infectious pathogen. The COVID-19
      pandemic especially highlighted the importance of pooled testing, with laboratories
      adopting it worldwide to increase their testing capacity. Pooled testing traditionally
      relies on observing the positive/negative test outcomes alone. However, during
      the pandemic, new pooled testing algorithms were developed that utilize the
      viral load information from a pool test. These new algorithms use penalized
      regression models to predict the viral load of each individual, which lead to
      individual positive/negative predictions. The purpose of our presentation is
      to provide a comparison of these new algorithms relative to standard ones. Both
      gains and losses are quantified by applying algorithms under fair comparison
      settings.
    talk_3: Handling Missingness in Prevalence Estimates from National Surveys
    talk_3_speakers:
    - "Oyelola Adegboye \U0001F464"
    - Li Siyu
    - Tomoki Fujii
    - Denis HY Leung
    talk_3_orgs: Menzies School of Health Research
    talk_3_abstract: Population-based surveys, such as the Demographic and Health
      Surveys (DHS), are pivotal for estimating the prevalence of important diseases,
      particularly in low-resource settings. However, missing data, such as person
      non-response, particularly refusals, poses a serious challenge, potentially
      introducing substantial bias in prevalence estimates. Using HIV data sets from
      Malawi's 2004 DHS, Antenatal Clinics surveillance, and the Malawi Diffusion
      and Ideational Change Project (MDICP), this paper evaluates existing estimators
      and proposes novel approaches to adjust for refusal bias. These include complete
      case analysis, mean score imputation, inverse propensity score weighting, bounding
      techniques using longitudinal or sentinel data and Manski’s partial identification
      approach. The study distinguishes between refusals and non-contacts and examines
      how prior knowledge of HIV status influences participation. Estimates of HIV
      prevalence varied notably across methods, with refusal-adjusted approaches generally
      yielding higher prevalence rates than complete case estimates. The divergence
      is more pronounced among women, largely due to higher refusal rates among them.
      Bounding methods provide credible intervals for prevalence under weak assumptions.
      In the case of HIV estimates, refusal bias, particularly when linked to prior
      testing knowledge, can significantly distort HIV estimates. Integrating multiple
      data sources and using methodologically transparent adjustment techniques are
      critical for robust HIV surveillance in low-resource settings.
  2A:
    talk_1: 'Integrated Species Distribution Models: A Single-Index Approach'
    talk_1_speakers:
    - "Quan Vu \U0001F464"
    - Francis Hui
    - Alan Welsh
    - Samuel Muller
    - Eva Cantoni
    - Chris Haak
    talk_1_orgs: Australian National University
    talk_1_abstract: 'In ecology and fisheries, species abundance data are often collected
      using a number of surveys that have different characteristics. Spatial statistical
      models can be used to integrate information from these datasets to improve both
      interpretability and prediction of the abundance. In this talk, we introduce
      the single-index integrated species distribution model, based on a single index
      representing the latent spatial distribution of the species abundance, and survey-specific
      link functions representing the different catchability properties of each survey.
      We demonstrate the use of the model through an analysis of scallop data collected
      from two surveys: a bottom trawl survey which covers a wide spatial domain but
      is less efficient, and a hydraulic dredge survey which is more efficient and
      spatially targeted. Results show that our model offers meaningful interpretations
      of covariate effects, spatial distribution, and survey catchability differences,
      and achieves superior predictive performance compared to contemporary species
      distribution models.'
    talk_2: Simultaneous Inference for Latent Variable Predictions in Factor Analytic
      Models
    talk_2_speakers:
    - "Zhining Wang \U0001F464"
    - Emi Tanaka
    - Francis K.C. Hui
    talk_2_orgs: Research School of Finance, Actuarial Studies and Statistics, Australian
      National University
    talk_2_abstract: Factor analytic models (also known as latent variable models)
      are fundamental tools in multivariate statistics, widely applied in fields such
      as psychology, economics, and social sciences. While considerable attention
      has been given to estimation and inference on parameters such as the loading
      matrix and the error variances, relatively less research has been done on how
      to perform inference on the predicted factors, e.g., how to construct prediction
      intervals for the latent variables jointly across all the clusters in a given
      dataset. We explore a framework for the simultaneous inference of the predicted
      latent variables in factor analytic models. We demonstrate the construction
      of simultaneous prediction intervals for the predicted factors, examining strategies
      such as the bootstrap and Monte Carlo simulation, and show how this also facilitates
      joint/multiple testing across different cluster-level predictions. We examine
      the practical feasibility and robustness of the proposed simultaneous inference
      methods with simulation studies and an application in the biosciences.
    talk_3: Model-based assessment of functional and phylogenetic diversity
    talk_3_speakers:
    - "Shaoqian Huang \U0001F464"
    - David Warton
    talk_3_orgs: School of Mathematics and Statistics, UNSW
    talk_3_abstract: "**Topic** \r\nBiodiversity is a quantity of fundamental interest,
      but measuring it remains challenging. Two key aspects of diversity are functional
      diversity (FD)—the extent to which species in a community differ in their ecological
      functions—and phylogenetic diversity (PD)—the extent to which species differ
      in their evolutionary histories.\r\n\r\n**Limitations** \r\nCommonly used distance-based
      diversity measures, such as Rao’s Q and Faith's PD, are typically affected by
      sampling intensity, which refers to the level of effort invested in collecting
      samples. Specifically, these measures confound true diversity changes with changes
      in species richness driven by different sampling intensities. In addition, there
      is no established statistical framework for analyzing how diversity changes
      along environmental gradients.\r\n\r\n**Our Model**\r\nWe propose a model-based
      assessment of PD and FD changes along gradients. It is robust to variations
      in sampling intensity, as the model explicitly captures the main trend in abundance
      change—namely, changes in species richness—and then isolates the residual variation,
      the $\\beta$-diversity change. We then measure PD or FD change using linear
      contrasts of the $\\beta$-diversity parameters, where the contrasts correspond
      to the main axes of the phylogenetic or functional similarity matrix.\r\n\r\n**Simulation
      Analysis** \r\nBased on the presence/absence pattern, we conducted simulations
      in which we designed phylogenetic distance matrices with different numbers of
      species, modelled on real data. The results show that our approach tends to
      outperform methods currently used in ecology."
    talk_4: Fitting integrated species distribution models using mgcv
    talk_4_speakers:
    - "Elliot Dovers \U0001F464"
    - Jakub Stoklosa
    - David L. Miller
    - David I. Warton
    talk_4_orgs: UNSW Sydney
    talk_4_abstract: "Integrated species distribution models (ISDMs) are a useful
      tool for ecologists, allowing them to use multiple sources of data to infer
      the distribution of species across geographic regions. Recent studies have shown
      that including latent spatial terms to account for dependence structures within
      and between datasets is often crucial to the performance these models. However,
      the inclusion of these latent terms can make the ISDMs technically challenging
      to fit, and often require users to learn/adopt bespoke software.\r\n\r\nWe describe
      how ISDMs can be fitted using `mgcv` on `R` by using the grouped family feature
      (`gfam`). This permits multiple likelihoods - and hence multiple data types
      - to be fitted within the same model by using each dataset to inform the estimation
      of common parameters. We additionally show how smoothers over the geographic
      coordinates that approximate Gaussian random fields can be used to incorporate
      the necessary latent spatial effects. We use presence/absence and presence-only
      data to demonstrate that ISDMs fitted via `mgcv` have comparable performance
      to other available software - through both simulations and in application, where
      we predict the occurrence of a tree species in NSW, Australia."
  2B:
    talk_1: Crossvalidation for predictive models in complex survey data
    talk_1_speakers:
    - "Thomas Lumley \U0001F464"
    - Amaia Iparragirre
    talk_1_orgs: University of Auckland
    talk_1_abstract: Cross-validation is a standard technique for choosing models
      with good out-of-sample prediction error in statistical learning. Chopping data
      up at random makes sense for independent observations, but not for observations
      from a multistage survey. For example, when a single cluster is split between
      test and training sets there is the potential for data leakage and underestimation
      of prediction error. I will describe an approach to cross-validation for complex
      survey data using replicate weights and describe its implementation in the R
      survey package.
    talk_2: A Set of Precise Asymptotics for Gaussian Variational Approximation in
      Generalised Linear Mixed Models
    talk_2_speakers:
    - "Nelson J. Y. Chua \U0001F464"
    - Francis K. C. Hui
    - Luca Maestrini
    - A. H. Welsh
    talk_2_orgs: Australian National University
    talk_2_abstract: "Generalised linear mixed models (GLMMs) are used to model data
      with a clustered or hierarchical structure, capturing intra-cluster dependence
      through the use of (latent) random effects. However, this dependence structure
      results in a likelihood function that is typically computationally intensive
      to evaluate, and so approximate likelihood approaches are often used instead
      for model fitting and inference. One such approach which has grown in popularity
      over the past decade is Gaussian variational approximation (GVA). In addition
      to estimates of the model parameters, GVA concurrently produces random effects
      predictions and associated uncertainty measures.\r\n\r\nIn this talk, we formulate
      a set of precise asymptotic properties for both the parameter estimates and
      random effects predictions obtained from GVA, focusing on independent cluster
      GLMMs. We find that these properties change substantially depending on whether
      or not the random effects are conditioned on. By comparing GVA's asymptotic
      properties and empirical finite-sample performance with that of other commonly
      used approximate likelihood approaches, we highlight situations in which the
      use of GVA would be advantageous in practice."
    talk_3: An allometric differential equation model quantifies energy trade-offs
      between growth and reproduction under temperature variation
    talk_3_speakers: "Hideyasu Shimadzu \U0001F464"
    talk_3_orgs: Kitasato University
    talk_3_abstract: "Individual variation in growth and reproduction is common even
      within a single\r\nspecies. Such differences play a crucial role in shaping
      life-history strategies, particularly\r\nin response to environmental factors
      such as temperature. This talk introduces a novel \r\nsystem of allometric differential
      equations that models internal energy allocation between\r\ntwo fundamental
      biological processes, growth and reproduction, under different\r\nthermal regimes.\r\n\r\nClimate
      change is not only driving increases in mean temperatures but also intensifying\r\ntemperature
      variability, resulting in greater environmental unpredictability. While\r\ntemperature
      is known to regulate resource allocation in ectotherms, the consequences\r\nof
      stochastic thermal fluctuations for life-history traits remain poorly understood.
      Using\r\n*Daphnia magna* as a model organism, we present a novel allometric
      growth model\r\nthat characterises energy allocation dynamics in fluctuating
      thermal environments. Our\r\nframework quantifies the effects of randomly varying
      temperatures on lifetime patterns\r\nof growth and reproduction. Our model reveals
      that exposure to unpredictable thermal\r\nregimes can elicit life-history responses
      similar to those observed under persistently\r\nelevated temperatures. Importantly,
      our energy-based approach identifies patterns of\r\nreproductive investment
      that are not discernible from growth data alone, highlighting\r\nthe pivotal
      role of temperature variability in shaping life-history trajectories. As climate\r\nchange
      increasingly entails unpredictable environmental conditions, the model\r\nand
      findings presented here offer valuable tools for anticipating and managing biological\r\nresponses
      to future climate scenarios."
    talk_4: A circular hidden Markov model for directional time series data
    talk_4_speakers:
    - "A.A.P.N.M. Perera \U0001F464"
    - Francis K.C. Hui
    - Carolyn Huston
    - A. H. Welsh
    talk_4_orgs: The Australian National University
    talk_4_abstract: "Modeling directional time series data such as wind or ocean
      current direction presents several interesting challenges. Standard linear time
      series techniques do not account for the circularity of the observations, while
      existing circular modeling approaches typically work best when the data span
      a small arc.\r\n\r\nMotivated by a series of fire burn experiments collecting
      wind direction in southeastern Australia, we propose a new method for directional
      time series data that combines the flexibility of hidden Markov models for capturing
      different latent states at different periods of time, with a conditional von
      Mises distribution given the latent state to explicitly account for the circular
      nature of the responses. The resulting circular hidden Markov model (cHMM) can
      allow for multimodality and/or varying amounts of circular dispersion over time.\r\n\r\nFurthermore,
      by utilizing a von Mises distribution whose mean direction depends on previous
      observations, we can accommodate serial correlations within a specific hidden
      state. We employ direct maximum likelihood estimation to fit the cHMM, and examine
      three approaches to perform forecasting based on extrapolating the latent state
      sequence and then direction observations conditional on this sequence. An application
      to the motivating wind direction datasets reveals that cHMMs produce similar
      or better point/probabilistic forecasting performance compared with several
      established time series methods.\r\n"
Friday:
  0A:
    talk_1: Saddlepoint approximations for likelihoods
    talk_1_speakers:
    - "Jesse Goodman \U0001F464"
    - Godrick Oketch
    - Rachel Fewster; Alice Hankin
    - Sixiang Shan
    - George Fan
    talk_1_orgs: University of Auckland
    talk_1_abstract: Classically, the saddlepoint approximation has been used as a
      systematic method for converting a known generating function into an approximation
      for an unknown density function. More recently, it has been used instead as
      an approximation to the *likelihood function*. In this viewpoint, it is the
      underlying data-generating process whose generating function is used, and the
      saddlepoint approximation can be maximized to compute an approximate saddlepoint
      MLE for given observed data. This talk will explain how the saddlepoint approximation
      can be interpreted with a statistical lens, including common features for those
      otherwise intractable models where we can compute a generating function but
      not a likelihood. Many of these models come from statistical ecology, including
      sitations where we gather population-wide observations only. In addition, the
      talk will describe a class of models having simple theoretical guarantees for
      the effect of using the saddlepoint MLE as a substitute for the unknown true
      MLE, and will demonstrate new tools to visualize the saddlepoint approximation
      intuitively, to simplify and automate the computation of saddlepoint MLEs, and
      to quantitatively assess the amount of approximation error introduced by using
      an approximate likelihood as a substitute for an intractable true likelihood.
  1A:
    talk_1: False Discovery Rate Controlled Robust Variable Selection under Cellwise
      Contamination
    talk_1_speakers:
    - "Xiaoya Sun \U0001F464"
    - Houying Zhu
    - Garth Tarr
    - Samuel Muller
    talk_1_orgs: School of Mathematical and Physical Sciences, Macquarie University
    talk_1_abstract: The increasing complexity and dimensionality of modern datasets
      make cellwise outliers a frequent and expected phenomenon, complicating tasks
      such as variable selection. An mRNA-Seq expression profiling data set of human
      brain tissue for Huntington's Disease (HD) illustrates this practically, where
      we work to identify key genes involved in HD by comparing affected individuals
      with neurologically normal controls. Our objective is to obtain reliable results
      while minimizing the impact of cellwise outliers. While many robust variable
      selection methods have been developed, few effectively handle high-rate cellwise
      contamination, and it remains an open problem to what extent such methods can
      control error rates. We introduce GRALF, the Gaussian-Ranked Adaptive Lasso
      with False discovery rate (FDR) control, a robust method designed for high-dimensional
      variable selection under cellwise contamination, taking advantage of FDR control
      to increase statistical reliability. GRALF builds on the framework of GR-ALasso,
      an effective robust variable selection method using the Gaussian-Rank estimator
      in the Adaptive Lasso, and integrates FDR control by generating fake counterparts
      of the Gaussian-rank transformed variables and estimating the number of false
      discoveries in the optimization process. Simulation studies demonstrate GRALF's
      desirable performance in terms of empirical power and FDR control under various
      conditions. The analysis of the HD data set identified potential gene markers
      associated with the development of HD, which overlap with findings from previous
      research, further validating the effectiveness of GRALF.
    talk_2: A covariate-adaptive test for replicability across multiple studies with
      false discovery rate control
    talk_2_speakers:
    - "Dennis Leung \U0001F464"
    - Ninh Tran
    talk_2_orgs: University of Melbourne
    talk_2_abstract: "Replicability is a gold standard for credible scientific discoveries.
      The partial conjunction (PC) p-value proposed by Benjamini and Heller (Biometrics,
      2008), which summarizes individual base p-values obtained from multiple similar
      studies, can gauge whether a signal of interest is replicable. However, when
      a large set of features are examined by these studies (such as comparable genomewide
      association studies conducted by different labs), testing for their replicated
      signals simultaneously can pose a very underpowered problem, due to both the
      multiplicity correction required and the inherent limitations of PC p-values.
      This power deficiency is particularly severe when replication is demanded for
      all studies, the most natural benchmark a practitioner performing meta-analyses
      may request. \r\n\r\nWe propose ParFilter, a procedure that combines the ideas
      of filtering and covariate-adaptiveness to power up large-scale testing for
      replicated signals as described above. It validly reduces the multiplicity burden
      by partitioning the studies into smaller groups and borrowing between-group
      information to filter out unpromising features. Moreover, harnessing side information
      offered by available covariates, it trains hypothesis weights to encourage rejections
      of features more likely to exhibit replicated signals. We prove its finite-sample
      false discovery rate control under standard assumptions on the dependence of
      the base p-values across features. In simulations as well as a real case study
      on autoimmunity based on RNA-Seq data obtained from thymic cells, the ParFilter
      has demonstrated competitive performance against other existing methods for
      such replicability analyses.\r\n"
    talk_3: A semi-supervised framework for diverse multiple hypothesis testing scenarios
    talk_3_speakers:
    - "Jack Freestone \U0001F464"
    - William S. Noble
    - Uri Keich
    talk_3_orgs: Macquarie University
    talk_3_abstract: Standard multiple testing procedures are designed to report a
      list of discoveries, or suspected false null hypotheses, given the hypotheses'
      p-values or test scores. Recently there has been a growing interest in enhancing
      such procedures by combining additional information with the primary p-value
      or score. In line with this idea, we develop RESET (REScoring via Estimating
      and Training), which uses a unique data-splitting protocol that subsequently
      allows any semi-supervised learning approach to factor in the available side
      information while maintaining finite sample error rate control. Our practical
      implementation, RESET Ensemble, selects from an ensemble of classification algorithms
      so that it is compatible with a range of multiple testing scenarios without
      the need for the user to select the appropriate one. We apply RESET to both
      p-value and competition based multiple testing problems and show that RESET
      is (1) power-wise competitive, (2) fast compared to most tools and (3) able
      to uniquely achieve finite sample false discovery rate or false discovery exceedance
      control, depending on the user's preference.
  1B:
    talk_1: 'Genstat Markdown: Reproducible Research with Genstat'
    talk_1_speakers:
    - "James M. Curran \U0001F464"
    - Yuxiao Wang
    - Simon Urbanek
    - ''
    - David Baird
    talk_1_orgs: University of Auckland
    talk_1_abstract: "** This talk should not be accepted unless you are desperate
      :-) ** \r\n\r\nReproducible research ensures that scientific findings can be
      independently verified by others using the same data and methods. Markdown plays
      a key role in supporting reproducibility by allowing researchers to combine
      code, results, and narrative text in a clear, lightweight format. Tools like
      R Markdown or Jupyter Notebooks use Markdown to embed executable code alongside
      explanations and outputs, making it easy to document workflows, share analyses,
      and regenerate results consistently across different environments.\r\n\r\nGenstat
      is a statistical software package designed for data analysis, particularly in
      biometrical research and applications. Many users know it through its user-friendly
      interface. However, it also It offers a powerful scripting language.\r\n\r\nIn
      this talk I will describe a collaborative project between students and staff
      at the University of Auckland and VSNi Ltd, that allows Genstat to be used in
      conjunction with R Markdown to provide flexible reproducible research capability
      to the Genstat user community."
    talk_2: 'The 4S method for the longitudinal analysis of multidimensional questionnaires:
      application to Parkinson’s disease progression from patient perception'
    talk_2_speakers:
    - "Tiphaine Saulnier \U0001F464"
    - C√©cile Proust-Lima
    - Daniel J. Myall
    - Toni L. Pitcher
    - Leslie Livingston
    - Rachel Nolan
    - Sophie Grenfell
    - John C. Dalrymple-Alford
    - Alexandra Foubert-Samier
    - Wassilios G. Meissner
    - Tim J. Anderson
    - Campbell J. Le Heron
    talk_2_orgs: New Zealand Brain Research Institute, Christchurch, New Zealand and
      Institute of Neurodegenerative Diseases (IMN), University of Bordeaux, Bordeaux,
      France
    talk_2_abstract: "**Introduction** In health research, questionnaires are widely
      used to assess clinical states or quality of life (QoL). Longitudinal analysis
      of these tools offers valuable insights into disease progression. However, this
      raises multiple challenges, such as handling repeated ordinal responses, capturing
      the multidimensional traits underlying all the items, and accounting for informative
      dropout due to death during follow-up.\r\nIn this work, we present the 4S method
      – a comprehensive strategy developed to address these challenges, and illustrate
      it describing health-related QoL (Hr-QoL) changes in Parkinson’s disease (PD).\r\n\r\n**Methods**
      The 4S method comprises four successive steps:\r\n(*1 – structuring*) identify
      questionnaire dimensions through factor analyses;\r\n(*2 – sequencing*) describe
      each dimension progression and associated predictors with a joint latent process
      model combining an item-response mixed model and a proportional hazard for death
      risk;\r\n(*3 – staging*) compare progression across dimension continuums by
      projecting clinical stages;\r\n(*4 – selecting*) highlight stage-specific most
      informative items based on Fisher information.\r\n\r\n**Application** We analyzed
      longitudinal data from the New Zealand Parkinson Progression Programme (NZP$^3$),
      following over 400 PD patients for up to 16 years. Hr-QoL was measured via the
      PDQ-39 questionnaire, covering motor and non-motor spheres. Five dimensions
      were identified: mobility, daily activities, psycho-social, stigma, and cognition/communication/bodily
      discomfort. All, except stigma, showed progressive decline, with patterns notably
      varying by sex and onset age. Items related to walking, dexterity, anxiety,
      and communication appeared as particularly sensitive during PD stages, offering
      guidance to enhance patient-centered care.\r\n\r\n**Conclusion** The 4S method
      is a comprehensive statistical strategy suited to analyze repeatedly-collected
      questionnaires in health studies."
    talk_3: '`heritable` An R package for heritability calculations for plant breeding
      trials'
    talk_3_speakers:
    - "Fonti Kar \U0001F464"
    - Emi Tanaka
    talk_3_orgs: Australian National University
    talk_3_abstract: Understanding how much biological variation is heritable - or
      passed down from one generation to next - is central to plant breeding. Heritability
      is a useful indictor to assess the genetic gain of desirable traits such as
      yield or disease resistance in crop trials. We created `heritable`, an R package
      to streamline the calculation of heritability from `asreml` and `lme4` mixed
      model outputs in a user-friendly manner. The package provides up to six different
      types of heritability measures and helper functions to compare between these.
      Our goal is to support decision makers by providing an intuitive, open and reproducible
      workflow in calculating heritability in R.
  2A:
    talk_1: Optimizing research impact through interdisciplinary and collaborative
      research
    talk_1_speakers: "Charmaine B. Dean \U0001F464"
    talk_1_orgs: University of Waterloo
    talk_1_abstract: Interdisciplinary collaborative research is a key component of
      data science and for some of us, plays an important part of our roles as statisticians.
      It is not unusual that we become accustomed to vertical thinking whereby we
      use existing tools and methods in our own specialty to problem solve, losing
      sight of the larger interdisciplinary context of data science, and the context
      of the scientific challenge. The Government of Canada - Science and Technology
      branch has identified several priority research challenge topics that involve
      cross-disciplinary work. Although statistical tools and analytics are identified
      in these research challenge priority areas, additionally, the development of
      fundamental transformative and enabling technological tools specifically for
      statistical methods and analytics to support research and societal advancement
      is also seen as a priority. This talk shares insights about the challenges and
      opportunities for statistics in interdisciplinary research. Specifically, monitoring
      viral signals in wastewater and assessing forest fire risk are given as complex,
      case studies that use a collaborative and interdisciplinary approach to solve
      difficult problems. This approach will demonstrate the significant benefits
      for not only optimizing research impact but for training students to become
      horizontal problem solvers across a wide range of research methods which will
      benefit them in navigating complex problems and in the development of appropriate
      tools for their analysis
