[
  {
    "objectID": "invited.html",
    "href": "invited.html",
    "title": "Invited Sessions",
    "section": "",
    "text": "Invited sessions aim to bring together experts to discuss emerging trends, present cutting-edge research, and engage in meaningful discussions on specialised topics that will be of interest to the biometrics community, over a 90 minute period."
  },
  {
    "objectID": "invited.html#statistics-for-biosecurity-surveillance",
    "href": "invited.html#statistics-for-biosecurity-surveillance",
    "title": "Invited Sessions",
    "section": "Statistics for Biosecurity Surveillance",
    "text": "Statistics for Biosecurity Surveillance\nOrganiser and Chair: A/Prof Robert Clark\n\n\nDr Mahdi Parsa Department of Agriculture, Fisheries and Forestry\n“Optimal allocation of resources between control and surveillance for complex eradication scenarios”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nEffective eradication of invasive species over large areas requires strategic allocation of resources between control measures and surveillance activities. This study presents an analytical Bayesian framework that integrates stochastic modelling and explicit measures of uncertainty to guide decisions in complex eradication scenarios. By applying Shannon entropy to quantify uncertainty and incorporating the expected value of perfect information (EVPI), the framework identifies conditions under which investment into control or surveillance becomes worthwhile. Findings show that strategies which hedge against uncertainty can markedly improve the robustness of eradication outcomes with only marginal increases in expected costs. This approach offers practical tools for designing more cost-effective and reliable eradication programs and for prioritising data collection to reduce uncertainty where it has the greatest impact.\n\n\n\nDr Sumon Das and A/Prof Robert Clark\nAustralian National University\n“Bayesian clustered ensemble prediction for multivariate time series”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nGroup testing plays a vital role in biosecurity operations worldwide, particularly in minimising the risk of introducing exotic pests, contaminants, and pathogens through imported agricultural products. A common screening strategy involves pooling items from consignments and testing each group for contamination presence, with consignments typically rejected if any group tests positive. Although screening designs often target a high probability of detection assuming a fixed minimum prevalence, analysing the historical results of these tests to infer the extent of contamination in non-rejected consignments (referred to as leakage) is less common.\nThis study advances censored beta-binomial (BB) models to address contamination risk in frozen seafood imports into Australia, incorporating imperfect tests. Motivated by the characteristics of our case study, we develop a new class of BB models that impose a minimum positive consignment propensity threshold, capturing scenarios where contamination is either absent or exceeds a known minimum level. To fit these models, we propose a Metropolis-Hastings (MH) algorithm conditioned on prior distributions for sensitivity and specificity, allowing efficient estimation of quantities related to contamination levels. We analyse historical testing data under multiple scenarios using the proposed MH algorithm, yielding novel insights into both contamination risk and leakage.\nFinally, we use model-based simulations to communicate risk levels, providing key insights into potential undetected contamination.\n\n\n\nDr Raphael Trouvé\nUniversity of Melbourne, Senior Research Fellow in Forest Dynamics\n“Optimal sampling in border biosecurity”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nBorder biosecurity faces mounting pressure from increasing global trade, requiring cost-effective inspection strategies to reduce the risk of importing pest and diseases. Current international standards recommend inspecting all incoming consignments (full census) with fixed sample sizes (e.g., 600 units) for high-risk pathways, but this may be overkill for lower-risk pathways with established compliance records. When should agencies use skip-lot sampling (SLS, sometimes called continuous sampling plan), which adaptively reduces inspections based on recent compliance history, over full census inspection?\nWe developed a propagule pressure equation for SLS in overdispersed pathways and used Lagrange multipliers to derive a solution. Results show the choice depends on pathway overdispersion, sampling costs, and budget constraints. Optimal sample sizes are typically smaller than current recommendations, with better returns from inspecting a larger proportion of consignments rather than larger samples per consignment. This framework provides biosecurity agencies with data-driven guidance for implementing adaptive sampling strategies."
  },
  {
    "objectID": "invited.html#a-cluster-of-modern-clustering-methods-for-biometrics",
    "href": "invited.html#a-cluster-of-modern-clustering-methods-for-biometrics",
    "title": "Invited Sessions",
    "section": "A cluster of modern clustering methods for Biometrics",
    "text": "A cluster of modern clustering methods for Biometrics\nOrganiser and Chair: A/Prof Francis Hui\n\n\nDr Skipton Wolley\nCSIRO Data 61\n“Species archetype models for presence-only data”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nJoint species distribution modelling is a recently emerged and potentially powerful statistical method for analysing biodiversity data. Despite the plethora of presence-only occurrence data available for biodiversity analysis, there remain few examples of presence-only multiple-species modelling approaches. We present a mixture-of-regressions model for understanding how groups of species are distributed based on presence-only data. Our approach extends Species Archetype Models using a point process framework and incorporates joint estimation of sighting bias based on all species occurrences included in the model. We demonstrate that our method can accurately recapture mean and variance of parameters from simulated data sets and provide better estimates than those generated from multiple single species presence-only species distribution models. We apply our approach to a Myrtaceae presence-only occurrence dataset from New South Wales, Australia. We show that presence-only Species Archetype Models allow for the propagation of variance and uncertainty from the data through predictions, improving inference made on presence-only biodiversity data for multiple species.\n\n\n\nDr Louise McMillian\nSchool of Mathematics and Statistics, Victoria University of Wellington\n“The difficulties of clustering categorical or mixed data”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nI will discuss techniques for clustering categorical data that go beyond treating the data as numerical or converting the categories to dummy variables. I will talk about a range of approaches to clustering categorical data, including the R package for clustering ordinal data, which uses model-based clustering via likelihoods and the EM algorithm. Then I will discuss a Bayesian approach from population genetics that may be extendable to general mixed datasets and is the subject of my latest research.\n\n\n\nDr Shonosuke Sugusawa\nFaculty of Economics, Keio University\n“Bayesian clustered ensemble prediction for multivariate time series”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nWe propose a novel methodology called the mixture of Bayesian predictive syntheses (MBPS) for multiple time series count data and apply the methodology to predict the numbers of COVID-19 inpatients and isolated cases in Japan and Korea at the subnational level. MBPS combines a set of predictive models and partitions the multiple time series into clusters based on their contribution to predicting the outcome. In this way MBPS leverages the shared information within each cluster and avoids using a multivariate count model, which is generally cumbersome to develop and implement. Our data analyses demonstrate that the proposed MBPS methodology has improved predictive accuracy and uncertainty quantification."
  },
  {
    "objectID": "invited.html#methods-and-practice-in-agricultural-analytics",
    "href": "invited.html#methods-and-practice-in-agricultural-analytics",
    "title": "Invited Sessions",
    "section": "Methods and Practice in Agricultural Analytics",
    "text": "Methods and Practice in Agricultural Analytics\nOrganiser and Chair: Dr Emi Tanaka\n\n\nDr Shuwen Hu\nRMIT University\n“Leveraging Statistical Modelling and Machine Learning in Animal Science”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nLive body weight gain is an important measurement of animal performance. In this study, we predict the cattle’s daily weight gain from five core cattle behaviours and a measure of total daily activity based on accelerometer data. To collect data, we conducted an experiment equipping a herd of 60 Brahman steers with research collars containing triaxial accelerometers over nearly one month in Australia. We used the accelerometer data, which represents the intensity of animal movement, to compute an activity metric within a five-minute window. In addition, we use pre-trained accelerometer-based machine learning models to classify cattle behaviour into grazing, ruminating, resting, walking, drinking or other classes over five-second time windows. Daily behaviour profiles were constructed for each animal and experiment day by aggregating the behaviour predictions over every calendar day. Our objective was to explore how to use behaviours and activity metrics to predict the cattle’s daily weight gain. The daily activity values ranging from 5.44g to 23.69g. The average daily time spent grazing, ruminating, resting, walking and drinking was 8.97±1.12, 7.78 ±1.03, 5.83±1.05, 1.00±0.4, and 0.18±0.12 hours, respectively. Some weather information data are combined in the model to predict the cattle live-weight gain. The best R-squared value is 0.467, with a minimum root mean square error (RMSE) of 0.867 from the linear regression model. The live-weight gain could not be fully explained by measurements taken in this study, but we showed how these factors can influence the variability in cattle performance.\n\n\n\nA/Prof Gota Morota\nUniveristy of Tokyo\n“Evaluating the impact of trait measurement error on genetic analysis of computer vision-based phenotypes”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nQuantitative genetic analysis of image- or video-derived phenotypes is increasingly being performed for a wide range of traits. Pig body weight values estimated by a conventional approach or a computer vision system can be considered as two different measurements of the same trait, but with different sources of phenotyping error. Previous studies have shown that trait measurement error, defined as the difference between manually collected phenotypes and image-derived phenotypes, can be influenced by genetics, suggesting that the error is systematic rather than random and is more likely to lead to misleading quantitative genetic analysis results. Therefore, we investigated the effect of trait measurement error on genetic analysis of pig body weight (BW). Calibrated scale-based and image-based BW showed high coefficients of determination and goodness of fit. Genomic heritability estimates for scale-based and image-based BW were mostly identical across growth periods. Genomic heritability estimates for trait measurement error were consistently negligible, regardless of the choice of computer vision algorithm. In addition, genome-wide association analysis revealed no overlap between the top markers identified for scale-based BW and those associated with trait measurement error. Overall, the deep learning-based regressions outperformed the adaptive thresholding segmentation methods. This study showed that manually measured scale-based and image-based BW phenotypes yielded the same quantitative genetic results. We found no evidence that BW trait measurement error could be influenced, at least in part, by genetic factors. This suggests that trait measurement error in pig BW does not contain systematic errors that could bias downstream genetic analysis.\n\n\n\nDr Elle Saber\nAustralian National University\n“Fishing for Heritability in the Gill Microbiome: Why Statisticians Should get out into the field”\n\n\n\n\n\n\nNoteAbstract\n\n\n\n\n\nHost‐associated microbiomes are increasingly recognised as integral to health, yet the extent to which host genetics shapes these communities remains unclear. While heritable components of gut and skin microbiomes have been documented in several vertebrates, evidence for the gill microbiome of fish is scarce. This exploratory study sampled the gill microbiome of Atlantic salmon within a Tasmanian breeding program to investigate potential genetic influences. Despite careful planning, study participants did not always behave, and the practical constraints of a commercial operation prevented complete data capture. Having participated in the data collection I was better prepared to understand the limitations of the dataset when doing the downstream analysis. The take home message is that, even if we feel like a fish out of water, time spent among the data can be just as valuable as time spent our desks."
  },
  {
    "objectID": "registration.html",
    "href": "registration.html",
    "title": "Registration",
    "section": "",
    "text": "To register see here."
  },
  {
    "objectID": "accommodation.html",
    "href": "accommodation.html",
    "title": "Accommodation",
    "section": "",
    "text": "The conference venue, National Film and Sound Archive of Australia (NFSA), is within walking distance to CBD, so any CBD hotel is generally suitable."
  },
  {
    "objectID": "accommodation.html#discounts-for-delegates",
    "href": "accommodation.html#discounts-for-delegates",
    "title": "Accommodation",
    "section": "Discounts for Delegates",
    "text": "Discounts for Delegates\n\nNovotel Canberra (15% off Best Available Rate, discount is subject to availability)\nPeppers Gallery Hotel Canberra (No pre-payment required, cancel up to 1 day prior to arrival date) Enter the promo code EVENTSPECIAL25 \nMidnight Hotel enter the code KF7 at the Corp/Promo Code area under Special Rates"
  },
  {
    "objectID": "accommodation.html#accommodation-options",
    "href": "accommodation.html#accommodation-options",
    "title": "Accommodation",
    "section": "Accommodation Options",
    "text": "Accommodation Options\n\nOvolo Nishi  (~5 minutes walk)\nPeppers Gallery Hotel Canberra  (~5 minutes walk)\nQT Canberra  (~7 minutes walk)\nBreakFree Capital Tower Canberra  (~7 minutes walk)\nQuest Canberra (Apartments)  (~12 minutes walk)\nNovotel Canberra  (~14 minutes walk)\nAdina Serviced Apartments Canberra James Court  (~20 minutes walk)\nA by Adina Canberra (~15 minutes walk)\nAvenue Hotel Canberra  (~21 minutes walk)\nMeriton Suites Canberra  (~22 minutes walk)"
  },
  {
    "objectID": "keynote.html",
    "href": "keynote.html",
    "title": "Keynote Speakers",
    "section": "",
    "text": "Charmaine Dean\n\n\n\n\n\n\nDr. Charmaine Dean (Ph.D., University of Waterloo) is Vice-President, Research and International at the University of Waterloo. In this role, she provides strategic leadership in the areas of research and innovation, commercialization, and internationalization. She is also responsible for building strategic alliances and partnerships with other academic institutions, governments, businesses, and industries at the regional, national, federal, and international levels.\n\n\nMore details\n\nSeveral key collaboration portfolios are managed by her office, including the university-level Centres and Institutes and several major industrial partnerships spanning various units in the university. She has drawn a focus to ethics and social impact related to technology developments through various initiatives and is a key driver for equity and diversity in the context of research and internationalization. Prior to joining the University of Waterloo, Dr. Dean served as the Dean of Science at Western University from 2011 to 2017. She also played a major role in establishing the Faculty of Health Sciences at Simon Fraser University, as the Associate Dean of that Faculty, and was the founding Chair of the Department of Statistics and Actuarial Science at Simon Fraser University. Dr. Dean has been awarded numerous honours for her work including Fellowships with the Institute of Mathematical Statistics, the Fields Institute, the American Statistical Association, and the American Association for the Advancement of Science. In 2023, she was awarded the Statistical Society of Canada Gold Medal. In 2024, she was bestowed the rank of Chevalier in the prestigious Ordre des Palmes académiques by the French government for her work advancing research connections between Canada and France. Additionally, Dr. Dean has held several editorships and served in numerous leadership roles internationally and nationally, related to equity and inclusion, statistics and data science, research, and computing infrastructure. In Canada, she served as President of the Statistical Society of Canada and serves on several Boards of Directors. She is currently Chair of Council for NSERC (the Natural Sciences and Engineering Research Council of Canada).\n\n\n\n\n\n\n\nOptimizing research impact through interdisciplinary and collaborative research\n\n\nInterdisciplinary collaborative research is a key component of data science and for some of us, plays an important part of our roles as statisticians. It is not unusual that we become accustomed to vertical thinking whereby we use existing tools and methods in our own specialty to problem solve, losing sight of the larger interdisciplinary context of data science, and the context of the scientific challenge. The Government of Canada - Science and Technology branch has identified several priority research challenge topics that involve cross-disciplinary work. Although statistical tools and analytics are identified in these research challenge priority areas, additionally, the development of fundamental transformative and enabling technological tools specifically for statistical methods and analytics to support research and societal advancement is also seen as a priority. This talk shares insights about the challenges and opportunities for statistics in interdisciplinary research. Specifically, monitoring viral signals in wastewater and assessing forest fire risk are given as complex, case studies that use a collaborative and interdisciplinary approach to solve difficult problems. This approach will demonstrate the significant benefits for not only optimizing research impact but for training students to become horizontal problem solvers across a wide range of research methods which will benefit them in navigating complex problems and in the development of appropriate tools for their analysis.\n\n\n\n\n\n\n\n\n\n\nCharmaine Dean\n\n\n\n\n\n\nDr. Charmaine Dean (Ph.D., University of Waterloo) is Vice-President, Research and International at the University of Waterloo. In this role, she provides strategic leadership in the areas of research and innovation, commercialization, and internationalization. She is also responsible for building strategic alliances and partnerships with other academic institutions, governments, businesses, and industries at the regional, national, federal, and international levels.\n\n\nMore details\n\nSeveral key collaboration portfolios are managed by her office, including the university-level Centres and Institutes and several major industrial partnerships spanning various units in the university. She has drawn a focus to ethics and social impact related to technology developments through various initiatives and is a key driver for equity and diversity in the context of research and internationalization. Prior to joining the University of Waterloo, Dr. Dean served as the Dean of Science at Western University from 2011 to 2017. She also played a major role in establishing the Faculty of Health Sciences at Simon Fraser University, as the Associate Dean of that Faculty, and was the founding Chair of the Department of Statistics and Actuarial Science at Simon Fraser University. Dr. Dean has been awarded numerous honours for her work including Fellowships with the Institute of Mathematical Statistics, the Fields Institute, the American Statistical Association, and the American Association for the Advancement of Science. In 2023, she was awarded the Statistical Society of Canada Gold Medal. In 2024, she was bestowed the rank of Chevalier in the prestigious Ordre des Palmes académiques by the French government for her work advancing research connections between Canada and France. Additionally, Dr. Dean has held several editorships and served in numerous leadership roles internationally and nationally, related to equity and inclusion, statistics and data science, research, and computing infrastructure. In Canada, she served as President of the Statistical Society of Canada and serves on several Boards of Directors. She is currently Chair of Council for NSERC (the Natural Sciences and Engineering Research Council of Canada).\n\n\n\n\nOptimizing research impact through interdisciplinary and collaborative research\n\n\nInterdisciplinary collaborative research is a key component of data science and for some of us, plays an important part of our roles as statisticians. It is not unusual that we become accustomed to vertical thinking whereby we use existing tools and methods in our own specialty to problem solve, losing sight of the larger interdisciplinary context of data science, and the context of the scientific challenge. The Government of Canada - Science and Technology branch has identified several priority research challenge topics that involve cross-disciplinary work. Although statistical tools and analytics are identified in these research challenge priority areas, additionally, the development of fundamental transformative and enabling technological tools specifically for statistical methods and analytics to support research and societal advancement is also seen as a priority. This talk shares insights about the challenges and opportunities for statistics in interdisciplinary research. Specifically, monitoring viral signals in wastewater and assessing forest fire risk are given as complex, case studies that use a collaborative and interdisciplinary approach to solve difficult problems. This approach will demonstrate the significant benefits for not only optimizing research impact but for training students to become horizontal problem solvers across a wide range of research methods which will benefit them in navigating complex problems and in the development of appropriate tools for their analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nCheng Soon Ong\n\n\n\n\n\n\nCheng Soon Ong is an Associate Science Director at Data61, CSIRO and a senior principal research scientist at the Statistical Machine Learning Group. He works on extending machine learning methods and enabling new approaches to scientific discovery, and has led the machine learning and artificial intelligence future science platform at CSIRO. He supervises and mentors many junior scientists, collaborates with scientists on problems in genomics and astronomy, advocates for open science, and is an adjunct Associate Professor at the Australian National University. He is co-author of the textbook Mathematics for Machine Learning, and his career has spanned multiple roles in Malaysia, Germany, Switzerland, and Australia.\n\n\n\n\n\n\nOn Finding Good Experiments\n\n\nOne of the key choices we have as scientists is to design informative experiments. With computational methods like AI promising accurate predictions, we revisit the question of adaptively designing new measurements that take previous data into account. Using examples from genomics, we illustrate some recent ideas on using machine learning to recommend experiments. Then we discuss potential impacts on choosing measurements in spatiotemporal problems. We conclude by outlining some opportunities and challenges of including machine learning in the scientific discovery process.\n\n\n\n\n\n\n\n\n\n\nCheng Soon Ong\n\n\n\n\n\n\nCheng Soon Ong is an Associate Science Director at Data61, CSIRO and a senior principal research scientist at the Statistical Machine Learning Group. He works on extending machine learning methods and enabling new approaches to scientific discovery, and has led the machine learning and artificial intelligence future science platform at CSIRO. He supervises and mentors many junior scientists, collaborates with scientists on problems in genomics and astronomy, advocates for open science, and is an adjunct Associate Professor at the Australian National University. He is co-author of the textbook Mathematics for Machine Learning, and his career has spanned multiple roles in Malaysia, Germany, Switzerland, and Australia.\n\n\n\nOn Finding Good Experiments\n\n\nOne of the key choices we have as scientists is to design informative experiments. With computational methods like AI promising accurate predictions, we revisit the question of adaptively designing new measurements that take previous data into account. Using examples from genomics, we illustrate some recent ideas on using machine learning to recommend experiments. Then we discuss potential impacts on choosing measurements in spatiotemporal problems. We conclude by outlining some opportunities and challenges of including machine learning in the scientific discovery process.\n\n\n\n\n\n\n\n\n\n\n\n\nHeather Turner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeather is an Associate Professor and EPSRC Research Software Engineering Fellow in the Statistics Department at the University of Warwick, UK. She has over 20 years of experience in the development of statistical code and software, gained through positions in academia, industry, and as a freelance consultant. In research, she has developed a portfolio of R packages for statistical modelling and collaborated on applications in sports, social science and agriculture. In her work with industry, she has specialised in applications in pharmaceutical R&D, with companies including Pfizer, Johnson & Johnson and Roche. Heather is active in community management and engagement among R users and developers. She is on the board of the R Foundation and chairs the R Contribution Working Group (fostering the community of contributors to the R project) and the R Forwards taskforce (widening the participation of under-represented groups in the R community).\n\n\n\n\n\n\nUses of gnm for Generalized (Non)linear Modelling\n\n\nThe R package {gnm} was designed as a unified interface to fit Generalized Nonlinear Models: generalized to handle responses with restricted range and/or a variance that depends on the mean, and nonlinear to allow the predictor for the mean to be nonlinear in its parameters. This framework covers several models that were proposed in the literature and adopted in practice before {gnm} was released, but used to require a mixed bag of specialised software to fit.\nWith {gnm} celebrating its 20th birthday this year, it’s a good time to review how the package is being used. I’ll highlight some of the applications we were aware of when {gnm} was first developed, that remain in common use, and explore more recent applications, particularly in the field of biometrics.\nWe’ll discover one motivation for using {gnm}, is for the ““eliminate”” feature that efficiently estimates stratification parameters. This can be useful even when the predictor is linear, as in the case of using conditional Poisson models to analyse case-crossover studies in epidemiology. We’ll also look at two of the packages that have built on {gnm}. The first, {multgee}, uses {gnm} to fit multiplicative interactions for certain correlation structures when modelling categorical data, with applications in public health, agriculture, and psychology. The second, {VFP}, is a more specialised package that uses {gnm} to model the mean-variance relationship for in-vitro diagnostic assays.\nThrough these use cases we’ll see how different features of {gnm} can be applied, demonstrating the versatility of this software.\n\n\n\n\n\n\n\n\n\n\nHeather Turner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeather is an Associate Professor and EPSRC Research Software Engineering Fellow in the Statistics Department at the University of Warwick, UK. She has over 20 years of experience in the development of statistical code and software, gained through positions in academia, industry, and as a freelance consultant. In research, she has developed a portfolio of R packages for statistical modelling and collaborated on applications in sports, social science and agriculture. In her work with industry, she has specialised in applications in pharmaceutical R&D, with companies including Pfizer, Johnson & Johnson and Roche. Heather is active in community management and engagement among R users and developers. She is on the board of the R Foundation and chairs the R Contribution Working Group (fostering the community of contributors to the R project) and the R Forwards taskforce (widening the participation of under-represented groups in the R community).\n\n\n\nUses of gnm for Generalized (Non)linear Modelling\n\n\nThe R package {gnm} was designed as a unified interface to fit Generalized Nonlinear Models: generalized to handle responses with restricted range and/or a variance that depends on the mean, and nonlinear to allow the predictor for the mean to be nonlinear in its parameters. This framework covers several models that were proposed in the literature and adopted in practice before {gnm} was released, but used to require a mixed bag of specialised software to fit.\nWith {gnm} celebrating its 20th birthday this year, it’s a good time to review how the package is being used. I’ll highlight some of the applications we were aware of when {gnm} was first developed, that remain in common use, and explore more recent applications, particularly in the field of biometrics.\nWe’ll discover one motivation for using {gnm}, is for the ““eliminate”” feature that efficiently estimates stratification parameters. This can be useful even when the predictor is linear, as in the case of using conditional Poisson models to analyse case-crossover studies in epidemiology. We’ll also look at two of the packages that have built on {gnm}. The first, {multgee}, uses {gnm} to fit multiplicative interactions for certain correlation structures when modelling categorical data, with applications in public health, agriculture, and psychology. The second, {VFP}, is a more specialised package that uses {gnm} to model the mean-variance relationship for in-vitro diagnostic assays.\nThrough these use cases we’ll see how different features of {gnm} can be applied, demonstrating the versatility of this software.\n\n\n\n\n\n\n\n\n\n\n\n\nJesse Goodman\n\n\n\n\n\n\nJesse Goodman is a Senior Lecturer in Statistics at the University of Auckland. He began his academic career in mathematical probability, using distributional approximations and large deviations techniques to understand random geometric structures, and increasingly looking at statistical models that exhibit probabilistic complexity. His research focuses on asymptotic techniques, particularly when several asymptotic effects interact or compete, in areas from branching processes and spatial structures to approximations for likelihoods and densities. Recent work on saddlepoint approximations has helped to quantify the degree to which approximation techniques can deliver valid inferential conclusiosn, and to extend classes of models to capture real-life complexity in capture-recapture and other settings that feature enigmatic sensor data.\n\n\n\n\n\n\nSaddlepoint approximations for likelihoods\n\n\nClassically, the saddlepoint approximation has been used as a systematic method for converting a known generating function into an approximation for an unknown density function. More recently, it has been used instead as an approximation to the likelihood function. In this viewpoint, it is the underlying data-generating process whose generating function is used, and the saddlepoint approximation can be maximized to compute an approximate saddlepoint MLE for given observed data. This talk will explain how the saddlepoint approximation can be interpreted with a statistical lens, including common features for those otherwise intractable models where we can compute a generating function but not a likelihood. Many of these models come from statistical ecology, including sitations where we gather population-wide observations only. In addition, the talk will describe a class of models having simple theoretical guarantees for the effect of using the saddlepoint MLE as a substitute for the unknown true MLE, and will demonstrate new tools to visualize the saddlepoint approximation intuitively, to simplify and automate the computation of saddlepoint MLEs, and to quantitatively assess the amount of approximation error introduced by using an approximate likelihood as a substitute for an intractable true likelihood.\n\n\n\n\n\n\n\n\n\n\nJesse Goodman\n\n\n\n\n\n\nJesse Goodman is a Senior Lecturer in Statistics at the University of Auckland. He began his academic career in mathematical probability, using distributional approximations and large deviations techniques to understand random geometric structures, and increasingly looking at statistical models that exhibit probabilistic complexity. His research focuses on asymptotic techniques, particularly when several asymptotic effects interact or compete, in areas from branching processes and spatial structures to approximations for likelihoods and densities. Recent work on saddlepoint approximations has helped to quantify the degree to which approximation techniques can deliver valid inferential conclusiosn, and to extend classes of models to capture real-life complexity in capture-recapture and other settings that feature enigmatic sensor data.\n\n\n\nSaddlepoint approximations for likelihoods\n\n\nClassically, the saddlepoint approximation has been used as a systematic method for converting a known generating function into an approximation for an unknown density function. More recently, it has been used instead as an approximation to the likelihood function. In this viewpoint, it is the underlying data-generating process whose generating function is used, and the saddlepoint approximation can be maximized to compute an approximate saddlepoint MLE for given observed data. This talk will explain how the saddlepoint approximation can be interpreted with a statistical lens, including common features for those otherwise intractable models where we can compute a generating function but not a likelihood. Many of these models come from statistical ecology, including sitations where we gather population-wide observations only. In addition, the talk will describe a class of models having simple theoretical guarantees for the effect of using the saddlepoint MLE as a substitute for the unknown true MLE, and will demonstrate new tools to visualize the saddlepoint approximation intuitively, to simplify and automate the computation of saddlepoint MLEs, and to quantitatively assess the amount of approximation error introduced by using an approximate likelihood as a substitute for an intractable true likelihood.\n\n\n\n\n\n\n\n\n\n\n\n\nMevin Hooten\n\n\n\n\n\n\nMevin Hooten is a Professor in Statistics and Data Sciences at The University of Texas at Austin. His research focuses on developing statistical methodology for ecological and environmental applications that involve spatial and spatio-temporal data. He is a Fellow of the American Statistical Association (ASA) and received the Distinguished Achievement Award from the ASA section on statistics and the environment. He has authored over 185 papers and 3 textbooks and serves as Associate Editor for Biometrics, Environmetrics, and JABES.\n\n\n\n\n\n\nModularizing Biometric Models Facilitates Multistage Computing\n\n\nBayesian modeling has become invaluable in biometrics. It allows us to formally consider unobserved processes while accommodating uncertainty about data collection and our understanding of biological and ecological mechanisms. Several excellent software packages are available for fitting Bayesian models to data and are being applied every day to analyze biometric data. These methods allow us to answer questions using data in ways that has never before been possible. The adoption of Bayesian methods has led to bigger models necessary to answer tough questions using large and varied data sets. Bigger models and data sets lead to computing bottlenecks. Fortunately, a solution to Bayesian computing roadblocks sits in plain sight. The structure of Bayesian models allows us to rearrange them so that we can perform computing in stages. We can break big models into pieces, fit them separately, and then recombine them in later computing stages. Recursive Bayesian approaches can save us time by leveraging the parallel architecture of modern computers. A modular perspective allows us to see Bayesian models in a way that facilitates multistage computing. I will demonstrate the procedure with a set of biometric examples. These include geostatistical models in marine science, capture-recapture models for abundance estimation, and spatial point process models for species distributions.\n\n\n\n\n\n\n\n\n\n\nMevin Hooten\n\n\n\n\n\n\nMevin Hooten is a Professor in Statistics and Data Sciences at The University of Texas at Austin. His research focuses on developing statistical methodology for ecological and environmental applications that involve spatial and spatio-temporal data. He is a Fellow of the American Statistical Association (ASA) and received the Distinguished Achievement Award from the ASA section on statistics and the environment. He has authored over 185 papers and 3 textbooks and serves as Associate Editor for Biometrics, Environmetrics, and JABES.\n\n\n\nModularizing Biometric Models Facilitates Multistage Computing\n\n\nBayesian modeling has become invaluable in biometrics. It allows us to formally consider unobserved processes while accommodating uncertainty about data collection and our understanding of biological and ecological mechanisms. Several excellent software packages are available for fitting Bayesian models to data and are being applied every day to analyze biometric data. These methods allow us to answer questions using data in ways that has never before been possible. The adoption of Bayesian methods has led to bigger models necessary to answer tough questions using large and varied data sets. Bigger models and data sets lead to computing bottlenecks. Fortunately, a solution to Bayesian computing roadblocks sits in plain sight. The structure of Bayesian models allows us to rearrange them so that we can perform computing in stages. We can break big models into pieces, fit them separately, and then recombine them in later computing stages. Recursive Bayesian approaches can save us time by leveraging the parallel architecture of modern computers. A modular perspective allows us to see Bayesian models in a way that facilitates multistage computing. I will demonstrate the procedure with a set of biometric examples. These include geostatistical models in marine science, capture-recapture models for abundance estimation, and spatial point process models for species distributions.\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contact",
    "section": "",
    "text": "For enquiries, please send emails to  conference@biometricsociety.org.au."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biometrics in the Bush Capital",
    "section": "",
    "text": "The Australasian Region Biometrics conference is a biennial conference sponsored by the International Biometric Society (Australasian Region). The conference is a forum for biometricians, statisticians, and those interested in the development and application of statistical and mathematical theory and methods to problems in the biosciences. The conference is open to members and non-members of the Society.\nPre-conference workshops are now open for registration:\nSee more details here. As places are limited, please register early to secure your spot.\nFor past updates, see here."
  },
  {
    "objectID": "index.html#important-dates",
    "href": "index.html#important-dates",
    "title": "Biometrics in the Bush Capital",
    "section": " Important dates",
    "text": "Important dates\n\n\n\nDate\n \n\n\n\n\nFeb 28\nInvited session proposals: email conference@biometricsociety.org.au\n\n\nApr 14\nOpen for abstract submission\n\n\nJun 1\nOpen for registration\n\n\nJul 21\nClosing date for abstract submission\n\n\nAug 15\nInform acceptance of the abstract\n\n\nSep 8\nClose for early bird registration\n\n\nAug 1 Oct 1\nLunchtime discussion proposals: email conference@biometricsociety.org.au (extended by two months)\n\n\nNov 23\nOceaniaR Hackathon co-hosted with the Statistical Computing and Visualisation Section of the Statistical Society of Australia.\n\n\nNov 24\nWorkshop\n\n\nNov 24-28\nConference"
  },
  {
    "objectID": "index.html#registering-for-updates",
    "href": "index.html#registering-for-updates",
    "title": "Biometrics in the Bush Capital",
    "section": " Registering for updates",
    "text": "Registering for updates\n\nLoading…"
  },
  {
    "objectID": "dates.html",
    "href": "dates.html",
    "title": "Important dates",
    "section": "",
    "text": "Date\n \n\n\n\n\nFeb 28\nInvited session proposals: email conference@biometricsociety.org.au\n\n\nApr 14\nOpen for abstract submission\n\n\nJun 1\nOpen for registration\n\n\nJul 21\nClosing date for abstract submission\n\n\nAug 15\nInform acceptance of the abstract\n\n\nSep 8\nClose for early bird registration\n\n\nAug 1 Oct 1\nLunchtime discussion proposals: email conference@biometricsociety.org.au (extended by two months)\n\n\nNov 23\nOceaniaR Hackathon co-hosted with the Statistical Computing and Visualisation Section of the Statistical Society of Australia.\n\n\nNov 24\nWorkshop\n\n\nNov 24-28\nConference"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "For enquiries, please send emails to  conference@biometricsociety.org.au."
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "The International Biometrics Society Australasian Region (IBS-AR) is dedicated to providing a welcoming and harassment-free experience for everyone. We do not tolerate unprofessional or inappropriate conduct, which includes discrimination and harassment, sexual or otherwise. Conference participants should always work towards maintaining an environment free of harassment and discrimination. At the discretion of the IBS-AR conference organisers and/or IBS-AR council, any participants violating these rules may be asked to leave the conference, without any fee refund.\nParticipants agree to:\n\nBe considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of fellow attendees at all times.\nRefrain from demeaning, discriminatory, or harassing behaviour and speech.\nAlert a member of the IBS-AR council or conference organisers if they notice a dangerous situation, someone in distress, or violation of this code of conduct.\n\nHarassment is demeaning or threatening behaviour directed at an individual or a group of people. Harassment includes, but is not limited to,\n\nOffensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, person’s lifestyle choices and practices, physical appearance, age, race, religion (or lack thereof) or beliefs;\nUnwelcome sexual attention and behaviour including inappropriate sexual propositions; unnecessary touching; verbal or written comments or physical actions of a sexual nature; sexually degrading words used to describe an individual; a display of sexually suggestive objects or pictures; or sexually explicit jokes; and\nThreatening behaviour including bullying, threat of violence, deliberate intimidation, stalking or incitement of violence. Reporting\n\nIf you are being harassed by a member/guest/participant of/at the IBS-AR Conference, or notice that someone else is being harassed, or have any other concerns, please contact the designated conference allies in the organising team. If the person who is harassing you is one of the allies, they will recuse themselves from handling your incident and will not be given any information without your explicit consent. We will respond as promptly as we can."
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "The workshop is limited in space so please register early to secure your spot."
  },
  {
    "objectID": "workshop.html#registration",
    "href": "workshop.html#registration",
    "title": "Workshop",
    "section": "",
    "text": "The workshop is limited in space so please register early to secure your spot."
  },
  {
    "objectID": "workshop.html#workshops",
    "href": "workshop.html#workshops",
    "title": "Workshop",
    "section": "Workshops",
    "text": "Workshops\nPlease click on Details to see more details about the workshop.\n\nGeneralized Nonlinear Models in R\nBy Heather Turner\n\nRegister\n\n\n\nDate and Time: 24th November 2025, 9AM-5PM\nLocation: Haydon-Allen Building, Australian National University\n\nThe class of generalized linear models covers several methods commonly used in data analysis, including multiple linear regression, logistic regression, and log-linear models. But a linear predictor does not always capture the relationship we wish to model. Rather, a nonlinear predictor may provide a better description of the observed data, often with fewer and more interpretable parameters. This workshop introduces the wider class of generalized nonlinear models (GNMs) and their implementation via the R package gnm. The day will begin with a brief refresher of linear and generalized linear models. The extension to a nonlinear predictor will be motivated by the case of structured interaction models, such as Goodman’s Row-Column association model for contingency tables. The remainder of the day will be devoted to further applications, such as the UNIDIFF models for social mobility data, the Lee-Carter model for mortality data, and bespoke GNMs.\n\nRequirements\nParticipants should\n\nbring a laptop with R installed, along with the gnm, logmult and vcdExtra packages.\nhave basic R knowledge (e.g. you have used R to load data, create simple visualisations, perform basic analyses and write simple functions or more specifically, you are familiar with concepts in Cookbook for R by Winston Chang)\nknow basic statistics (e.g. simple linear regression, hypothesis testing, basic summary statistics and plots)\n\n\n\nDesirable\nFamiliarity with:\n\ngeneralized linear models and/or\nnonlinear least squares models\n\nwould be beneficial but not essential.\n\n\nBiography\nHeather is an Associate Professor and EPSRC Research Software Engineering Fellow in the Statistics Department at the University of Warwick, UK. She has over 20 years of experience in the development of statistical code and software, gained through positions in academia, industry, and as a freelance consultant. In research, she has developed a portfolio of R packages for statistical modelling and collaborated on applications in sports, social science and agriculture. In her work with industry, she has specialised in applications in pharmaceutical R&D, with companies including Pfizer, Johnson & Johnson and Roche. Heather is active in community management and engagement among R users and developers. She is on the board of the R Foundation and chairs the R Contribution Working Group (fostering the community of contributors to the R project) and the R Forwards taskforce (widening the participation of under-represented groups in the R community)."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "Call for abstracts",
    "section": "",
    "text": "We are pleased to invite submissions for abstracts for the Biometrics in the Bush Capital, to be held on 24th-28th November 2025 at Canberra, Australia. The conference aims to bring together researchers, practitioners and educators to share ideas, current research findings, and emerging trends in the development and application of statistical and data science theory and methods to problems in biosciences."
  },
  {
    "objectID": "abstract.html#key-themes-and-topics",
    "href": "abstract.html#key-themes-and-topics",
    "title": "Call for abstracts",
    "section": "Key themes and topics",
    "text": "Key themes and topics\nWe welcome abstracts addressing, but not limited to, the following areas:\n\nAgricultural Statistics\nBayesian Statistics\nData Science and Artificial Intelligence\nDesign or Surveys\nDemographic or Population Modelling\nEcological or Environmental Statistics\nGeneralised Linear Mixed Modelling\nMachine Learning or Deep Learning\nMedical or Health Statistics\nMultivariate Statistics\nSpatial/Temporal Modelling\nStatistical Consulting/Workflow/Practice\nStatistical Genetics/Omics/Bioinformatics"
  },
  {
    "objectID": "abstract.html#submission-guidelines",
    "href": "abstract.html#submission-guidelines",
    "title": "Call for abstracts",
    "section": "Submission guidelines",
    "text": "Submission guidelines\nAbstracts should be a maximum of 250 words written in English presented either as a talk or a poster. Abstract submission is due by 14th July 2025.\n\nSubmit Abstract\n\nFor questions or further information, please contact conference@biometricsociety.org.au."
  },
  {
    "objectID": "travel.html",
    "href": "travel.html",
    "title": "Travel",
    "section": "",
    "text": "By flight, Canberra is:\n\nless than an hour from Sydney,\nabout one hour from Melbourne,\nless than two hours from Adelaide and Brisbane,\nabout four hours from Perth, and\nabout five hours from Auckland.\n\nCanberra Airport is less than 10 minutes from the CBD.\nConnect to the city via rental car, public bus, taxi or with a rideshare operator (e.g. Uber).\nPublic transport accepts Mastercard or Visa (physical card or digital card stored on your smart phone or watch), MyWay+ pass or card, or printed ticket purchased online or machine.\nIf coming from Sydney, it is recommended that you catch a bus to Canberra (about 3.5 hours) that departs from Central Station and Sydney airport to Canberra CBD. Bus companies include Murrays and Greyhound.\nThe conference venue, National Film and Sound Archive of Australia (NFSA), is located close to CBD.\nSee accommodation options here."
  },
  {
    "objectID": "sponsorship.html",
    "href": "sponsorship.html",
    "title": "Sponsorship",
    "section": "",
    "text": "If you would like to sponsor the conference, please contact conference@biometricsociety.org.au or go below for more informaton.\nWe gratefully acknowledge our sponsors for their generous support, which has made this conference possible."
  },
  {
    "objectID": "sponsorship.html#platinum-sponsor",
    "href": "sponsorship.html#platinum-sponsor",
    "title": "Sponsorship",
    "section": "Platinum sponsor",
    "text": "Platinum sponsor"
  },
  {
    "objectID": "sponsorship.html#gold-sponsor",
    "href": "sponsorship.html#gold-sponsor",
    "title": "Sponsorship",
    "section": "Gold sponsor",
    "text": "Gold sponsor"
  },
  {
    "objectID": "sponsorship.html#supporter",
    "href": "sponsorship.html#supporter",
    "title": "Sponsorship",
    "section": "Supporter",
    "text": "Supporter\n\n\n\n\nACT Branch"
  },
  {
    "objectID": "updates.html",
    "href": "updates.html",
    "title": "Updates",
    "section": "",
    "text": "2025-07-14\n\n\n\nAbstract submission extended by one week\n\nLast chance! The abstract submission has been extended by one week and is due now on 21st July 2025 5PM AEST. Those who have already submitted their abstract will get an individual email confirmation and an opportunity to revise their abstract until the new deadline.\n\n\n\n\n2025-06-29\n\n\n\nWorkshops\n\nPre-conference workshops are now open for registration:\n\n\nGeneralized Nonlinear Models in R by Heather Turner\n\n\nAnalysing Complex Survey and Subsample Data (with R) by Thomas Lumley\n\n\nLost in Translation: Speaking Statistician in a Multi-Lingual World by Peter Humburg and Eve Slavich\n\n\nDeep Learning and Computer Vision in R: A Practical Introduction by Patrick (Weihao) Li\n\n\n\nSee more details here. As places are limited, please register early to secure your spot.\n\n\n\n\n\n2025-04-14\n\n\n\n Open for abstract submissions\n\nWe are pleased to invite submissions for abstracts for the Biometrics in the Bush Capital, to be held on 24th-28th November 2025 at Canberra, Australia. The conference aims to bring together researchers, practitioners and educators to share ideas, current research findings, and emerging trends in the development and application of statistical and data science theory and methods to problems in biosciences.  Key themes and topics We welcome abstracts addressing, but not limited to, the following areas:\n\nAgricultural Statistics\n\n\nBayesian Statistics\n\n\nData Science and Artificial Intelligence\n\n\nDesign or Surveys\n\n\nDemographic or Population Modelling\n\n\nEcological or Environmental Statistics\n\n\nGeneralised Linear Mixed Modelling\n\n\nMachine Learning or Deep Learning\n\n\nMedical or Health Statistics\n\n\nMultivariate Statistics\n\n\nSpatial/Temporal Modelling\n\n\nStatistical Consulting/Workflow/Practice\n\n\nStatistical Genetics/Omics/Bioinformatics\n\n Submission guidelines\n\nAbstracts should be a maximum of 250 words written in English presented either as a talk or a poster.  Submit your abstract here by 14th July 2025. For questions or further information, please contact conference@biometricsociety.org.au.\n\n\n\n\n\n2025-02-24\n\n\n\n Call for invited sessions and lunchtime discussions\n\nThe organising committee invites expression of interest for invited sessions and lunchtime discussion topics to be held during the conference. Invited sessions aim to bring together experts to discuss emerging trends, present cutting-edge research, and engage in meaningful discussions on specialised topics that will be of interest to the biometrics community, over a 90 minute period. Lunchtime discussions, over the 60 minute lunchtime break, are an opportunity to discuss in parallel forums issues or topics of common interest to attendees, e.g. what skills are needed by modern biometricians, how should we interact with the machine learning and artificial intelligence community, best practices for statistical consulting, emerging research topics and so on.\n\n\n\nNo matching items"
  },
  {
    "objectID": "social.html",
    "href": "social.html",
    "title": "Social",
    "section": "",
    "text": "Use #BIBC2025 in your posts to share your experience at the conference!\n\nFollow @biometricsociety.org.au on Bluesky for the latest updates!"
  },
  {
    "objectID": "social.html#photos",
    "href": "social.html#photos",
    "title": "Social",
    "section": "Photos",
    "text": "Photos\nIf you take photos during the conference, please share them with us! You can send the photo to conference@biometricsociety.org.au or tag us on social media using #BIBC2025."
  },
  {
    "objectID": "social.html#organised-tours",
    "href": "social.html#organised-tours",
    "title": "Social",
    "section": "Organised tours",
    "text": "Organised tours\nThe following tours are available for conference attendees on Wednesday afternoon. Please ensure to register for these tours when you register for the conference.\nIf you did not register for these in the conference registration, you can register for them separately here.\n\nMurrumbateman Winery Tour ($140)\nDate: Wednesday 26 November\nThe tour will commence with a pick-up at 1.15pm (near NFSA), then tour at Clonakilla at 2pm, followed by tasting and cheese patter at McKellar Ridge Winery at 3.15pm. The departure will be at 4pm with return to NFSA around 5pm.\nPlease note that we have to have a minimum of 4 sign ups for this tour to go ahead.\n\n\nTidbinbilla Nature Reserve ($35)\nDate: Wednesday 26 November\nThe tour will begin with a pick-up at 1pm from NFSA, arriving at Tidbinbilla Nature Reserve around 2pm. Once there, you will have the chance to enjoy bushwalking and spot native Australian wildlife, including platypuses, koalas, and rock-wallabies. We expect the bushwalk to be relatively easy and suitable for most fitness levels. The return trip will depart at 4.30pm, with an expected arrival back at NFSA around 5.30pm.\nPlease note that we have to have a minimum of 20 sign ups for this tour to go ahead.\n\n\nLake Burley Griffin (FREE)\nDate: Wednesday 26 November\nThe tour will begin with meeting at 1.30pm at NFSA then having a guided walk around Lake Burley Griffin. The walk will be about 5km in total to be completed in about 2 hours. The walk is relatively easy so any able person will not have too much difficulty completing the walk. We recommend that you wear a hat, wear sunscreen and bring a bottle of water with you."
  },
  {
    "objectID": "social.html#self-guided-tours",
    "href": "social.html#self-guided-tours",
    "title": "Social",
    "section": "Self-guided tours",
    "text": "Self-guided tours\nIf none of the organised tours suit you, you are most welcomed to self-organise yourself or with others your own tour. Canberra has many great attractions, including:\n\nAustralian War Memorial\nAustralian National Botanic Gardens\nMount Ainslie Lookout\nMusuem of Australian Democracy at Old Parliament House\nNational Arboretum\nNational Gallery of Australia\nNational Library of Australia\nNational Museum of Australia\nNational Portrait Gallery\nNational Zoo & Aquarium\nParliament House\nQuestacon"
  },
  {
    "objectID": "book/sessions.html",
    "href": "book/sessions.html",
    "title": "Schedule",
    "section": "",
    "text": "Please note that this is a draft program and is subject to change."
  },
  {
    "objectID": "book/sessions.html#tuesday",
    "href": "book/sessions.html#tuesday",
    "title": "Schedule",
    "section": "Tuesday",
    "text": "Tuesday\n\n\n\n\n\n\n\n\n\n\nStart\nArc Cinema\nTheatrette\n\n\n\n\n08:15\nConference Registration\n\n\n\n08:45\nOpening remarks and housekeeping\n\n\n\n09:05\nChair: Chris Brien\n\n\n\n\nOn Finding Good Experiments by Cheng Soon Ong\n\n\nAbstract\n\nOne of the key choices we have as scientists is to design informative experiments. With computational methods like AI promising accurate predictions, we revisit the question of adaptively designing new measurements that take previous data into account. Using examples from genomics, we illustrate some recent ideas on using machine learning to recommend experiments. Then we discuss potential impacts on choosing measurements in spatiotemporal problems. We conclude by outlining some opportunities and challenges of including machine learning in the scientific discovery process.\n\n\n\n\n10:00\nChair: Garth Tarr\nChair: Scott Foster\n\n\n\nData-Adaptive Automatic Threshold Calibration for Stability Selection by Martin Huang\n\n\nAbstract\n\nStability selection has gained popularity as a method for enhancing the performance of variable selection algorithms while controlling false discovery rates. However, achieving these desirable properties depends on correctly specifying the stable threshold parameter, which can be challenging. An arbitrary choice of this parameter can substantially alter the set of selected variables, as the variables' selection probabilities are inherently data-dependent. To address this issue, we propose Exclusion Automatic Threshold Selection (EATS), a data-adaptive algorithm that streamlines stability selection by automating the threshold specification process. EATS initially filters out potential noise variables using an exclusion probability threshold, derived from applying stability selection to a randomly shuffled version of the dataset. Following this, EATS selects the stable threshold parameter using the elbow method, balancing the marginal utility of including additional variables against the risk of selecting superfluous variables. We evaluate our approach through an extensive simulation study, benchmarking across commonly used variable selection algorithms and static stable threshold values.\n\nEstimating abundance in small populations using pedigree reconstruction by Sarah Croft\n\n\nAbstract\n\nAccurate measures of abundance are essential for successful monitoring of animal populations, and for assessing the efficacy of conservation interventions. In previous work, genotypic information has been incorporated into Mark-Recapture models to enable the identification of individuals, as well as determination of kinship between observed individuals in Close-Kin Mark-Recapture (CKMR) models. Generally, CKMR models make large sample assumptions limiting their application to many endangered and at-risk species. We have developed Bayesian methodology to estimate population abundance and dynamics for small, isolated populations using pedigree reconstruction. The true underlying pedigree completely describes the abundance and population structure over time, however the true relationships between individuals in wild populations are rarely known. Given a set of observed genotypes, along with supplementary data, our methodology is able to successfully reconstruct the pedigree without the need for large sample assumptions. Prior knowledge of the mating structure and reproductive dynamics of the population can also be incorporated in the model. In this talk I will present our pedigree reconstruction approach for population estimation using dead recovery data, and will discuss the challenges associated with the full pedigree approach.\n\n\n\n\nVariable Selection in a Joint Model for Huntington's Disease Data by Rajan Shankar\n\n\nAbstract\n\nHuntington's disease is a neurodegenerative disease caused by a defective Huntingtin gene, with symptoms that progressively worsen and eventually lead to a clinical diagnosis. Identifying the clinical and demographic factors that influence symptom severity and time-to-diagnosis is critical for understanding disease progression so that early-intervention strategies can be timely implemented. We propose a joint model to relate symptom severity $y$ and time-to-diagnosis $x$, conditional on clinical and demographic predictor variables $\\mathbf{z}$. However, it may be that certain predictor variables are important for $y$ but not for $x$ and vice-versa, so we use regularisation techniques to select different sets of predictor variables for $y$ and $x$. Since $x$ is a time-to-event variable, there is the added challenge that many of its values are right-censored due to individuals who did not develop the disease during the study. Therefore, to fit the joint model, we apply the expectation-maximisation (EM) algorithm to alternate between parameter estimation and imputation of the right-censored values until convergence. We demonstrate our method on Huntington's disease patient data, showcasing how users can choose appropriate values for the regularisation tuning parameters.\n\nAccounting for heterogeneous detection rates when inferring eradication of an invasive species by Sean A. Martin\n\n\nAbstract\n\nInvasive species eradications are central to protecting island biodiversity, yet accurately declaring when eradication has occurred remains difficult. Detection is both imperfect and variable, leading to significant uncertainty in the state of eradication once detections of the target population cease. Contemporary eradication inference models account for imperfect detection, however, and critically, they ignore natural variation in rates of detection between individuals and across time and space. Although capture-mark-recapture studies have shown that ignoring individual variation in detection leads certain models to biased estimates of population parameters (e.g. population size), few such examinations have been applied to eradication contexts. In this presentation, I will highlight how costly it is to assume homogeneous detection during eradication campaigns and describe how we have incorporated variable detection rates into an eradication inference model using an ABC-SMC framework. I will elaborate on the problems that arose due to the inherent and significant stochasticity of the system, how we overcame them, and future work required to make this model applicable to case studies.\n\n\n\n\nStableMate: a regression framework for selecting stable predictors across heterogeneous data environments by Yidi Deng\n\n\nAbstract\n\nInferring reproducible relationships between biological variables remains a challenge in the statistical analysis of omics data where p &gt; 10,000 and n\n\nZero-inflated Tweedie distribution for abundance of rare ecological species by Nokuthaba Sibanda\n\n\nAbstract\n\nAbundance data for rare species can be extremely zero-inflated, where percentage of zeros can be over 90%. This poses a challenge even for the standard Tweedie model which naturally allows for a probability mass at zero with continuous non-negative values. We investigate use of a zero-inflated Tweedie distribution when modelling non-negative continuous abundance values with zeros for rare species. Despite their significance, research on zero-inflated models has predominantly focused on count models such as zero-inflated Poisson or negative binomial regressions, with only recent studies exploring zero-inflated Tweedie models in insurance claims (Zhou, Qian, and Yang 2022; Gu2024; So and Valdez 2024; So and Deng 2025). The zero-inflated Tweedie model uses a mixture model approach that integrates a Tweedie model with a binary model to distinguish between excess zeros - those resulting from an independent process, and true zeros those resulting from the Tweedie model itself. We use a Bayesian approach to estimate the model parameters. We model means of the Tweedie model using a log-link function with covariates and unobserved random effects. The spatial association between observations is accounted for using a conditionally auto-regressive prior.\n\n\n\n11:00\nMorning Tea\n\n\n\n11:30\nChair: Alan Welsh\nChair: James Curran\n\n\n\nExtending Spatial Capture-Recapture with the Hawkes Process by Alec B. M. van Helsdingen\n\n\nAbstract\n\nSpatial capture-recapture (SCR) is a well-established method used to estimate animal population size from animal sighting or trapping data. Standard SCR methods assume animal movements are independent and consequently cannot incorporate site fidelity (attachment to a particular region) nor the temporal correlation of an animal’s location. Recent work has sought to solve these issues by explicitly modelling animal movement. In this talk we propose an alternative solution for camera trapping surveys based on a multivariate self-exciting Hawkes process. Here the rates of detection of a given animal at a given camera are a function of not only the location and its proximity to the animal’s activity center, but also where and when the animal was most recently detected. Through a mixture of Gaussian distributions, our model expects more detections closer in space to the last detection, and reduces to SCR when an animal is yet to be detected. This formulation, we believe, better reflects animal behaviour because shortly after detection, we expect to see an individual close to where it was last seen. Thus, our model allows us to account for both site fidelity and the inherent temporal correlation in detections that have not previously been accounted for in SCR-type models. In this talk, I will 1) give an overview of Self-Exciting Spatial Capture-Recapture (SESCR) models, and 2) demonstrate the additional inference that can be drawn from such models and 3) apply the framework using a few case studies to compare traditional SCR and SESCR.\n\nGroup Sampling with Imperfect Testing for Biosecurity Applications by Adele Jackson\n\n\nAbstract\n\nGroup sampling, also known as pooled or batch sampling, is a standard technique in biological sciences and the health sector to use limited resources efficiently. Common objectives include detecting pest species and inferring prevalence of disease in communities, livestock or wildlife. The purpose of this paper is to support the design of robust group sampling strategies when testing processes are imperfect. We formulate analytical distributions and statistics for grouped hypergeometric sampling and its binomial approximation that incorporate a variety of types of imperfect test. These include tests that respond to the presence or absence of contaminated material in a group, as well as PCR and serological testing processes where sensitivity depends on the number of contaminated items in each group. We also formulate the Hellinger information of a sampling scheme, which allows us to develop group sampling design strategies that increase the accuracy of inferred prevalence. This is an essential component in decision-making during outbreaks of disease and in operational biosecurity applications. Based on this work, we estimate leakage through a grouped sampling scheme and show how accounting for leakage can alter sampling strategies and improve risk management decisions.\n\n\n\n\nA Test for Detecting Multiple Clusters with Hotspot Spatial Properties by Kunihiko Takahashi\n\n\nAbstract\n\nVarious statistical tests have been widely used in spatial epidemiology to investigate regional patterns in disease occurrence, particularly to assess whether disease risk is significantly elevated in specific areas compared to neighboring regions or adjacent time periods. One such method is the cluster detection test (CDT), which identifies non-random spatial distributions of diseases and highlights high-prevalence regions without prior assumptions. Among CDT methods, scan statistics are compelling and use a maximum likelihood framework to search across spatial and/or temporal windows for potential clusters. Examples include Kulldorff’s circular scan statistic and the flexibly shaped scan statistic by Tango and Takahashi. More recently, Takahashi and Shimadzu proposed a scan-based method that simultaneously detects multiple clusters by integrating generalized linear models and an information criterion to determine the optimal number of clusters. Traditional scan-based tests often assume that disease risk is uniformly elevated within a single cluster. However, they may mistakenly combine multiple adjacent hotspots—each with potentially different risk levels—into one, thereby masking meaningful spatial heterogeneity. In this study, we propose a new test procedure that more accurately identifies adjacent hotspot clusters as distinct entities. Our approach enhances the scan statistic framework by incorporating Cochran’s Q-statistic to assess heterogeneity within clusters. We demonstrate the effectiveness of the proposed method through real-world applications and compare its performance with conventional scan-based tests.\n\nKoala Distribution and Abundance by Scott D. Foster\n\n\nAbstract\n\nThe koala (*Phascolarctos cinereus*) is a well-known and studied Australian marsupial, but the species presents a complex case for conservation. Currently, most koala conservation efforts focus on local-scale population estimates, which are often based on expert opinion or anecdotal evidence, and either precede or ignore most (or all) available data. In contrast, conservation listing advice and associated recovery plans require population estimates at the species-range scale. A data-driven, national-scale population estimate including all available information will therefore guide effective management of koala populations by providing high-quality and objective information. To this end, we have designed nationally consistent survey, implemented it (with the aid of partners) and analysed the resulting data (and others). The design uses recently developed techniques (clustered spatially-balanced designs) whilst the analysis uses emerging models that incorporate multiple data types (e.g. point process, binary and count) that are often-enough collected using different equipment, protocols and staff. Integrated species distribution models (ISDMs) have, at their heart, a simple point process but this simplicity still allows for some complexity in terms of how different types of data can inform the point process. The model, when fitted to koala data, indicates that the distribution of koalas is patchy throughout much of eastern Australia. It also infers that there are more koalas than previously guessed. Our estimates provide unprecedented evidence to support nationally consistent and spatially explicit decision-making for koala conservation, and do so with relevant measures of uncertainty. \n\n\n\n\nOutlier-robust estimation of state-space models using a penalised approach by Garth Tarr\n\n\nAbstract\n\nState-space models are a broad class of statistical models for time-varying data. The Gaussian distributional assumption on the disturbances in the model leads to poor parameter estimates in the presence of additive outliers. Whilst there are ways to mitigate the influence of outliers via traditional robust estimation methods such as M-estimation, this issue is approached from a more modern perspective that utilises penalisation. A shift parameter is introduced at each timepoint, with the goal being that outliers receive a non-zero shift parameter while clean timepoints receive a zero shift parameter after estimation. The vector of shift parameters is penalised to ensure that not all shift parameters are trivially non-zero. Apart from making it feasible to fit accurate and reliable time series models in the presence of additive outliers, other benefits of this approach include automatic outlier flagging and visual diagnostics to provide researchers and practitioners with better insights into the outlier structure of their data. We will demonstrate the utility of this method on animal tracking data.\n\nSpeed: An R package for Spatially Efficient Experimental Designs by Sam Rogers\n\n\nAbstract\n\nAgricultural field trials are typically designed using robust statistical randomisation, and best practice agricultural field trials also consider the spatial layouts of the experiment, and how that interacts with the treatments of interest. However, software providing access to spatially optimised experimental designs for field trials is not readily available and can be difficult for new users to get started with due to sparse documentation. In this talk, we discuss the newly developed *speed* R package which provides easy access to a fully open-source package with comprehensive documentation to enable the design of spatially optimal experiments. The package offers model-free spatially optimal designs via a simulated annealing optimisation algorithm. It can produce a spatially optimal version of many types of experimental designs commonly used in agricultural research as well as many more complex designs such as incomplete block designs and partially replicated designs. It provides multiple objective functions out of the box, with the additional flexibility to choose or enable custom optimisation metrics, depending on the objective of the researcher. It also provides some helper functions for plotting and evaluating experimental designs either produced via *speed* or alternative design packages. To demonstrate the package's capabilities, we present spatially optimal designs for challenging scenarios including two-dimensional blocking and partially replicated designs. We also show that speed is significantly faster compared to alternative software.\n\n\n\n\nDisease cluster detection via functional additive models incorporating spatial correlation by Michio Yamamoto\n\n\nAbstract\n\nDetecting spatial clusters of diseases is crucial for understanding disease patterns and developing effective prevention and treatment strategies. Spatial scan statistics are powerful tools for detecting spatial clusters with a variable scanning window size. If covariates are related to an outcome and not geographically randomly distributed, searching for spatial clusters may require adjusting for the covariates. In addition, spatial correlation in the outcome, which is often overlooked during cluster detection, can affect the results. In this study, we propose a new spatial scan statistic that handles multiple functional covariates indicating past information over time and the spatial correlation of the outcome. Our method flexibly models these factors in the framework of functional additive models. We develop an optimization algorithm to estimate the model parameters for the normal outcome case. A simulation study and real data analysis indicate that the proposed method can detect disease clusters despite longitudinal covariates and spatial correlations compared to existing methods.\n\nRunning Human Subject Experiments via Online Crowdsourcing by Patrick Li\n\n\nAbstract\n\nCrowdsourcing platforms such as Amazon Mechanical Turk and Prolific provide scalable and accessible tools for running online human subject experiments. This talk offers an overview of how to design, set up, and manage such studies effectively. Drawing on experience from multiple projects, I will walk through the key steps for obtaining ethics approval, compare platform workflows, and discuss considerations for participant recruitment, screening, task design, quality control, cost estimation, and common pitfalls. This session is intended for researchers planning to conduct behavioural, perceptual, or decision-making experiments, as well as those developing data annotation pipelines for machine learning or applied research.\n\n\n\n12:50\nLunch\n\n\n\n13:50\nChair: Francis Hui\nChair: Ruth Butler\n\n\n\nBayesian clustered ensemble prediction for multivariate time series by Shonosuke Sugasawa\n\n\nAbstract\n\nWe propose a novel methodology called the mixture of Bayesian predictive syntheses (MBPS) for multiple time series count data and apply the methodology to predict the numbers of COVID-19 inpatients and isolated cases in Japan and Korea at the subnational level. MBPS combines a set of predictive models and partitions the multiple time series into clusters based on their contribution to predicting the outcome. In this way MBPS leverages the shared information within each cluster and avoids using a multivariate count model, which is generally cumbersome to develop and implement. Our data analyses demonstrate that the proposed MBPS methodology has improved predictive accuracy and uncertainty quantification.\n\nBuilding Trust Without Peer Review: Establishing Reproducibility Standards in Industrial Statistical Consulting by Dean Marchiori\n\n\nAbstract\n\nIn academic research, peer review serves as a cornerstone for ensuring the credibility and reproducibility of findings. However, in industry, statistical consultants often operate without the formal structures of peer review, posing gaps in ensuring the reproducibility and trustworthiness of their analyses. This talk explores how statistical consultants for industry can proactively establish and adhere to standards that promote reproducibility and foster trust, even in the absence of traditional peer review mechanisms. Researchers, data scientists and statisticians often engage with and provide expertise to government, industry and other groups outside of academia. This expertise is often trusted implicitly and relied on to make important decisions. On the other hand, there are many well known criticisms of conventional peer review systems both in academic publishing and in commercial work. Drawing on practices from various industries, we will propose a pragmatic framework for the development and implementation of standardized reporting formats, the use of open-source tools for reproducible analysis, and the adoption of best practices for documentation and code sharing. We will also examine the role of professional organizations and industry consortia in setting guidelines that encourage transparency and accountability. Attendees will gain insights into practical strategies for implementing reproducibility standards in their own consulting and research practices and contribute to a broader movement towards transparency and accountability in industrial statistics.\n\n\n\n\nThe difficulties of clustering categorical or mixed data by Louise McMillan\n\n\nAbstract\n\nI will discuss techniques for clustering categorical data that go beyond treating the data as numerical or converting the categories to dummy variables. I will talk about a range of approaches to clustering categorical data, including the \\texttt{clustord} R package for clustering ordinal data, which uses model-based clustering via likelihoods and the EM algorithm. Then I will discuss a Bayesian approach from population genetics that may be extendable to general mixed datasets and is the subject of my latest research.\n\nTeaching Meta-Analysis for Systematic Reviewers with Mixed Statistical Training by Xu Ning\n\n\nAbstract\n\nHigher-degree research (HDR) students often consider conducting a systematic review of their research domain, with the goal of synthesising their findings with a meta-analysis. However, the statistical training of HDR students and their supervisors can range from little to comprehensive. Consequently, we found in our consultations with these students that they may struggle in specifying their meta-analytical model, misinterpret their model outputs and diagnostics, or both, especially with complex meta-analytical models. Hence, we have developed a short course on meta-analysis to address these knowledge gaps. The course aims to develop students’ mastery in meta-analytical methods. We will present empirical findings on whether the course’s aim was met and discuss the pedagogical considerations made to accommodate the range of statistical training in our target cohort.\n\n\n\n\nSpecies archetype models for presence-only data by Skipton N.C. Woolley and Scott Woolley\n\n\nAbstract\n\nJoint species distribution modelling is a recently emerged and potentially powerful statistical method for analysing biodiversity data. Despite the plethora of presence-only occurrence data available for biodiversity analysis, there remain few examples of presence-only multiple-species modelling approaches. We present a mixture-of-regressions model for understanding how groups of species are distributed based on presence-only data. Our approach extends Species Archetype Models using a point process framework and incorporates joint estimation of sighting bias based on all species occurrences included in the model. We demonstrate that our method can accurately recapture mean and variance of parameters from simulated data sets and provide better estimates than those generated from multiple single species presence-only species distribution models. We apply our approach to a Myrtaceae presence-only occurrence dataset from New South Wales, Australia. We show that presence-only Species Archetype Models allow for the propagation of variance and uncertainty from the data through predictions, improving inference made on presence-only biodiversity data for multiple species.\n\nTales from the jungle: a personal perspective of statistical consulting since COVID by Alice Richardson\n\n\nAbstract\n\nThe period since COVID-19 has brought unique challenges to statistical consulting, with researchers venturing into unfamiliar statistical territory. The activities of an academic statistical consulting practice have therefore very much taken on the feel of a journey through a statistical jungle. In this talk I will describe some of the more alarming encounters I have experienced in this jungle over the last five years. Examples will be selected from my client interactions within the broad biometric research field. Names and topics will be altered to protect the participants, but not so much as to alter the key learnings from the encounters. These examples will cover the entire range of statistical endeavour. From power and sample size calculation through data collection to modelling, presentation of results and visualisation, no area of statistical activity is untouched by surprising methodological suggestions from researchers. The simultaneous emergence of large language models has also brought unexpected consequences in consulting practice, as researchers increasingly turn to AI tools for statistical guidance. I’ll draw out the patterns of common traps and highlight how the biometric community can assist and advise. I will also discuss some of the practical strategies I have developed to carve out a safe path. These strategies can be used for navigating similar consulting challenges and for fostering better statistical practices in other collaborative research environments beyond academia.\n\n\n\n\n\nBetter Conversations, Better Support: Strengthening Consulting through Practical Education and Community by Sharon G. Nielsen\n\n\nAbstract\n\nEffective statistical consulting in agriculture depends not just on technical expertise, but on shared understanding, clear communication, and the confidence of those seeking advice. Through its series of practical agronomic workshops and the linked Community of Practice, the Biometry Hub has shown how targeted education can transform the way consulting support is delivered and received. The workshops, typically four core sessions delivered over five days, cover the design of robust experiments, sound data collection, practical analysis, and clear interpretation of results. By equipping researchers and industry staff with these practical skills, the workshops lay the groundwork for more productive conversations between statisticians and collaborators. The Community of Practice then extends this support beyond the workshop room. Monthly meetings held over Zoom create an informal space for participants to revisit concepts, tackle real-world questions, and build confidence alongside peers and mentors. This talk will share how combining hands-on training, tailored tools like the biometryassist R package, and an ongoing network of support strengthens the consulting relationship, helping ensure that good statistical practice is not only taught, but actively put into practice across the grains industry.\n\n\n\n15:20\nAfternoon Tea\n\n\n\n15:50\nChair: Zhanglong Cao\nChair: Graham Hepworth\n\n\n\nNested-factorial treatment models: types, their uses and examples by Chris Brien\n\n\nAbstract\n\nMost commonly factorial experiments involve several crossed treatment factors and the treatment model includes terms for all possible combinations of the treatment factors. However, treatment models that employ nested treatment factors are beneficial in a number of situations and could be used more often. Applicable situations include those in which (i) treatment factors are intrinsically nested, (ii) treatments involve one or more factors plus a control and (iii) treatments have been systematically allocated and/or are pseudoreplicated. Four basic types of nested factors that start with a single nesting factor are (i) one nested factor, (ii) multiple, crossed, nested factors, (iii) multiple, independent, nested factors, and (iv) a hierarchy of multiple, nested factors. Examples involving the different situations and types of nesting will be presented. Constructing the factors using the `R` package `dae` (Brien, 2025, &lt;&gt;) will be described and the properties of the models discussed for the examples via their anatomies produced using `dae`.\n\nEnhancing Fraud Detection in Banking through Random Survival Forests: Addressing Data Imbalance and Model Transparency by Arjun Sekhar\n\n\nAbstract\n\nWith the rise in financial fraud amid growing transaction volumes, in this presentation we share our insights from the use of Random Survival Forests (RSFs) for enhancing fraud detection in banking systems. While many machine learning techniques prioritise accuracy, they overlook two crucial issues: the imbalance of fraud versus legitimate transaction data, and the transparency of the model decision-making process. We position RSFs as a potential solution, leveraging their time-to-event structure to enable dynamic fraud prediction and their ensemble nature to handle high-dimensional, skewed datasets. Using real-world transactional data, the presentation addresses model imbalance through resampling strategies and evaluates the model’s ability to detect minority-class fraud events without inflating false positives. Furthermore, the interpretability of RSFs is explored through explainable AI frameworks, offering transparency essential for regulatory compliance and institutional trust. By investigating RSFs to deliver fraud detection that is both accurate and accountable, this presentation bridges algorithmic robustness with transparency, advancing a practical and governance-aligned solution for responsible AI deployment in today’s increasingly regulated financial services landscape.\n\n\n\n\nIntegrating Spatial Data and On-Farm Experimentation to Understand Wheat Variety Performance Across Western Australia by Sandra K. Tanz\n\n\nAbstract\n\nUnderstanding spatial variability in crop performance is critical to improving the statistical design and interpretation of variety trials at the farm scale. This pilot study investigates the integration of large-scale on-farm experimentation (OFE) with spatial data analysis to better understand genotype-by-environment interactions in wheat. The trial spans five grower-managed sites across Western Australia, incorporating two core wheat varieties—Scepter (a widely grown commercial line) and IGW6993 (a near-release InterGrain line)—with some growers also including Rockstar and Tomahawk CL Plus based on local relevance. Each trial is embedded within commercial paddocks and implemented using strip trial designs aligned with paddock production zones identified from historical yield maps. This design allows statistical comparisons of varietal performance across contrasting spatial zones defined by environmental variation in soil, topography, and management history. The study aims to (1) develop robust methods for spatially aware experimental design in commercial cropping systems, and (2) assess the potential for OFE to complement small-plot trials in supporting variety selection and agronomic decision-making. While the trial was only recently seeded (May 2025), preliminary outputs include trial design validation and early NDVI imagery from emergence. Future analyses will incorporate additional covariates such as soil electrical conductivity (EM mapping), NDVI time series, and topographic indices to model spatial responses. This work highlights the value of integrating spatial data into trial workflows and explores the role of predictive breeding tools in commercial agriculture.\n\nUsing point cloud data to discover genomic regions associated with dynamic height by Colleen H Hunt\n\n\nAbstract\n\nPlant height in grain sorghum (*Sorghum bicolor L. Moench*), influences yield, lodging resistance, and harvestability, yet conventional methods usually measure plant height at maturity, but this single-time-point measurement misses the growth dynamics across the season. Tracking and analysing height over time can reveal more about the genetic factors controlling development rate and pattern. Data from 881 sorghum lines from a diverse population were planted in a partially replicated design with 1190 plots. UAV-based high-throughput phenotyping was used to generate weekly plant height measurements from emergence to flowering. These aerial images were collected and processed using photogrammetric software to create dense point clouds and canopy height models for each plot. A logistic regression model within a linear mixed model framework was applied to effectively depict the sigmoidal growth pattern typical of sorghum, with parameters indicating maximum height, growth rate, and inflection point. These biologically relevant parameters offered a straightforward, quantitative summary of each genotype’s growth trajectory. The logistic growth parameters were then used in a genome-wide association study (GWAS) to identify loci linked to the dynamic aspects of height development, with several significant associations detected. This is among the first studies to combine logistic growth modelling of high-throughput UAV data with GWAS to dissect the genetic control of growth dynamics in sorghum. Our findings underline the usefulness of logistic regression for modelling crop growth and showcase the effectiveness of combining UAV phenotyping, statistical growth modelling, and GWAS to identify genomic regions that influence developmental trajectories in sorghum.\n\n\n\n\nMulti-environment trial analysis of count data with complex variance structures using generalised linear mixed models by Michael H. Mumford\n\n\nAbstract\n\nThe analysis of response data from agricultural field experiments conducted in multi-environment trials (METs) is typically performed using linear mixed models (LMMs). The strength of the LMM framework is in the ability to seamlessly model experimental design terms and to include complex variance structures, especially for unbalanced data. When the response variable is count data, the assumptions underpinning the LMM are violated and it is necessary to extend to a generalised linear mixed model (GLMM) approach. Statistical modelling using GLMMs introduces additional complexities arising from a combination of (i) susceptibility to large estimation biases due to approximations of the marginal likelihood, (ii) accounting for heterogeneity of variance/dispersion, and (iii) the increase in computational resources required. In this talk, a statistical methodology is proposed for the MET analysis of count data. The analysis approach uses a GLMM framework, assuming an underlying mean-parameterised Conway-Maxwell Poisson distribution, that can account for arbitrarily under and over-dispersed count data. This framework enables partitioning of residual variation from genetic and other extraneous sources of variation, and adopts a factor-analytic model for the genotype by environment interaction effects. The proposed methodology is applied to a series of common bean trials, where the aim is genotype selection for the response variable pod count per plant. The analysis is implemented using the `glmmTMB R`-package, which uses automatic differentiation to enhance computational speed, the Laplace approximation estimation method to reduce estimation biases, and a residual maximum likelihood (REML)-like correction to further reduce estimation biases for variance components.\n\nAutomatic debiased machine learning (autoDML) for causal inference: implementation and evaluation in real-world observational studies by Tong Chen\n\n\nAbstract\n\nThe estimation of average treatment effect (ATE) is a central goal of observational health research. Causal machine learning (CML) methods are increasingly popular for ATE estimation, as they leverage data-adaptive algorithms to account for complex relationships amongst confounders, exposure and outcome while ensuring valid inference. Major CML methods, such as double machine learning and targeted maximum likelihood estimation, offer robustness and efficiency advantages but they require first estimating the propensity score and then calculating treatment divided by the propensity score, known as the Riesz representer. Standard machine learning algorithms for the propensity score estimation are suboptimal as they are designed to minimise prediction error, ignoring that equally large estimation errors in small and large propensity scores propagate very differently into the ATE estimates upon taking the reciprocal. Automatic debiased machine learning (autoDML), introduced by Chernozhukov et al. (2021), addresses this issue by directly estimating the Riesz representer using machine learning methods, yielding a more stable alternative to standard CML methods. Despite its theoretical advances, autoDML remains largely unknown in biostatistical research and has not been compared to standard CML methods in realistic simulation studies. In this talk, we bridge these gaps by introducing autoDML and highlighting its advantages. We describe a practical, step-by-step guide for implementing autoDML, and compare autoDML with standard CML methods using comprehensive, realistic simulation studies. We further illustrate the utility of autoDML using data from the Longitudinal Study of Australian Children to evaluate the impact of overweight or obesity on cardiovascular outcomes in adolescence."
  },
  {
    "objectID": "book/sessions.html#wednesday",
    "href": "book/sessions.html#wednesday",
    "title": "Schedule",
    "section": "Wednesday",
    "text": "Wednesday\n\n\n\n\n\n\n\n\n\n\nStart\nArc Cinema\nTheatrette\n\n\n\n\n08:30\nConference Registration\n\n\n\n08:50\nHousekeeping\n\n\n\n08:55\nChair: Thomas Lumley\n\n\n\n\nUses of gnm for Generalized (Non)linear Modelling by Heather L. Turner\n\n\nAbstract\n\nThe R package {gnm} was designed as a unified interface to fit Generalized Nonlinear Models: _generalized_ to handle responses with restricted range and/or a variance that depends on the mean, and _nonlinear_ to allow the predictor for the mean to be nonlinear in its parameters. This framework covers several models that were proposed in the literature and adopted in practice before {gnm} was released, but used to require a mixed bag of specialised software to fit. With {gnm} celebrating its 20th birthday this year, it's a good time to review how the package is being used. I'll highlight some of the applications we were aware of when {gnm} was first developed, that remain in common use, and explore more recent applications, particularly in the field of biometrics. We'll discover one motivation for using {gnm}, is for the \"eliminate\" feature that efficiently estimates stratification parameters. This can be useful even when the predictor is linear, as in the case of using conditional Poisson models to analyse case-crossover studies in epidemiology. We'll also look at two of the packages that have built on {gnm}. The first, {multgee}, uses {gnm} to fit multiplicative interactions for certain correlation structures when modelling categorical data, with applications in public health, agriculture, and psychology. The second, {VFP}, is a more specialised package that uses {gnm} to model the mean-variance relationship for in-vitro diagnostic assays. Through these use cases we'll see how different features of {gnm} can be applied, demonstrating the versatility of this software.\n\n\n\n\n09:50\nChair: David Warton\nChair: Sam Rogers\n\n\n\nScalable finite mixture of regression models for clustering species responses in ecology by Francis KC Hui\n\n\nAbstract\n\nWhen modeling species assemblages in ecology, clustering species with similar responses to the environment can facilitate a more parsimonious understanding of the assemblage, and improve prediction by borrowing strength across species within the same cluster. One statistical method for achieving the above is species archetype models (SAMs), a type of finite mixture of regression model where species are clustered according to the *shape* of their environmental response. In this talk, we introduce approximate and scalable SAMs or asSAMs, which overcomes some of the current computational drawbacks in fitting SAMs. We show how asSAMs promotes fast uncertainty quantification via bootstrapping, along with fast variable selection on archetypal regression coefficients courtesy of a sparsity-inducing penalty. Simulation studies and an application to presence-absence records of over 230 species from the Great Barrier Reef Seabed biodiversity project demonstrate asSAMs can achieve similar to or better estimation, selection, and predictive performance than several existing methods in the literature.\n\nUsing a linear mixed model based wavelet transform to model non-smooth trends arising from designed experiments by Clayton Forknall\n\n\nAbstract\n\nThe linear mixed model (LMM) representation of the cubic smoothing spline is a powerful tool for modelling smooth trends arising from designed experiments. However, when trends arising from such experiments are non-smooth, meaning that they are characterised by jagged features, spikes and/or regions of rapid change approximating discontinuities, smoothing spline techniques prove ineffective. A technique that has proven useful for the modelling of non-smooth trends is the wavelet transform. Existing methods to incorporate the wavelet transform into the LMM framework are varied, but often share a common limitation; that is, a reliance on classical wavelet approaches that require observations to be equidistant and dyadic ($\\log_{2}(n)$ is an integer) in number. More recently, second generation wavelet methods have been developed, which overcome the limiting constraints imposed by classical wavelet approaches, enabling the wavelet transform to be applied to sets of non-equidistant observations, of any number. We present a method for the incorporation of these second generation wavelets, namely second generation B-spline wavelets, into an LMM framework to facilitate the wavelet transform. Furthermore, using the structure implicit in the resulting B-spline wavelet basis, we propose extensions to the LMM framework to enable heterogeneity of the associated wavelet variance across wavelet scales. This provides a new LMM based method which enables the flexible modelling of non-smooth trends arising in the conduct of designed experiments. The proposed method is demonstrated through application to a data set exhibiting non-smooth characteristics, that arises from a designed experiment exploring the proteome of barley malt.\n\n\n\n\nElastic Net Regularization for Vector Generalized Linear Models: A Flexible Framework for High-Dimensional Biomedical Data by Wenqi Zhao\n\n\nAbstract\n\nWe introduce a novel implementation of elastic net regularization for vector generalized linear models (VGLMs), capable of fitting over 100 family functions and designed to support complex, high-dimensional modeling tasks commonly encountered in the biosciences. VGLMs extend classical GLMs by accommodating multivariate and multi-parameter responses, making them particularly well-suited for heterogeneous biomedical data. Our method integrates sparse estimation techniques—such as lasso, ridge, and their convex combinations—into this broader modeling framework, enhancing model interpretability and stability in high-dimensional settings. The algorithm is implemented in the vglmnet function within the new VGAMplus package for R. It leverages a modified iteratively reweighted least squares (IRLS) procedure, combined with pathwise coordinate descent, Karush-Kuhn-Tucker (KKT) condition checks, and strong rules for variable screening to ensure computational efficiency. This framework supports a wide range of models beyond the exponential family, including ordinal, categorical, and zero-inflated distributions commonly encountered in fields such as epidemiology, genomics, and clinical research. We illustrate the utility of our approach through comparisons with existing tools (glmnet, ordinalNet, mpath) and apply it to real-world datasets involving survival outcomes, count data, and bivariate binary responses. By uniting the structural flexibility of VGLMs with the benefits of regularization, our method provides a powerful and scalable solution for modern statistical modeling in the biosciences.\n\nFunctional Data Analysis for the Australian Grains Industry by Braden J. Thorne\n\n\nAbstract\n\nWhen performing statistical analysis on time series data with regular sampling, it is common to apply filtering or windowing methods to reduce the complexity of the task. While this process enables many classical approaches, it inevitably leads to compression of the full information available. An alternative approach is to treat observations as samples of a continuous mathematical function and focus analysis on the curves these functions produce rather than the samples. This is the underlying idea of functional data analysis, a statistical analysis approach that has seen growing attention in recent years. In this talk I will offer an introduction to functional data analysis and detail our exploration of these methods for application to the grains industry across Australia. Specifically, I will present two case studies; estimating charcoal rot prevalence in sixteen years of data from in-paddock soybean experimental trials using weather data, and analysing frost risk in broadacre crops with varying stubble management practices using sensor data.\n\n\n\n\nFitting Generalised Linear Mixed Models using Sequential Quadratic Programming by Peter Green\n\n\nAbstract\n\nFinding maximum likelihood estimates in generalised linear mixed models (GLMMs) can be difficult due to the often-intractable integral over the random effects. Ensuring convergence can be tricky, especially in binomial GLMMs, and often multiple optimisers and settings need to be tried to get satisfactory results. This is exacerbated if you want to use parametric bootstrap for your inference. Sequential quadratic programming (SQP) is a method for solving optimisation problems with non-linear constraints. SQP offers an alternative option for maximising the Laplace approximation to the GLMM likelihood, bypassing the need for an inner penalised iterative reweighted least squares (PIRLS) loop. This talk discusses the implementation of SQP for GLMMs and compares its performance to other common approaches.\n\nBayesian Ordinal Regression for Crop Development and Disease Assessment by Zhanglong Cao\n\n\nAbstract\n\nAccurate assessment of crop development and disease severity is essential for informed agronomic decision-making. This study presents a Bayesian framework for analysing ordinal data from field trials, including growth scale progression and disease ranking scores. Using the brms package in R, we applied cumulative logit models to evaluate the effects of sowing depth and treatment combinations on cereal growth stages, measured via the Zadok’s scale, across two Western Australian sites (Merredin and Wickepin). The same framework is being extended to model disease severity scores, demonstrating its versatility across categorical biological measurements. Our workflow incorporates rigorous model testing and evaluation, including posterior predictive checks and leave-one-out cross-validation (LOO-CV), to ensure robust inference and model fit. Rather than relying on p-values from linear mixed models, the Bayesian approach provides interpretable probabilities of achieving specific growth stages or disease severity levels. This shift enables more nuanced understanding of treatment effects and supports decision-making under uncertainty.\n\n\n\n10:50\nMorning Tea\n\n\n\n11:20\nChair: Robert Clark\nChair: Linh Nghiem\n\n\n\nOptimal allocation of resources between control and surveillance for complex eradication scenarios by Mahdi Parsa\n\n\nAbstract\n\nEffective eradication of invasive species over large areas requires strategic allocation of resources between control measures and surveillance activities. This study presents an analytical Bayesian framework that integrates stochastic modelling and explicit measures of uncertainty to guide decisions in complex eradication scenarios. By applying Shannon entropy to quantify uncertainty and incorporating the expected value of perfect information (EVPI), the framework identifies conditions under which investment into control or surveillance becomes worthwhile. Findings show that strategies which hedge against uncertainty can markedly improve the robustness of eradication outcomes with only marginal increases in expected costs. This approach offers practical tools for designing more cost-effective and reliable eradication programs and for prioritising data collection to reduce uncertainty where it has the greatest impact.\n\nEstimating extinction time from the fossil record using regression inversion by David I. Warton\n\n\nAbstract\n\nAn important problem in palaeoecology is estimating the extinction or invasion time of a species from the fossil record - whether because this is of interest in and of itself, or in order to understand the causes of extinctions and invasions, for which we need to know when they actually happened. There are two main sources of error to contend with - sampling error (because the last time you see a species need not be the last time it was there) and measurement error (dating specimens, usually well known). The paleobiology literature typically ignores one or other of these sources of error, leading to bias and underestimation of uncertainty to an extent that is often qualitatively important. The problem is surprisingly difficult to address statistically, because while standard regularity conditions are technically satisfied, we are typically close to a boundary where they break down, and hence standard asymptotic approaches to inference typically perform poorly in practice. We propose using a novel method, which we call regression inversion, for exact inference, and we apply this technique to a compound uniform-truncated t (CUTT) model for fossil data. We show via simulation that this approach leads to unbiased estimators, and accurate interval inference, in contrast to its competitors. We show how to check the CUTT assumption visually, and provide software to apply all of the above in the reginv package.\n\n\n\n\nInferring the rate of undetected contamination using random effects modelling of biosecurity screening histories by Sumonkanti Das and Robert Clark\n\n\nAbstract\n\nGroup testing plays a vital role in biosecurity operations worldwide, particularly in minimising the risk of introducing exotic pests, contaminants, and pathogens through imported agricultural products. A common screening strategy involves pooling items from consignments and testing each group for contamination presence, with consignments typically rejected if any group tests positive. Although screening designs often target a high probability of detection assuming a fixed minimum prevalence, analysing the historical results of these tests to infer the extent of contamination in non-rejected consignments (referred to as leakage) is less common. This study advances censored beta-binomial (BB) models to address contamination risk in frozen seafood imports into Australia, incorporating imperfect tests. Motivated by the characteristics of our case study, we develop a new class of BB models that impose a minimum positive consignment propensity threshold, capturing scenarios where contamination is either absent or exceeds a known minimum level. To fit these models, we propose a Metropolis-Hastings (MH) algorithm conditioned on prior distributions for sensitivity and specificity, allowing efficient estimation of quantities related to contamination levels. We analyse historical testing data under multiple scenarios using the proposed MH algorithm, yielding novel insights into both contamination risk and leakage. Finally, we use model-based simulations to communicate risk levels, providing key insights into potential undetected contamination.\n\nThe performance of Yu and Hoff's confidence intervals for treatment means in a one-way layout by Paul Kabaila\n\n\nAbstract\n\nConsider a one-way layout and suppose that we have uncertain prior information that the treatment population means are equal or close to equal. Yu & Hoff (2018) extended the \"tail method\" for finding a confidence interval for a scalar parameter of interest that has (a) specified coverage probability and (b) relatively small expected length when this parameter takes values in some given set. They used this extension to find confidence intervals for these treatment means that have (a) specified coverage probability individually and (b) relatively small expected lengths when this uncertain prior information happens to be correct. They assessed the expected lengths of these confidence intervals, over the whole parameter space, using a semi-Bayesian analysis. I describe a revealing alternative assessment of these expected lengths using a fully frequentist analysis. Yu, C. & Hoff, P. (2018) Adaptive multigroup confidence intervals with coverage. *Biometrika*, 105, 319-335.\n\n\n\n\nOptimal sampling in border biosecurity: Application to skip-lot sampling by Raphael Trouve\n\n\nAbstract\n\nBorder biosecurity faces mounting pressure from increasing global trade, requiring cost-effective inspection strategies to reduce the risk of importing pest and diseases. Current international standards recommend inspecting all incoming consignments (full census) with fixed sample sizes (e.g., 600 units) for high-risk pathways, but this may be overkill for lower-risk pathways with established compliance records. When should agencies use skip-lot sampling (SLS, sometimes called continuous sampling plan), which adaptively reduces inspections based on recent compliance history, over full census inspection? We developed a propagule pressure equation for SLS in overdispersed pathways and used Lagrange multipliers to derive a solution. Results show the choice depends on pathway overdispersion, sampling costs, and budget constraints. Optimal sample sizes are typically smaller than current recommendations, with better returns from inspecting a larger proportion of consignments rather than larger samples per consignment. This framework provides biosecurity agencies with data-driven guidance for implementing adaptive sampling strategies.\n\nRate-optimal sparse gamma scale mixture detection by Michael Stewart\n\n\nAbstract\n\nWe consider a model where observations from a known gamma distribution are possibly contaminated by observations from another gamma distribution with the same shape but a different mean. Such a model has been considered for times between neurotransmitter releases based on a Markov chain with amalgamated indistinguishable states. We focus on the case where the contaminating component occurs rarely, the so-called sparse gamma scale mixture detection problem. Due to the irregularity of such models theoretical results concerning detectability bounds are non-standard. Nonetheless in recent years a body of theory has been developed which covers the case when the mean of the unknown contaminating component is smaller than the null mean, but not when it is larger. We present some recent results filling this gap in the literature. In particular we describe a test which attains the optimal rate of convergence in various local alternative scenarios which is a Bonferroni-type test combining three different tests.\n\n\n\n\n\nExtension of the corrected score estimator in a Poisson regression model with a measurement error by Kentarou Wada\n\n\nAbstract\n\nKukush et al. (2004) discussed the bias of the naive estimator for the regression parameters in a Poisson regression model with a measurement error for the case where the explanatory variable and measurement error follow normal distributions. Wada and Kurosawa (2023) proposed the corrected naive (CN) estimator as a consistent estimator for a Poisson regression model with a measurement error for the case where the explanatory variable and measurement error are general distributions. The CN estimator directly calibrates the bias of the naive estimator. The CN estimator is given by the solution of the estimation equation of the Poisson regression model under the error-in-variables framework. However, the CN estimator does not always have an explicit expression under the condition that the explanatory variable and measurement error follow general distributions. On the other hand, Kukush et al. (2004) considered the corrected score (CS) estimator as a consistent estimator for the true parameter of the Poisson regression model with a measurement error. In this research, we extend the CS estimator to the case where the explanatory variable and measurement error are general distributions. The new estimator can be applied for the condition that the CN estimator does not have an explicit expression. As illustrative examples, we give simulation studies to verify the effectiveness of the new estimator.\n\n\n\n12:50\nLunch\n\n\n\n13:00\nSocial Activities"
  },
  {
    "objectID": "book/sessions.html#thursday",
    "href": "book/sessions.html#thursday",
    "title": "Schedule",
    "section": "Thursday",
    "text": "Thursday\n\n\n\n\n\n\n\n\n\n\nStart\nArc Cinema\nTheatrette\n\n\n\n\n08:30\nConference Registration\n\n\n\n08:50\nHousekeeping\n\n\n\n08:55\nChair: Warren Muller\n\n\n\n\nModularizing Biometric Models Facilitates Multistage Computing by Mevin B. Hooten\n\n\nAbstract\n\nBayesian modeling has become invaluable in biometrics. It allows us to formally consider unobserved processes while accommodating uncertainty about data collection and our understanding of biological and ecological mechanisms. Several excellent software packages are available for fitting Bayesian models to data and are being applied every day to analyze biometric data. These methods allow us to answer questions using data in ways that has never before been possible. The adoption of Bayesian methods has led to bigger models necessary to answer tough questions using large and varied data sets. Bigger models and data sets lead to computing bottlenecks. Fortunately, a solution to Bayesian computing roadblocks sits in plain sight. The structure of Bayesian models allows us to rearrange them so that we can perform computing in stages. We can break big models into pieces, fit them separately, and then recombine them in later computing stages. Recursive Bayesian approaches can save us time by leveraging the parallel architecture of modern computers. A modular perspective allows us to see Bayesian models in a way that facilitates multistage computing. I will demonstrate the procedure with a set of biometric examples. These include geostatistical models in marine science, capture-recapture models for abundance estimation, and spatial point process models for species distributions.\n\n\n\n\n09:50\nChair: Vanessa Cave\nChair: Alice Richardson\n\n\n\nVisualize your fitted non-linear dimension reduction model in the high-dimensional data space by P. G. Jayani Lakshika\n\n\nAbstract\n\nNon-linear dimension reduction (NLDR) techniques such as t-SNE, UMAP, PHATE, PaCMAP and TriMAP provide a low-dimensional representation of high-dimensional data by applying a non-linear transformation. The methods and parameter choices can create wildly different representations, so much so that it is difficult to decide which is best, or whether any or all are accurate or misleading. NLDR often exaggerates random patterns, sometimes due to the samples observed, but NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of high-dimensional distributions. To help evaluate the NLDR, we have developed a way to take the fitted model, as represented by the positions of points in 2D, and turn it into a high-dimensional wireframe to overlay on the data, viewing it with a tour. Viewing a model in the data space is an ideal way to examine the fit. One can see whether it fits the points everywhere or fits better in some places, or simply mismatches the pattern. It is used here to help with the difficult decision on which 2D layout is the best representation of the high-dimensional distribution, or whether the 2D layout is displaying mostly random structure. It can also help to see how different layouts made by different methods are effectively the same summary, or how the different methods have some particular quirks. This methodology is available in the R package `quollr`. We will demonstrate the technique using single-cell data, particularly to understand cluster structure.\n\nReporting Odds Ratios under Fluctuating Reporting Rates in Spontaneous Reporting Systems by Tatsuhiko Anzai\n\n\nAbstract\n\nSpontaneous adverse event reporting systems, including the Japanese Adverse Drug Event Report (JADER) database, the FDA Adverse Event Reporting System (FAERS), and VigiBase, play a critical role in post-marketing drug safety surveillance. The reporting odds ratio (ROR) is a commonly used measure for detecting suspected adverse drug reactions as \"signals\", based on disproportionality in reporting between a specific drug and all others, as summarized in a two-by-two contingency table. However, during events such as the COVID-19 pandemic, reporting rates of adverse drug reactions can fluctuate, potentially introducing bias into ROR. This study proposes a method for deriving the ROR that incorporates four parameters, each corresponding to fluctuations in reporting ratios for the cells in the two-by-two contingency table. These parameters can be estimated using the divergence between observed and predicted values, where the predicted values are derived from a regression model under the assumption that the reporting rates for the drug and the adverse reaction remain stable. We evaluate the properties and performance of the proposed method through both real-world data analysis and simulation studies, demonstrating its effectiveness for signal detection.\n\n\n\n\nThe geometry of diet: using projections to quantify the similarity between sets of dietary patterns by Beatrix Jones\n\n\nAbstract\n\nFood consumption is complex and high dimensional. Nutrition researchers measure (or attempt to measure) consumption using food frequency questionnaires, food recalls, or food records. This generates high dimensional data which is frequently summarized using principle components, principle components with rotation, or factor analysis. The resulting linear combinations are called “dietary patterns.” In this talk we consider quantifying how similar two different sets of dietary patterns are, assuming the same set of underlying variables has been collected. We explore using a multivariate extension of Tucker’s congruence coefficient for this purpose. Tucker’s congruence coefficient can be thought of as the length of the projection of one factor direction onto another, or equivalently the (absolute) cosine of the angle between them; thus it ranges from 0 to 1. In (eg) two dimensions, we consider the square root of the area of a unit square projected from one space onto another; this can be generalized for any number of dimensions. The measure is symmetric and invariant to rotation. To contextualize scores on this measure, we compute this measure of agreement for several datasets from the dietary pattern validation literature, where the same food questionnaire is given to the same people a few weeks apart. We also consider the effect of truncating small loadings, as is common when describing dietary patterns.\n\nHandling Missingness in Prevalence Estimates from National Surveys by Oyelola Adegboye\n\n\nAbstract\n\nPopulation-based surveys, such as the Demographic and Health Surveys (DHS), are pivotal for estimating the prevalence of important diseases, particularly in low-resource settings. However, missing data, such as person non-response, particularly refusals, poses a serious challenge, potentially introducing substantial bias in prevalence estimates. Using HIV data sets from Malawi's 2004 DHS, Antenatal Clinics surveillance, and the Malawi Diffusion and Ideational Change Project (MDICP), this paper evaluates existing estimators and proposes novel approaches to adjust for refusal bias. These include complete case analysis, mean score imputation, inverse propensity score weighting, bounding techniques using longitudinal or sentinel data and Manski’s partial identification approach. The study distinguishes between refusals and non-contacts and examines how prior knowledge of HIV status influences participation. Estimates of HIV prevalence varied notably across methods, with refusal-adjusted approaches generally yielding higher prevalence rates than complete case estimates. The divergence is more pronounced among women, largely due to higher refusal rates among them. Bounding methods provide credible intervals for prevalence under weak assumptions. In the case of HIV estimates, refusal bias, particularly when linked to prior testing knowledge, can significantly distort HIV estimates. Integrating multiple data sources and using methodologically transparent adjustment techniques are critical for robust HIV surveillance in low-resource settings.\n\n\n\n\nMultivariate meta-analysis methods for high-dimensional data by Alysha M. De Livera\n\n\nAbstract\n\nMultivariate meta-analysis methods for high-dimensional data Meta-analysis is a statistical method that combines quantitative results from multiple independent studies on a particular research question or hypothesis, with the goal of making inference about the population effect size of interest. Traditional meta-analysis methods have focused on combining results from multiple independent studies, each of which has measured an effect size associated with a single outcome of interest. Modern studies in evidence synthesis, such as those in biological studies have focused on combining results from studies which have measured multiple effect sizes associated with multiple correlated outcomes. We will present a novel, multivariate meta-analysis method for obtaining summary estimates of the effect sizes of interest for high-dimensional data, with applications to real and simulated high-dimensional data. We will discuss advantages, disadvantages and the statistical challenges, and present a CRAN-based R package for implementation of the methods.\n\nPooled testing with penalized regression models by Christopher Bilder\n\n\nAbstract\n\nPooled testing (also known as group testing) is a widely used procedure to test individuals for infectious diseases. Rather than testing each specimen separately, multiple specimens are pooled together and tested as one. The pool test outcome, along with further tests as needed, are used to determine which individuals are positive or negative for an infectious pathogen. The COVID-19 pandemic especially highlighted the importance of pooled testing, with laboratories adopting it worldwide to increase their testing capacity. Pooled testing traditionally relies on observing the positive/negative test outcomes alone. However, during the pandemic, new pooled testing algorithms were developed that utilize the viral load information from a pool test. These new algorithms use penalized regression models to predict the viral load of each individual, which lead to individual positive/negative predictions. The purpose of our presentation is to provide a comparison of these new algorithms relative to standard ones. Both gains and losses are quantified by applying algorithms under fair comparison settings.\n\n\n\n10:50\nMorning Tea\n\n\n\n11:20\nChair: Matthew Schofield\nChair: Chris Triggs\n\n\n\nIntegrated Species Distribution Models: A Single-Index Approach by Quan Vu\n\n\nAbstract\n\nIn ecology and fisheries, species abundance data are often collected using a number of surveys that have different characteristics. Spatial statistical models can be used to integrate information from these datasets to improve both interpretability and prediction of the abundance. In this talk, we introduce the single-index integrated species distribution model, based on a single index representing the latent spatial distribution of the species abundance, and survey-specific link functions representing the different catchability properties of each survey. We demonstrate the use of the model through an analysis of scallop data collected from two surveys: a bottom trawl survey which covers a wide spatial domain but is less efficient, and a hydraulic dredge survey which is more efficient and spatially targeted. Results show that our model offers meaningful interpretations of covariate effects, spatial distribution, and survey catchability differences, and achieves superior predictive performance compared to contemporary species distribution models.\n\nCrossvalidation for predictive models in complex survey data by Thomas Lumley\n\n\nAbstract\n\nCross-validation is a standard technique for choosing models with good out-of-sample prediction error in statistical learning. Chopping data up at random makes sense for independent observations, but not for observations from a multistage survey. For example, when a single cluster is split between test and training sets there is the potential for data leakage and underestimation of prediction error. I will describe an approach to cross-validation for complex survey data using replicate weights and describe its implementation in the R survey package.\n\n\n\n\nSimultaneous Inference for Latent Variable Predictions in Factor Analytic Models by Zhining Wang\n\n\nAbstract\n\nFactor analytic models (also known as latent variable models) are fundamental tools in multivariate statistics, widely applied in fields such as psychology, economics, and social sciences. While considerable attention has been given to estimation and inference on parameters such as the loading matrix and the error variances, relatively less research has been done on how to perform inference on the predicted factors, e.g., how to construct prediction intervals for the latent variables jointly across all the clusters in a given dataset. We explore a framework for the simultaneous inference of the predicted latent variables in factor analytic models. We demonstrate the construction of simultaneous prediction intervals for the predicted factors, examining strategies such as the bootstrap and Monte Carlo simulation, and show how this also facilitates joint/multiple testing across different cluster-level predictions. We examine the practical feasibility and robustness of the proposed simultaneous inference methods with simulation studies and an application in the biosciences.\n\nA Set of Precise Asymptotics for Gaussian Variational Approximation in Generalised Linear Mixed Models by Nelson J. Y. Chua\n\n\nAbstract\n\nGeneralised linear mixed models (GLMMs) are used to model data with a clustered or hierarchical structure, capturing intra-cluster dependence through the use of (latent) random effects. However, this dependence structure results in a likelihood function that is typically computationally intensive to evaluate, and so approximate likelihood approaches are often used instead for model fitting and inference. One such approach which has grown in popularity over the past decade is Gaussian variational approximation (GVA). In addition to estimates of the model parameters, GVA concurrently produces random effects predictions and associated uncertainty measures. In this talk, we formulate a set of precise asymptotic properties for both the parameter estimates and random effects predictions obtained from GVA, focusing on independent cluster GLMMs. We find that these properties change substantially depending on whether or not the random effects are conditioned on. By comparing GVA's asymptotic properties and empirical finite-sample performance with that of other commonly used approximate likelihood approaches, we highlight situations in which the use of GVA would be advantageous in practice.\n\n\n\n\nModel-based assessment of functional and phylogenetic diversity by Shaoqian Huang\n\n\nAbstract\n\n**Topic** Biodiversity is a quantity of fundamental interest, but measuring it remains challenging. Two key aspects of diversity are functional diversity (FD)—the extent to which species in a community differ in their ecological functions—and phylogenetic diversity (PD)—the extent to which species differ in their evolutionary histories. **Limitations** Commonly used distance-based diversity measures, such as Rao’s Q and Faith's PD, are typically affected by sampling intensity, which refers to the level of effort invested in collecting samples. Specifically, these measures confound true diversity changes with changes in species richness driven by different sampling intensities. In addition, there is no established statistical framework for analyzing how diversity changes along environmental gradients. **Our Model** We propose a model-based assessment of PD and FD changes along gradients. It is robust to variations in sampling intensity, as the model explicitly captures the main trend in abundance change—namely, changes in species richness—and then isolates the residual variation, the $\\beta$-diversity change. We then measure PD or FD change using linear contrasts of the $\\beta$-diversity parameters, where the contrasts correspond to the main axes of the phylogenetic or functional similarity matrix. **Simulation Analysis** Based on the presence/absence pattern, we conducted simulations in which we designed phylogenetic distance matrices with different numbers of species, modelled on real data. The results show that our approach tends to outperform methods currently used in ecology.\n\nAn allometric differential equation model quantifies energy trade-offs between growth and reproduction under temperature variation by Hideyasu Shimadzu\n\n\nAbstract\n\nIndividual variation in growth and reproduction is common even within a single species. Such differences play a crucial role in shaping life-history strategies, particularly in response to environmental factors such as temperature. This talk introduces a novel system of allometric differential equations that models internal energy allocation between two fundamental biological processes, growth and reproduction, under different thermal regimes. Climate change is not only driving increases in mean temperatures but also intensifying temperature variability, resulting in greater environmental unpredictability. While temperature is known to regulate resource allocation in ectotherms, the consequences of stochastic thermal fluctuations for life-history traits remain poorly understood. Using *Daphnia magna* as a model organism, we present a novel allometric growth model that characterises energy allocation dynamics in fluctuating thermal environments. Our framework quantifies the effects of randomly varying temperatures on lifetime patterns of growth and reproduction. Our model reveals that exposure to unpredictable thermal regimes can elicit life-history responses similar to those observed under persistently elevated temperatures. Importantly, our energy-based approach identifies patterns of reproductive investment that are not discernible from growth data alone, highlighting the pivotal role of temperature variability in shaping life-history trajectories. As climate change increasingly entails unpredictable environmental conditions, the model and findings presented here offer valuable tools for anticipating and managing biological responses to future climate scenarios.\n\n\n\n\nFitting integrated species distribution models using mgcv by Elliot Dovers\n\n\nAbstract\n\nIntegrated species distribution models (ISDMs) are a useful tool for ecologists, allowing them to use multiple sources of data to infer the distribution of species across geographic regions. Recent studies have shown that including latent spatial terms to account for dependence structures within and between datasets is often crucial to the performance these models. However, the inclusion of these latent terms can make the ISDMs technically challenging to fit, and often require users to learn/adopt bespoke software. We describe how ISDMs can be fitted using `mgcv` on `R` by using the grouped family feature (`gfam`). This permits multiple likelihoods - and hence multiple data types - to be fitted within the same model by using each dataset to inform the estimation of common parameters. We additionally show how smoothers over the geographic coordinates that approximate Gaussian random fields can be used to incorporate the necessary latent spatial effects. We use presence/absence and presence-only data to demonstrate that ISDMs fitted via `mgcv` have comparable performance to other available software - through both simulations and in application, where we predict the occurrence of a tree species in NSW, Australia.\n\nA circular hidden Markov model for directional time series data by A.A.P.N.M. Perera\n\n\nAbstract\n\nModeling directional time series data such as wind or ocean current direction presents several interesting challenges. Standard linear time series techniques do not account for the circularity of the observations, while existing circular modeling approaches typically work best when the data span a small arc. Motivated by a series of fire burn experiments collecting wind direction in southeastern Australia, we propose a new method for directional time series data that combines the flexibility of hidden Markov models for capturing different latent states at different periods of time, with a conditional von Mises distribution given the latent state to explicitly account for the circular nature of the responses. The resulting circular hidden Markov model (cHMM) can allow for multimodality and/or varying amounts of circular dispersion over time. Furthermore, by utilizing a von Mises distribution whose mean direction depends on previous observations, we can accommodate serial correlations within a specific hidden state. We employ direct maximum likelihood estimation to fit the cHMM, and examine three approaches to perform forecasting based on extrapolating the latent state sequence and then direction observations conditional on this sequence. An application to the motivating wind direction datasets reveals that cHMMs produce similar or better point/probabilistic forecasting performance compared with several established time series methods.\n\n\n\n12:40\nLunch\n\n\n\n13:00\nAGM\n\n\n\n13:40\nChair: Emi Tanaka\nChair: Sam Mason\n\n\n\nEvaluating the impact of trait measurement error on genetic analysis of computer vision-based phenotypes by Gota Morota\n\n\nAbstract\n\nQuantitative genetic analysis of image- or video-derived phenotypes is increasingly being performed for a wide range of traits. Pig body weight values estimated by a conventional approach or a computer vision system can be considered as two different measurements of the same trait, but with different sources of phenotyping error. Previous studies have shown that trait measurement error, defined as the difference between manually collected phenotypes and image-derived phenotypes, can be influenced by genetics, suggesting that the error is systematic rather than random and is more likely to lead to misleading quantitative genetic analysis results. Therefore, we investigated the effect of trait measurement error on genetic analysis of pig body weight (BW). Calibrated scale-based and image-based BW showed high coefficients of determination and goodness of fit. Genomic heritability estimates for scale-based and image-based BW were mostly identical across growth periods. Genomic heritability estimates for trait measurement error were consistently negligible, regardless of the choice of computer vision algorithm. In addition, genome-wide association analysis revealed no overlap between the top markers identified for scale-based BW and those associated with trait measurement error. Overall, the deep learning-based regressions outperformed the adaptive thresholding segmentation methods. This study showed that manually measured scale-based and image-based BW phenotypes yielded the same quantitative genetic results. We found no evidence that BW trait measurement error could be influenced, at least in part, by genetic factors. This suggests that trait measurement error in pig BW does not contain systematic errors that could bias downstream genetic analysis.\n\nDo Mice Matter? The Impact of Mice on a New Zealand Ecosanctuary by Vanessa Cave\n\n\nAbstract\n\nMammal-resistant fences have enabled the successful eradication of exotic mammals from ecosanctuaries in New Zealand. However, preventing the re-invasion of mice remains a challenge. Indeed, mice are still present in many fenced ecosanctuaries and can reach high population densities. Scientists at Manaaki Whenua – Landcare Research have been studying the impact of mice on biodiversity at Sanctuary Mountain Maungatautari. Two independently fenced sites within the sanctuary were managed to create contrasting mouse populations: one with high mouse numbers, and the other with undetectable levels. After two years, management protocols were reversed, with mice eradicated from the first site and allowed to increase at the second. Data on the abundance of invertebrates, seedlings, and fungi were collected throughout the duration of the study. Temporal trends in abundance were analysed and compared using linear mixed models with smoothing splines. The findings suggest that mice can have severe impacts on native biodiversity, particularly invertebrates, posing a significant threat to ecological recovery efforts in fenced ecosanctuaries.\n\n\n\n\nPredication of Daily Weight Gain with Cattle Behaviour and Daily Activity Using Triaxial Accelerometer Data by Shuwen Hu and Antonio Hu\n\n\nAbstract\n\nLive body weight gain is an important measurement of animal performance. In this study, we predict the cattle's daily weight gain from five core cattle behaviours and a measure of total daily activity based on accelerometer data. To collect data, we conducted an experiment equipping a herd of 60 Brahman steers with research collars containing triaxial accelerometers over nearly one month in Australia. We used the accelerometer data, which represents the intensity of animal movement, to compute an activity metric within a five-minute window. In addition, we use pre-trained accelerometer-based machine learning models to classify cattle behaviour into grazing, ruminating, resting, walking, drinking or other classes over five-second time windows. Daily behaviour profiles were constructed for each animal and experiment day by aggregating the behaviour predictions over every calendar day. Our objective was to explore how to use behaviours and activity metrics to predict the cattle's daily weight gain. The daily activity values ranging from 5.44g to 23.69g. The average daily time spent grazing, ruminating, resting, walking and drinking was 8.97±1.12, 7.78 ±1.03, 5.83±1.05, 1.00±0.4, and 0.18±0.12 hours, respectively. Some weather information data are combined in the model to predict the cattle live-weight gain. The best R-squared value is 0.467, with a minimum root mean square error (RMSE) of 0.867 from the linear regression model. The live-weight gain could not be fully explained by measurements taken in this study, but we showed how these factors can influence the variability in cattle performance.\n\nModelling Species Diversity from Citizen-Science Bird Counts by Graham Hepworth\n\n\nAbstract\n\nBird count data from the Greater Melbourne region over a 25-year period were recorded by volunteer observers, based on 20-minute surveys. Counts for a large range of bird species were reported either as counts or simply as “present”. Our investigation focused on whether there has been an increase in the presence and abundance of noisy miners, an endemic species known for its aggressive and territorial behaviour, and whether this could partly explain apparent declines in other species. The dataset was augmented with weather, geospatial and ecological information, and a mixed-effects modelling framework was adopted. Seasonal effects and long-term trends were accounted for via regression splines. The influence of different observers and locations were represented as random effects. GAMMs were fitted for (a) the presence/absence of noisy miners, (b) the abundance of noisy miners, and (c) species richness for particular groupings of birds. In modelling species richness, noisy miner abundance was included as a predictor; we used multiple imputation to account for the missing data when this species was reported as “present” but no count was recorded. There was a small, positive association found between noisy miner abundance and all-species diversity – a surprising result. A stronger negative association was found for some species groupings. For many species groups, diversity tended to decrease with increasing distance from the nearest native vegetation feature, and with increasing wind speed. The work provided several statistical and computational challenges.\n\n\n\n\nFishing for Heritability in the Gill Microbiome: Why Statisticians Should get out into the field by Elle Saber\n\n\nAbstract\n\nHost‐associated microbiomes are increasingly recognised as integral to health, yet the extent to which host genetics shapes these communities remains unclear. While heritable components of gut and skin microbiomes have been documented in several vertebrates, evidence for the gill microbiome of fish is scarce. This exploratory study sampled the gill microbiome of Atlantic salmon within a Tasmanian breeding program to investigate potential genetic influences. Despite careful planning, study participants did not always behave, and the practical constraints of a commercial operation prevented complete data capture. Having participated in the data collection I was better prepared to understand the limitations of the dataset when doing the downstream analysis. The take home message is that, even if we feel like a fish out of water, time spent among the data can be just as valuable as time spent our desks.\n\nContinental-Scale Bayesian Analysis of Acacia Flowering Phenology: A Novel Framework Integrating Phylogenetic Signal and Circular Statistics by Owen Forbes\n\n\nAbstract\n\nClimate change is driving widespread shifts in plant phenology globally, with critical implications for ecosystem functioning and species interactions. While most phenological studies focus on temperate deciduous species at local scales, Australia's diverse Acacia genus offers unique opportunities to understand phenological responses to variations in rainfall and temperature across continental gradients. Leveraging digitised herbarium collections, we present the largest phylogenetically-informed analysis of acacia flowering phenology undertaken to date, spanning 548 species across Australia's full climatic spectrum. We developed a novel phenological modelling approach using a Bayesian hierarchical framework that makes several key innovations. First, we extend traditional phylogenetic least squares (PGLS) approaches by incorporating phylogenetic signal as a structured random effect within a fully Bayesian context, enabling simultaneous estimation of phylogenetic covariance alongside spatial and temporal dependencies. Second, we employ von Mises circular distributions to model flowering time, naturally accommodating seasonal cyclicity while avoiding linear boundary artifacts. This modular approach also integrates spatial autocorrelation structure, time-lagged climate covariates, and phylogenetic relationships within a unified probabilistic framework. Our analysis of 20,998 herbarium specimens spans 110 years (1910 – 2020) across Australia, using climate observations from the Australian Gridded Climate Data, with Köppen climate zones included in the model to stratify climate-phenology relationships. This framework successfully quantifies species-specific climate sensitivities while accounting for phylogenetic constraints. Expected effect of time-lagged temperature and rainfall effects vary significantly across Köppen climate zones and species. Species vulnerability patterns could help identify taxa which could be potentially at higher risk under projected climate scenarios. This modular Bayesian approach demonstrates the research potential of natural history collections for continental-scale ecological inference, providing a statistical template for global phenological studies addressing critical climate change impacts.\n\n\n\n\n\nSpatio-Temporal Species Distribution Modelling by Sam Mason\n\n\nAbstract\n\nDetecting species response to climate change is a critical concern in ecology requiring relevant climatic predictors over long temporal windows and large spatial extents. To date this has been technologically challenging resulting in the widespread use of 30 year averaged datasets which, while readily accessible, actually mask species responses by effectively treating the climate as temporally static. Recent advances in data acquisition technology have made it easier to obtain a wide range of environmental predictors at appropriate spatio-temporal scales and at consistent resolutions needed for effective research. By using dynamic predictors and anomalies from their mean values in spatio-temporal species distribution models we see if we can detect species response to climate change and if this yields better predictive performance than current practice. Interestingly, we have not found appreciable spatio-temporal signal, in contrast to expectations and other work which used naïve approaches to analysis.\n\n\n\n15:10\nAfternoon Tea\n\n\n\n15:40\nChair: Julian Taylor\nChair: Oyelola Adegboye\n\n\n\nThe *equalto* covariance structure for meta-analysis using the `glmmTMB` R package by Coralie C. Williams\n\n\nAbstract\n\nMeta-analysis is a widely used statistical method for synthesising quantitative results from related studies, enabling researchers to address broad questions and explore sources of heterogeneity. In R statistical software, several packages support meta-analytic modelling, including the actively maintained `metafor` package which is commonly used across all disciplines. But meta-analysis typically uses a mixed model, with the variance structure of study-level random effects known, so it should be possible to fit such models using standard mixed modelling software. We propose using the `glmmTMB` package for this purpose. In the last decade, `glmmTMB` has developed into a flexible package to fit generalised linear mixed models (GLMMs) via the Template Model Builder (TMB) framework. The package holds a similar interface as the `lme4` package but supports a broader range of distributions and covariance structures using Laplace approximation. Here, we introduce a new covariance structure, called “*equalto*”, to the `glmmTMB` package, so that it can be used to fit meta-analytic models by allowing users to specify a known variance-covariance matrix for the sampling error. This enables explicit modelling of heteroscedasticity or dependence among effect sizes. The new implementation offers an alternative way to do a meta-analysis, convenient for users already familiar with fitting mixed models in `lme4` or `glmmTMB` packages, and capable of handling complex model structures. This novel implementation supports more flexible modelling of meta-analytical data, expanding the R toolkit available for evidence synthesis. We showcase its applicability with illustrative examples in ecology and evolution.\n\nExtending diagnostic validity meta-analysis to several diagnostic guidelines by Alain C. Vandal\n\n\nAbstract\n\nOptimal management of acute biliary disease should include an assessment for possible choledocholithiasis (stones in the bile duct). Various diagnostic guidelines have been developed by expert bodies for this purpose, but uncertainties remain about their performance in wider practice. We carried out a meta-analysis to compare three of these guidelines. Each guideline yields a score, for which different thresholds are considered in terms of sensitivity and specificity. Of 1,892 records, thirty-one studies were identified for this meta-analysis. All studies focused on one or more of three international guidelines, namely the ASGE guidelines from 2010, the revised ASGE guidelines from 2019 and the ESGE guidelines from 2019. Each study reported on the diagnostic validity of one or more of these guidelines at one or more score cutpoint. The analysis was carried out using the _diagmeta_ R package [Rücker G, Steinhauser S, Kolampally S, Schwarzer G (2022)]. Their approach allows the meta-analytical estimation of the area under the receiver operating curve, a quantity that summarises the performance of a diagnostic tool. The approach consists in fitting transformed values of sensitivity and specificity at various cutpoints, which can differ between studies, accounting for study heterogeneity using linear mixed effects modelling. While the _diagmeta_ approach applies to the meta-analysis of a single diagnostic procedure, we show how its framework can be extended to jointly and severally test for performance similarity between guidelines. We also show how it can be extended to identify outlier studies, for the purpose of carrying out sensitivity analyses.\n\n\n\n\nSimulation-based Inference about Variance Components by Farwa Saleem\n\n\nAbstract\n\nVariance components in generalized linear mixed models (GLMMs) are used to quantify random effects, for example, they can be used in ecological studies to quantify the size of the effect of an environmental variable on species composition. However, constructing accurate confidence intervals for these components remains challenging with small samples or near-boundary estimates, where asymptotic methods (e.g., profile likelihood, Wald intervals) do not perform well. To address this, we propose a novel simulation-based framework using bootstrap test inversion coupled with quantile regression. Our approach efficiently estimates confidence limits by leveraging the continuous distribution of the estimator, eliminating discretization steps inherent in conventional bootstrap procedures. This enhances robustness in low-information scenarios while adapting to boundary constraints. Simulation studies demonstrate that our method achieves superior coverage accuracy compared to existing asymptotic and bootstrap alternatives under small-sample conditions and near-boundary parameters. We show the application of the technique to an ecological case study analysing response of species composition to environmental gradients. This work delivers a flexible tool for more defensible inference in applications demanding precise uncertainty quantification for random effects. Keywords: Variance components, GLMM, bootstrap, quantile regression, confidence intervals, ecological statistics.\n\nRisk of Guillain-Barré Syndrome after COVID-19 vaccination and SARS-CoV-2 infection: A multinational self-controlled case series study by Han Lu\n\n\nAbstract\n\nIntroduction: The Global Vaccine Data Network (GVDN) is a multinational collaboration established to conduct globally coordinated epidemiological studies on vaccine safety using health data. This study aimed to assess the risk of Guillain-Barré syndrome (GBS) within 42 days after exposure to any COVID-19 vaccines and SARS-CoV-2 infection, combing multisite data using a common GVDN protocol and data model. Methods: We used a self-controlled case series (SCCS) design and identified GBS cases via electronic data sources (EDS) from 20 GVDN sites globally. Fifteen sites performed medical chart review using Brighton Collaboration case definitions to determine the level of diagnostic certainty (LOC). The relative incidence (RI) between pre-defined risk and control windows was calculated using conditional Poisson regression, controlling for seasonality. De-identified case-level data were combined for individual patient data (IPD) analysis, and the estimates were aggregated with otherwise locally analysed site-level results to calculate the overall RI with 95% confidence interval (95%CI) using random effects meta-analysis. Results: We identified 2086 GBS cases (4329 vaccine doses) from EDS and 410 cases were chart-reviewed (12% with LOC 1-2). We observed an association between Vaxzevria/Covishield vaccine and GBS among LOC 1-2 cases within 42 days post-exposure (RI 3.10, 95%CI 1.12-8.62). No increased risk of GBS was observed with mRNA and other vaccines. Among 489 EDS -identified cases post-infection, significant risk was found between SARS-CoV-2 and GBS (RI 3.35, 95%CI 1.83-6.11). The SCCS method is an efficient approach to collect minimal data for IPD meta-analysis. Full results using different SCCS models will be discussed.\n\n\n\n\nA Proportional Random Effect Block Bootstrap for Highly Unbalanced Clustered Data by Zhi Yang Tho\n\n\nAbstract\n\nClustered data arise naturally in a wide range of scientific and applied research settings where units are grouped within cluster, and are commonly analyzed using linear mixed model to account for within-cluster correlations. This article focuses on the scenario in which cluster sizes might be highly unbalanced, and proposes a proportional random effect block bootstrap that is applicable in such case and is robust to misspecification of the stochastic assumptions of the linear mixed model. The proposed method is a generalization of the random effect block bootstrap, originally designed for the balanced case, and can be used to perform inferences on parameters of the linear mixed model or functions thereof. We establish asymptotic consistency of the proposed bootstrap under general cluster sizes scenario, showing that the original random effect block bootstrap is only consistent when cluster sizes are balanced. A modified random effect block bootstrap is also proposed which enjoys similar asymptotic consistency properties as the proportional random effect block bootstrap. Simulation study demonstrates the strong finite sample inferential performance of the proposed bootstraps, particularly compared with the random effect block bootstrap and several existing bootstrap methods for clustered data. We apply the proposed bootstraps to the Oman rainfall enhancement trial dataset with cluster sizes ranging from 1 to 58. Results show that the bootstrap confidence intervals based on our proposed bootstraps are more adequate than those of random effect block bootstrap and that the employed ionization technology has a statistically significant effect in increasing the amount of rainfall.\n\nChildhood Risk and Resilience Factors for Pasifika Youth Respiratory Health: Accounting for Attrition and Missingness by Siwei Zhai\n\n\nAbstract\n\nIn New Zealand, 7% of deaths are related to respiratory diseases, with Pacific people at higher risk. Our work investigates the causal effects of early-life risks and resilience factors on early-adulthood lung function amongst Pacific Islands Families Study (PIFS) cohort members (n=1,398). 466 from the cohort participated in the respiratory study. Primary outcome was forced expiratory volume in 1 second (FEV1) z-score at age 18 years. FEV1 and healthy lung function (HLF), defined as the z-score being larger than -1.64, were secondary outcomes. A previous study had evaluated the effects of early-life nutrition factors on the respiratory health of Pacific youth. The results suggested a positive impact of consuming more fruit and vegetables during childhood on respiratory health later in life. The follow-up study will continue to explore the effects of factors from relevant domains based on the PIFS cohort, where a new integrated model will be applied. A simulation will be conducted to determine this model.\n\n\n\n\n\nA missing data detective story – how I navigated through a perfect storm of drop-out’s, COVID and informative missingness. by Eve Slavich\n\n\nAbstract\n\nReal-world clinical studies often present missing data challenges that leave the textbook educated scratching their heads. Through consulting, I become involved in [a study](https://doi.org/10.1016/j.cont.2025.101762) where multiple correlated missing data mechanisms created analytical challenges. Following implementation of new birthing protocols aimed at reducing birth-related injuries, in a Sydney health district, a follow-up study was designed to evaluate incontinence outcomes. However, the missing data pattern proved to be informatively related to both treatment and outcome variables: women who experienced successful outcomes (no incontinence) were systematically less likely to attend follow-up appointments. The situation was further complicated when the COVID-19 pandemic struck mid-study, dramatically reducing attendance rates and introducing an additional layer of correlated missingness that varied by treatment timing and baseline characteristics. Working within the constraints of the study design and the budget for a statistician, I developed a model for missingness which I will outline in this talk. While missing data complications can severely challenge study validity, thoughtful statistical approaches can still yield meaningful insights when the caveats (caveats galore!) are properly acknowledged and addressed. References: Young, R et al, 2025, Outcomes for obstetric anal sphincter injuries and anal incontinence following introduction of a perineal bundle, Continence, Volume 14 Keywords: Missing data analysis, informative missingness, multiple imputation, statistical consulting, statistical practice\n\n\n\n18:30\nConference Dinner"
  },
  {
    "objectID": "book/sessions.html#friday",
    "href": "book/sessions.html#friday",
    "title": "Schedule",
    "section": "Friday",
    "text": "Friday\n\n\n\n\n\n\n\n\n\n\nStart\nArc Cinema\nTheatrette\n\n\n\n\n08:40\nConference Registration\n\n\n\n09:00\nHousekeeping\n\n\n\n09:05\nChair: Louise McMillan\n\n\n\n\nSaddlepoint approximations for likelihoods by Jesse Goodman\n\n\nAbstract\n\nClassically, the saddlepoint approximation has been used as a systematic method for converting a known generating function into an approximation for an unknown density function. More recently, it has been used instead as an approximation to the *likelihood function*. In this viewpoint, it is the underlying data-generating process whose generating function is used, and the saddlepoint approximation can be maximized to compute an approximate saddlepoint MLE for given observed data. This talk will explain how the saddlepoint approximation can be interpreted with a statistical lens, including common features for those otherwise intractable models where we can compute a generating function but not a likelihood. Many of these models come from statistical ecology, including sitations where we gather population-wide observations only. In addition, the talk will describe a class of models having simple theoretical guarantees for the effect of using the saddlepoint MLE as a substitute for the unknown true MLE, and will demonstrate new tools to visualize the saddlepoint approximation intuitively, to simplify and automate the computation of saddlepoint MLEs, and to quantitatively assess the amount of approximation error introduced by using an approximate likelihood as a substitute for an intractable true likelihood.\n\n\n\n\n10:00\nChair: Yidi Deng\nChair: David Baird\n\n\n\nFalse Discovery Rate Controlled Robust Variable Selection under Cellwise Contamination by Xiaoya Sun\n\n\nAbstract\n\nThe increasing complexity and dimensionality of modern datasets make cellwise outliers a frequent and expected phenomenon, complicating tasks such as variable selection. An mRNA-Seq expression profiling data set of human brain tissue for Huntington's Disease (HD) illustrates this practically, where we work to identify key genes involved in HD by comparing affected individuals with neurologically normal controls. Our objective is to obtain reliable results while minimizing the impact of cellwise outliers. While many robust variable selection methods have been developed, few effectively handle high-rate cellwise contamination, and it remains an open problem to what extent such methods can control error rates. We introduce GRALF, the Gaussian-Ranked Adaptive Lasso with False discovery rate (FDR) control, a robust method designed for high-dimensional variable selection under cellwise contamination, taking advantage of FDR control to increase statistical reliability. GRALF builds on the framework of GR-ALasso, an effective robust variable selection method using the Gaussian-Rank estimator in the Adaptive Lasso, and integrates FDR control by generating fake counterparts of the Gaussian-rank transformed variables and estimating the number of false discoveries in the optimization process. Simulation studies demonstrate GRALF's desirable performance in terms of empirical power and FDR control under various conditions. The analysis of the HD data set identified potential gene markers associated with the development of HD, which overlap with findings from previous research, further validating the effectiveness of GRALF.\n\nGenstat Markdown: Reproducible Research with Genstat by James M. Curran\n\n\nAbstract\n\n** This talk should not be accepted unless you are desperate :-) ** Reproducible research ensures that scientific findings can be independently verified by others using the same data and methods. Markdown plays a key role in supporting reproducibility by allowing researchers to combine code, results, and narrative text in a clear, lightweight format. Tools like R Markdown or Jupyter Notebooks use Markdown to embed executable code alongside explanations and outputs, making it easy to document workflows, share analyses, and regenerate results consistently across different environments. Genstat is a statistical software package designed for data analysis, particularly in biometrical research and applications. Many users know it through its user-friendly interface. However, it also It offers a powerful scripting language. In this talk I will describe a collaborative project between students and staff at the University of Auckland and VSNi Ltd, that allows Genstat to be used in conjunction with R Markdown to provide flexible reproducible research capability to the Genstat user community.\n\n\n\n\nA covariate-adaptive test for replicability across multiple studies with false discovery rate control by Dennis Leung\n\n\nAbstract\n\nReplicability is a gold standard for credible scientific discoveries. The partial conjunction (PC) p-value proposed by Benjamini and Heller (Biometrics, 2008), which summarizes individual base p-values obtained from multiple similar studies, can gauge whether a signal of interest is replicable. However, when a large set of features are examined by these studies (such as comparable genomewide association studies conducted by different labs), testing for their replicated signals simultaneously can pose a very underpowered problem, due to both the multiplicity correction required and the inherent limitations of PC p-values. This power deficiency is particularly severe when replication is demanded for all studies, the most natural benchmark a practitioner performing meta-analyses may request. We propose ParFilter, a procedure that combines the ideas of filtering and covariate-adaptiveness to power up large-scale testing for replicated signals as described above. It validly reduces the multiplicity burden by partitioning the studies into smaller groups and borrowing between-group information to filter out unpromising features. Moreover, harnessing side information offered by available covariates, it trains hypothesis weights to encourage rejections of features more likely to exhibit replicated signals. We prove its finite-sample false discovery rate control under standard assumptions on the dependence of the base p-values across features. In simulations as well as a real case study on autoimmunity based on RNA-Seq data obtained from thymic cells, the ParFilter has demonstrated competitive performance against other existing methods for such replicability analyses.\n\nThe 4S method for the longitudinal analysis of multidimensional questionnaires: application to Parkinson’s disease progression from patient perception by Tiphaine Saulnier\n\n\nAbstract\n\n**Introduction** In health research, questionnaires are widely used to assess clinical states or quality of life (QoL). Longitudinal analysis of these tools offers valuable insights into disease progression. However, this raises multiple challenges, such as handling repeated ordinal responses, capturing the multidimensional traits underlying all the items, and accounting for informative dropout due to death during follow-up. In this work, we present the 4S method – a comprehensive strategy developed to address these challenges, and illustrate it describing health-related QoL (Hr-QoL) changes in Parkinson’s disease (PD). **Methods** The 4S method comprises four successive steps: (*1 – structuring*) identify questionnaire dimensions through factor analyses; (*2 – sequencing*) describe each dimension progression and associated predictors with a joint latent process model combining an item-response mixed model and a proportional hazard for death risk; (*3 – staging*) compare progression across dimension continuums by projecting clinical stages; (*4 – selecting*) highlight stage-specific most informative items based on Fisher information. **Application** We analyzed longitudinal data from the New Zealand Parkinson Progression Programme (NZP$^3$), following over 400 PD patients for up to 16 years. Hr-QoL was measured via the PDQ-39 questionnaire, covering motor and non-motor spheres. Five dimensions were identified: mobility, daily activities, psycho-social, stigma, and cognition/communication/bodily discomfort. All, except stigma, showed progressive decline, with patterns notably varying by sex and onset age. Items related to walking, dexterity, anxiety, and communication appeared as particularly sensitive during PD stages, offering guidance to enhance patient-centered care. **Conclusion** The 4S method is a comprehensive statistical strategy suited to analyze repeatedly-collected questionnaires in health studies.\n\n\n\n\nA semi-supervised framework for diverse multiple hypothesis testing scenarios by Jack Freestone\n\n\nAbstract\n\nStandard multiple testing procedures are designed to report a list of discoveries, or suspected false null hypotheses, given the hypotheses' p-values or test scores. Recently there has been a growing interest in enhancing such procedures by combining additional information with the primary p-value or score. In line with this idea, we develop RESET (REScoring via Estimating and Training), which uses a unique data-splitting protocol that subsequently allows any semi-supervised learning approach to factor in the available side information while maintaining finite sample error rate control. Our practical implementation, RESET Ensemble, selects from an ensemble of classification algorithms so that it is compatible with a range of multiple testing scenarios without the need for the user to select the appropriate one. We apply RESET to both p-value and competition based multiple testing problems and show that RESET is (1) power-wise competitive, (2) fast compared to most tools and (3) able to uniquely achieve finite sample false discovery rate or false discovery exceedance control, depending on the user's preference.\n\n`heritable` An R package for heritability calculations for plant breeding trials by Fonti Kar\n\n\nAbstract\n\nUnderstanding how much biological variation is heritable - or passed down from one generation to next - is central to plant breeding. Heritability is a useful indictor to assess the genetic gain of desirable traits such as yield or disease resistance in crop trials. We created `heritable`, an R package to streamline the calculation of heritability from `asreml` and `lme4` mixed model outputs in a user-friendly manner. The package provides up to six different types of heritability measures and helper functions to compare between these. Our goal is to support decision makers by providing an intuitive, open and reproducible workflow in calculating heritability in R.\n\n\n\n11:00\nMorning Tea\n\n\n\n11:30\nChair: James Curran\n\n\n\n\nOptimizing Research Impact Through Interdisciplinary and Collaborative Research by Charmaine B. Dean\n\n\nAbstract\n\nInterdisciplinary collaborative research is a key component of data science and for some of us, plays an important part of our roles as statisticians. It is not unusual that we become accustomed to vertical thinking whereby we use existing tools and methods in our own specialty to problem solve, losing sight of the larger interdisciplinary context of data science, and the context of the scientific challenge. The Government of Canada - Science and Technology branch has identified several priority research challenge topics that involve cross-disciplinary work. Although statistical tools and analytics are identified in these research challenge priority areas, additionally, the development of fundamental transformative and enabling technological tools specifically for statistical methods and analytics to support research and societal advancement is also seen as a priority. This talk shares insights about the challenges and opportunities for statistics in interdisciplinary research. Specifically, monitoring viral signals in wastewater and assessing forest fire risk are given as complex, case studies that use a collaborative and interdisciplinary approach to solve difficult problems. This approach will demonstrate the significant benefits for not only optimizing research impact but for training students to become horizontal problem solvers across a wide range of research methods which will benefit them in navigating complex problems and in the development of appropriate tools for their analysis.\n\n\n\n\n12:20\nClosing Ceremony\n\n\n\n12:30\nLunch"
  },
  {
    "objectID": "history.html",
    "href": "history.html",
    "title": "History",
    "section": "",
    "text": "Australasian region of the International Biometric Society has existed since 1948 with its flagship regional conference held biannually. The regional conference is the prime destination for Australasian statistics and data science in the biosciences. Society has a rich history rooted in statistical theory, methodology and practice. More recent advancements attract diverse skills that span into data science.\nWith only a maximum of two parallel sessions, the conference offers an intimate gathering of biometricians, statisticians, data scientists and alike in the Australasian region. The conference gives a prime opportunity to network with the regional biometrics community.\nBelow are photos from past conferences. Photo credit: Hans Hockey."
  },
  {
    "objectID": "organisers.html",
    "href": "organisers.html",
    "title": "Organisers",
    "section": "",
    "text": "Emi Tanaka (ANU, Chair)\nWarren Muller (CSIRO)\nAlan Welsh (ANU)\nJoanne Thandrayen (ANU)\nPatrick Li (ANU)\nFonti Kar (ANU)\nYidi Deng (ANU)"
  },
  {
    "objectID": "organisers.html#local-organising-committee",
    "href": "organisers.html#local-organising-committee",
    "title": "Organisers",
    "section": "",
    "text": "Emi Tanaka (ANU, Chair)\nWarren Muller (CSIRO)\nAlan Welsh (ANU)\nJoanne Thandrayen (ANU)\nPatrick Li (ANU)\nFonti Kar (ANU)\nYidi Deng (ANU)"
  },
  {
    "objectID": "organisers.html#scientific-and-program-committee",
    "href": "organisers.html#scientific-and-program-committee",
    "title": "Organisers",
    "section": "Scientific and program committee",
    "text": "Scientific and program committee\n\nDavid Warton (UNSW, Chair)\nCarolyn Huston (CSIRO)\nEmi Tanaka (ANU)\nJean Yang (USyd)\nJulian Taylor (UAdelaide)\nNicola Armstrong (Curtin)\nThomas Lumley (UAuckland)"
  },
  {
    "objectID": "organisers.html#social-program-committee",
    "href": "organisers.html#social-program-committee",
    "title": "Organisers",
    "section": "Social program committee",
    "text": "Social program committee\n\nQuan Vu (ANU, Chair)"
  },
  {
    "objectID": "organisers.html#conference-allies",
    "href": "organisers.html#conference-allies",
    "title": "Organisers",
    "section": "Conference allies",
    "text": "Conference allies\n\nFonti Kar (ANU)\nLouise McMillan(VUW)\nScott Foster (CSIRO)"
  }
]